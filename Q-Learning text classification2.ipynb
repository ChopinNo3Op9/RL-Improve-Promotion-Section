{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "texts = [\"good movie\", \"not a good movie\", \"did not like\", \"liked it\", \"fantastic movie\"]\n",
    "labels = [1, 0, 0, 1, 1]  # 1: Positive, 0: Negative\n",
    "\n",
    "# Text preprocessing and splitting\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "y = np.array(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 8 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m classifier \u001b[38;5;241m=\u001b[39m TextClassifierRL(n_states, n_actions)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Test the classifier\u001b[39;00m\n\u001b[0;32m     36\u001b[0m predictions \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m, in \u001b[0;36mTextClassifierRL.train\u001b[1;34m(self, X_train, y_train, epochs, learning_rate, gamma)\u001b[0m\n\u001b[0;32m     13\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action_pred \u001b[38;5;241m==\u001b[39m action_true \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Q-learning algorithm\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m next_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Estimate of optimal future value\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[current_state][action_pred] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[current_state][action_pred] \u001b[38;5;241m+\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m next_max \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[current_state][action_pred])\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 8 were indexed"
     ]
    }
   ],
   "source": [
    "class TextClassifierRL:\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=1000, learning_rate=0.1, gamma=0.6):\n",
    "        for i in range(epochs):\n",
    "            for state, action_true in zip(X_train, y_train):\n",
    "                current_state = tuple(state)\n",
    "                reward = -1  # Assume a negative reward for any action\n",
    "                \n",
    "                # Simulating taking action and getting new state (in a real scenario, this would come from the environment)\n",
    "                action_pred = np.random.choice([0, 1])  # Randomly choose an action\n",
    "                reward = 1 if action_pred == action_true else -1\n",
    "\n",
    "                # Q-learning algorithm\n",
    "                next_max = np.max(self.q_table[current_state])  # Estimate of optimal future value\n",
    "                self.q_table[current_state][action_pred] = self.q_table[current_state][action_pred] + learning_rate * (reward + gamma * next_max - self.q_table[current_state][action_pred])\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for state in X:\n",
    "            state = tuple(state)\n",
    "            action = np.argmax(self.q_table[state])  # Choose the action with the highest Q-value\n",
    "            predictions.append(action)\n",
    "        return predictions\n",
    "\n",
    "# Initialize our classifier\n",
    "n_states = len(X_train[0])  # Number of features in our data\n",
    "n_actions = 2  # Number of possible classifications (positive, negative)\n",
    "classifier = TextClassifierRL(n_states, n_actions)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.train(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "predictions = classifier.predict(X_test)\n",
    "print(f\"Predictions: {predictions}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
