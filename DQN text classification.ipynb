{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock dataset for demonstration\n",
    "class TextDataset(Dataset):\n",
    "    # def __init__(self, vocabulary_size, sequence_length, num_samples):\n",
    "    #     self.data = torch.randint(0, vocabulary_size, (num_samples, sequence_length))\n",
    "    #     self.labels = torch.randint(0, 2, (num_samples,))\n",
    "    def __init__(self, texts, labels, sequence_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab = self.build_vocab(texts)\n",
    "        self.encoded_texts = [self.encode_text(text) for text in texts]\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        unique_words = set(word for text in texts for word in text.lower().split())\n",
    "        vocab = {word: i + 1 for i, word in enumerate(unique_words)}  # +1 for padding token at index 0\n",
    "        return vocab\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        return [self.vocab.get(word, 0) for word in text.lower().split()][:self.sequence_length] + [0] * (self.sequence_length - len(text.split()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.encoded_texts[idx]), torch.tensor(self.labels[idx])\n",
    "    \n",
    "\n",
    "# Define the Q-network model\n",
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, vocabulary_size, embedding_dim, hidden_dim, num_classes):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         embeds = self.embedding(x)\n",
    "#         lstm_out, _ = self.lstm(embeds)\n",
    "#         q_values = self.fc(lstm_out[:, -1])\n",
    "#         return q_values\n",
    "\n",
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, vocabulary_size, embedding_dim, hidden_dim, num_classes):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         embeds = self.embedding(x)\n",
    "#         lstm_out, _ = self.lstm(embeds)\n",
    "#         q_values = self.fc(lstm_out[:, -1])\n",
    "#         return q_values\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim, hidden_dim, num_classes, dropout_rate=0.5, pre_trained_embeddings=None):\n",
    "        super(DQN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        if pre_trained_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(pre_trained_embeddings)\n",
    "            self.embedding.weight.requires_grad = False  # Or True if you want to fine-tune\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2) # Stacked LSTMs\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Use dropout_rate from arguments\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # Adjust for bidirectional LSTM\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.dropout(lstm_out[:, -1])\n",
    "        out = self.relu(out)\n",
    "        q_values = self.fc(out)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # Using CrossEntropyLoss which combines LogSoftmax and NLLLoss\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "def validate(model, device, validation_loader):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in validation_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            validation_loss += nn.CrossEntropyLoss()(output, target).item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    validation_loss /= len(validation_loader.dataset)\n",
    "    validation_acc = correct / len(validation_loader.dataset)\n",
    "    print(f'\\nValidation set: Average loss: {validation_loss:.4f}, Accuracy: {correct}/{len(validation_loader.dataset)} ({100. * correct / len(validation_loader.dataset):.0f}%)\\n')\n",
    "    return validation_loss, validation_acc\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Encoded text: tensor([[20266, 14445, 11654,  7230, 18993, 14206, 12027, 17419,  3660,   525],\n",
      "        [ 9855,  6087,  4362,  1801,  9848,   354, 18140, 19195, 10090,  2209],\n",
      "        [ 6397, 18993, 18192, 13909,  6419, 11927, 14401, 11180, 15236,  9362],\n",
      "        [17641, 18365, 10094, 12794,  1912, 15159,  9509, 13795, 16944, 11758]])\n",
      "Train Label: tensor([1, 1, 1, 0])\n",
      "Validation Encoded text: tensor([[2399, 3263, 5861, 6981, 1847, 7823, 8057, 7878, 6652, 8291],\n",
      "        [1162, 1386, 4916, 7381, 2305, 6181, 1947, 6556, 5600, 5340],\n",
      "        [3521, 8157, 6887,  585, 4568, 1853, 1114, 7966, 6089,  614],\n",
      "        [1162,  221, 4916, 6859, 7478, 5534, 8358, 1866, 4948, 7728]])\n",
      "Validation Label: tensor([1, 1, 1, 1])\n",
      "Test Encoded text: tensor([[5943, 4363, 6742, 8266, 7125, 5790, 6126, 2067, 3345, 5229],\n",
      "        [3631, 1346, 7111, 3429, 3303, 7643,  484, 4489, 7662, 5296],\n",
      "        [4628, 6126, 8552,  546, 1701, 1068, 6903, 4743, 6178, 7028],\n",
      "        [3285, 6041, 8001, 1719, 5866, 4782, 3370, 8702,  925,  755]])\n",
      "Test Label: tensor([1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/sentiment_analysis.csv')\n",
    "\n",
    "# Extracting texts and labels\n",
    "texts = df['tweet'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Splitting dataset into train+val and test\n",
    "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=RAND_STATE)\n",
    "\n",
    "# Splitting train+val into train and val\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(train_val_texts, train_val_labels, test_size=0.25, random_state=RAND_STATE)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Creating datasets\n",
    "sequence_length = 10  # Max number of words in a text\n",
    "train_dataset = TextDataset(train_texts, train_labels, sequence_length)\n",
    "validation_dataset = TextDataset(validation_texts, validation_labels, sequence_length)\n",
    "test_dataset = TextDataset(test_texts, test_labels, sequence_length)\n",
    "\n",
    "# Creating DataLoaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for data, label in train_loader:\n",
    "    print(f\"Train Encoded text: {data}\")\n",
    "    print(f\"Train Label: {label}\")\n",
    "    break  # Just show one batch for brevity\n",
    "\n",
    "for data, label in validation_loader:\n",
    "    print(f\"Validation Encoded text: {data}\")\n",
    "    print(f\"Validation Label: {label}\")\n",
    "    break  # Just show one batch for brevity\n",
    "\n",
    "for data, label in test_loader:\n",
    "    print(f\"Test Encoded text: {data}\")\n",
    "    print(f\"Test Label: {label}\")\n",
    "    break  # Just show one batch for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.733271\n",
      "Train Epoch: 1 [40/4752 (1%)]\tLoss: 0.688850\n",
      "Train Epoch: 1 [80/4752 (2%)]\tLoss: 0.578902\n",
      "Train Epoch: 1 [120/4752 (3%)]\tLoss: 0.752718\n",
      "Train Epoch: 1 [160/4752 (3%)]\tLoss: 0.752234\n",
      "Train Epoch: 1 [200/4752 (4%)]\tLoss: 0.681360\n",
      "Train Epoch: 1 [240/4752 (5%)]\tLoss: 0.258072\n",
      "Train Epoch: 1 [280/4752 (6%)]\tLoss: 0.290984\n",
      "Train Epoch: 1 [320/4752 (7%)]\tLoss: 0.257531\n",
      "Train Epoch: 1 [360/4752 (8%)]\tLoss: 0.532985\n",
      "Train Epoch: 1 [400/4752 (8%)]\tLoss: 0.443105\n",
      "Train Epoch: 1 [440/4752 (9%)]\tLoss: 0.348711\n",
      "Train Epoch: 1 [480/4752 (10%)]\tLoss: 0.718589\n",
      "Train Epoch: 1 [520/4752 (11%)]\tLoss: 0.506109\n",
      "Train Epoch: 1 [560/4752 (12%)]\tLoss: 0.301907\n",
      "Train Epoch: 1 [600/4752 (13%)]\tLoss: 0.157683\n",
      "Train Epoch: 1 [640/4752 (13%)]\tLoss: 0.376611\n",
      "Train Epoch: 1 [680/4752 (14%)]\tLoss: 0.369956\n",
      "Train Epoch: 1 [720/4752 (15%)]\tLoss: 0.203477\n",
      "Train Epoch: 1 [760/4752 (16%)]\tLoss: 0.583367\n",
      "Train Epoch: 1 [800/4752 (17%)]\tLoss: 0.275060\n",
      "Train Epoch: 1 [840/4752 (18%)]\tLoss: 0.683232\n",
      "Train Epoch: 1 [880/4752 (19%)]\tLoss: 0.202718\n",
      "Train Epoch: 1 [920/4752 (19%)]\tLoss: 0.497009\n",
      "Train Epoch: 1 [960/4752 (20%)]\tLoss: 0.336579\n",
      "Train Epoch: 1 [1000/4752 (21%)]\tLoss: 0.490638\n",
      "Train Epoch: 1 [1040/4752 (22%)]\tLoss: 0.425091\n",
      "Train Epoch: 1 [1080/4752 (23%)]\tLoss: 0.201262\n",
      "Train Epoch: 1 [1120/4752 (24%)]\tLoss: 0.250789\n",
      "Train Epoch: 1 [1160/4752 (24%)]\tLoss: 0.452175\n",
      "Train Epoch: 1 [1200/4752 (25%)]\tLoss: 0.446696\n",
      "Train Epoch: 1 [1240/4752 (26%)]\tLoss: 0.516224\n",
      "Train Epoch: 1 [1280/4752 (27%)]\tLoss: 0.207169\n",
      "Train Epoch: 1 [1320/4752 (28%)]\tLoss: 0.246916\n",
      "Train Epoch: 1 [1360/4752 (29%)]\tLoss: 0.289789\n",
      "Train Epoch: 1 [1400/4752 (29%)]\tLoss: 0.252385\n",
      "Train Epoch: 1 [1440/4752 (30%)]\tLoss: 0.168615\n",
      "Train Epoch: 1 [1480/4752 (31%)]\tLoss: 0.878448\n",
      "Train Epoch: 1 [1520/4752 (32%)]\tLoss: 0.379748\n",
      "Train Epoch: 1 [1560/4752 (33%)]\tLoss: 0.311637\n",
      "Train Epoch: 1 [1600/4752 (34%)]\tLoss: 0.857581\n",
      "Train Epoch: 1 [1640/4752 (35%)]\tLoss: 0.106807\n",
      "Train Epoch: 1 [1680/4752 (35%)]\tLoss: 0.198000\n",
      "Train Epoch: 1 [1720/4752 (36%)]\tLoss: 0.413443\n",
      "Train Epoch: 1 [1760/4752 (37%)]\tLoss: 1.020260\n",
      "Train Epoch: 1 [1800/4752 (38%)]\tLoss: 0.444978\n",
      "Train Epoch: 1 [1840/4752 (39%)]\tLoss: 0.097040\n",
      "Train Epoch: 1 [1880/4752 (40%)]\tLoss: 0.475892\n",
      "Train Epoch: 1 [1920/4752 (40%)]\tLoss: 0.291847\n",
      "Train Epoch: 1 [1960/4752 (41%)]\tLoss: 0.388261\n",
      "Train Epoch: 1 [2000/4752 (42%)]\tLoss: 0.124563\n",
      "Train Epoch: 1 [2040/4752 (43%)]\tLoss: 0.579725\n",
      "Train Epoch: 1 [2080/4752 (44%)]\tLoss: 0.219589\n",
      "Train Epoch: 1 [2120/4752 (45%)]\tLoss: 0.422628\n",
      "Train Epoch: 1 [2160/4752 (45%)]\tLoss: 0.846667\n",
      "Train Epoch: 1 [2200/4752 (46%)]\tLoss: 1.267282\n",
      "Train Epoch: 1 [2240/4752 (47%)]\tLoss: 0.461479\n",
      "Train Epoch: 1 [2280/4752 (48%)]\tLoss: 0.102978\n",
      "Train Epoch: 1 [2320/4752 (49%)]\tLoss: 0.702403\n",
      "Train Epoch: 1 [2360/4752 (50%)]\tLoss: 0.497785\n",
      "Train Epoch: 1 [2400/4752 (51%)]\tLoss: 0.369969\n",
      "Train Epoch: 1 [2440/4752 (51%)]\tLoss: 0.072343\n",
      "Train Epoch: 1 [2480/4752 (52%)]\tLoss: 0.204620\n",
      "Train Epoch: 1 [2520/4752 (53%)]\tLoss: 0.609376\n",
      "Train Epoch: 1 [2560/4752 (54%)]\tLoss: 0.215549\n",
      "Train Epoch: 1 [2600/4752 (55%)]\tLoss: 0.214255\n",
      "Train Epoch: 1 [2640/4752 (56%)]\tLoss: 0.041029\n",
      "Train Epoch: 1 [2680/4752 (56%)]\tLoss: 0.782451\n",
      "Train Epoch: 1 [2720/4752 (57%)]\tLoss: 0.007515\n",
      "Train Epoch: 1 [2760/4752 (58%)]\tLoss: 0.128167\n",
      "Train Epoch: 1 [2800/4752 (59%)]\tLoss: 0.477470\n",
      "Train Epoch: 1 [2840/4752 (60%)]\tLoss: 0.265421\n",
      "Train Epoch: 1 [2880/4752 (61%)]\tLoss: 0.623959\n",
      "Train Epoch: 1 [2920/4752 (61%)]\tLoss: 0.144954\n",
      "Train Epoch: 1 [2960/4752 (62%)]\tLoss: 1.110884\n",
      "Train Epoch: 1 [3000/4752 (63%)]\tLoss: 1.061289\n",
      "Train Epoch: 1 [3040/4752 (64%)]\tLoss: 0.514247\n",
      "Train Epoch: 1 [3080/4752 (65%)]\tLoss: 0.453968\n",
      "Train Epoch: 1 [3120/4752 (66%)]\tLoss: 0.032556\n",
      "Train Epoch: 1 [3160/4752 (66%)]\tLoss: 0.431780\n",
      "Train Epoch: 1 [3200/4752 (67%)]\tLoss: 0.713074\n",
      "Train Epoch: 1 [3240/4752 (68%)]\tLoss: 1.275111\n",
      "Train Epoch: 1 [3280/4752 (69%)]\tLoss: 0.827914\n",
      "Train Epoch: 1 [3320/4752 (70%)]\tLoss: 0.301235\n",
      "Train Epoch: 1 [3360/4752 (71%)]\tLoss: 0.815587\n",
      "Train Epoch: 1 [3400/4752 (72%)]\tLoss: 0.301272\n",
      "Train Epoch: 1 [3440/4752 (72%)]\tLoss: 0.558575\n",
      "Train Epoch: 1 [3480/4752 (73%)]\tLoss: 0.081645\n",
      "Train Epoch: 1 [3520/4752 (74%)]\tLoss: 0.188833\n",
      "Train Epoch: 1 [3560/4752 (75%)]\tLoss: 0.439363\n",
      "Train Epoch: 1 [3600/4752 (76%)]\tLoss: 0.265355\n",
      "Train Epoch: 1 [3640/4752 (77%)]\tLoss: 0.330789\n",
      "Train Epoch: 1 [3680/4752 (77%)]\tLoss: 0.026665\n",
      "Train Epoch: 1 [3720/4752 (78%)]\tLoss: 0.988622\n",
      "Train Epoch: 1 [3760/4752 (79%)]\tLoss: 0.160167\n",
      "Train Epoch: 1 [3800/4752 (80%)]\tLoss: 0.275807\n",
      "Train Epoch: 1 [3840/4752 (81%)]\tLoss: 0.511221\n",
      "Train Epoch: 1 [3880/4752 (82%)]\tLoss: 0.457506\n",
      "Train Epoch: 1 [3920/4752 (82%)]\tLoss: 0.231916\n",
      "Train Epoch: 1 [3960/4752 (83%)]\tLoss: 0.401637\n",
      "Train Epoch: 1 [4000/4752 (84%)]\tLoss: 0.235510\n",
      "Train Epoch: 1 [4040/4752 (85%)]\tLoss: 0.636542\n",
      "Train Epoch: 1 [4080/4752 (86%)]\tLoss: 0.499122\n",
      "Train Epoch: 1 [4120/4752 (87%)]\tLoss: 0.238941\n",
      "Train Epoch: 1 [4160/4752 (88%)]\tLoss: 0.560937\n",
      "Train Epoch: 1 [4200/4752 (88%)]\tLoss: 0.138125\n",
      "Train Epoch: 1 [4240/4752 (89%)]\tLoss: 0.419751\n",
      "Train Epoch: 1 [4280/4752 (90%)]\tLoss: 0.381890\n",
      "Train Epoch: 1 [4320/4752 (91%)]\tLoss: 0.566183\n",
      "Train Epoch: 1 [4360/4752 (92%)]\tLoss: 0.024369\n",
      "Train Epoch: 1 [4400/4752 (93%)]\tLoss: 0.002158\n",
      "Train Epoch: 1 [4440/4752 (93%)]\tLoss: 0.451130\n",
      "Train Epoch: 1 [4480/4752 (94%)]\tLoss: 0.224716\n",
      "Train Epoch: 1 [4520/4752 (95%)]\tLoss: 0.178841\n",
      "Train Epoch: 1 [4560/4752 (96%)]\tLoss: 0.605407\n",
      "Train Epoch: 1 [4600/4752 (97%)]\tLoss: 0.338221\n",
      "Train Epoch: 1 [4640/4752 (98%)]\tLoss: 0.241134\n",
      "Train Epoch: 1 [4680/4752 (98%)]\tLoss: 0.209846\n",
      "Train Epoch: 1 [4720/4752 (99%)]\tLoss: 0.433936\n",
      "\n",
      "Validation set: Average loss: 0.2060, Accuracy: 1176/1584 (74%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 0.294358\n",
      "Train Epoch: 2 [40/4752 (1%)]\tLoss: 0.089228\n",
      "Train Epoch: 2 [80/4752 (2%)]\tLoss: 0.052422\n",
      "Train Epoch: 2 [120/4752 (3%)]\tLoss: 0.442464\n",
      "Train Epoch: 2 [160/4752 (3%)]\tLoss: 0.034938\n",
      "Train Epoch: 2 [200/4752 (4%)]\tLoss: 0.422015\n",
      "Train Epoch: 2 [240/4752 (5%)]\tLoss: 0.128647\n",
      "Train Epoch: 2 [280/4752 (6%)]\tLoss: 0.056307\n",
      "Train Epoch: 2 [320/4752 (7%)]\tLoss: 0.012940\n",
      "Train Epoch: 2 [360/4752 (8%)]\tLoss: 0.058658\n",
      "Train Epoch: 2 [400/4752 (8%)]\tLoss: 0.201023\n",
      "Train Epoch: 2 [440/4752 (9%)]\tLoss: 0.888387\n",
      "Train Epoch: 2 [480/4752 (10%)]\tLoss: 0.055415\n",
      "Train Epoch: 2 [520/4752 (11%)]\tLoss: 0.905844\n",
      "Train Epoch: 2 [560/4752 (12%)]\tLoss: 0.125141\n",
      "Train Epoch: 2 [600/4752 (13%)]\tLoss: 0.235754\n",
      "Train Epoch: 2 [640/4752 (13%)]\tLoss: 0.009835\n",
      "Train Epoch: 2 [680/4752 (14%)]\tLoss: 0.968936\n",
      "Train Epoch: 2 [720/4752 (15%)]\tLoss: 0.032034\n",
      "Train Epoch: 2 [760/4752 (16%)]\tLoss: 0.109734\n",
      "Train Epoch: 2 [800/4752 (17%)]\tLoss: 0.007335\n",
      "Train Epoch: 2 [840/4752 (18%)]\tLoss: 0.023282\n",
      "Train Epoch: 2 [880/4752 (19%)]\tLoss: 0.509478\n",
      "Train Epoch: 2 [920/4752 (19%)]\tLoss: 0.014706\n",
      "Train Epoch: 2 [960/4752 (20%)]\tLoss: 0.557507\n",
      "Train Epoch: 2 [1000/4752 (21%)]\tLoss: 0.184582\n",
      "Train Epoch: 2 [1040/4752 (22%)]\tLoss: 0.091673\n",
      "Train Epoch: 2 [1080/4752 (23%)]\tLoss: 0.717981\n",
      "Train Epoch: 2 [1120/4752 (24%)]\tLoss: 0.108723\n",
      "Train Epoch: 2 [1160/4752 (24%)]\tLoss: 0.178989\n",
      "Train Epoch: 2 [1200/4752 (25%)]\tLoss: 0.824154\n",
      "Train Epoch: 2 [1240/4752 (26%)]\tLoss: 0.104212\n",
      "Train Epoch: 2 [1280/4752 (27%)]\tLoss: 1.107136\n",
      "Train Epoch: 2 [1320/4752 (28%)]\tLoss: 0.295299\n",
      "Train Epoch: 2 [1360/4752 (29%)]\tLoss: 0.188434\n",
      "Train Epoch: 2 [1400/4752 (29%)]\tLoss: 0.199988\n",
      "Train Epoch: 2 [1440/4752 (30%)]\tLoss: 0.106093\n",
      "Train Epoch: 2 [1480/4752 (31%)]\tLoss: 0.358857\n",
      "Train Epoch: 2 [1520/4752 (32%)]\tLoss: 0.001008\n",
      "Train Epoch: 2 [1560/4752 (33%)]\tLoss: 0.132932\n",
      "Train Epoch: 2 [1600/4752 (34%)]\tLoss: 0.053586\n",
      "Train Epoch: 2 [1640/4752 (35%)]\tLoss: 0.082896\n",
      "Train Epoch: 2 [1680/4752 (35%)]\tLoss: 0.833561\n",
      "Train Epoch: 2 [1720/4752 (36%)]\tLoss: 0.140404\n",
      "Train Epoch: 2 [1760/4752 (37%)]\tLoss: 0.674247\n",
      "Train Epoch: 2 [1800/4752 (38%)]\tLoss: 0.807575\n",
      "Train Epoch: 2 [1840/4752 (39%)]\tLoss: 0.862011\n",
      "Train Epoch: 2 [1880/4752 (40%)]\tLoss: 0.195890\n",
      "Train Epoch: 2 [1920/4752 (40%)]\tLoss: 0.044963\n",
      "Train Epoch: 2 [1960/4752 (41%)]\tLoss: 0.321690\n",
      "Train Epoch: 2 [2000/4752 (42%)]\tLoss: 0.013099\n",
      "Train Epoch: 2 [2040/4752 (43%)]\tLoss: 0.409491\n",
      "Train Epoch: 2 [2080/4752 (44%)]\tLoss: 0.077364\n",
      "Train Epoch: 2 [2120/4752 (45%)]\tLoss: 0.018159\n",
      "Train Epoch: 2 [2160/4752 (45%)]\tLoss: 0.023980\n",
      "Train Epoch: 2 [2200/4752 (46%)]\tLoss: 0.027887\n",
      "Train Epoch: 2 [2240/4752 (47%)]\tLoss: 0.001929\n",
      "Train Epoch: 2 [2280/4752 (48%)]\tLoss: 0.438080\n",
      "Train Epoch: 2 [2320/4752 (49%)]\tLoss: 0.095132\n",
      "Train Epoch: 2 [2360/4752 (50%)]\tLoss: 0.111492\n",
      "Train Epoch: 2 [2400/4752 (51%)]\tLoss: 0.034417\n",
      "Train Epoch: 2 [2440/4752 (51%)]\tLoss: 0.323289\n",
      "Train Epoch: 2 [2480/4752 (52%)]\tLoss: 0.092370\n",
      "Train Epoch: 2 [2520/4752 (53%)]\tLoss: 0.051433\n",
      "Train Epoch: 2 [2560/4752 (54%)]\tLoss: 0.011185\n",
      "Train Epoch: 2 [2600/4752 (55%)]\tLoss: 0.543199\n",
      "Train Epoch: 2 [2640/4752 (56%)]\tLoss: 0.229909\n",
      "Train Epoch: 2 [2680/4752 (56%)]\tLoss: 0.350641\n",
      "Train Epoch: 2 [2720/4752 (57%)]\tLoss: 0.234351\n",
      "Train Epoch: 2 [2760/4752 (58%)]\tLoss: 0.078925\n",
      "Train Epoch: 2 [2800/4752 (59%)]\tLoss: 0.245675\n",
      "Train Epoch: 2 [2840/4752 (60%)]\tLoss: 0.723722\n",
      "Train Epoch: 2 [2880/4752 (61%)]\tLoss: 0.389261\n",
      "Train Epoch: 2 [2920/4752 (61%)]\tLoss: 0.179298\n",
      "Train Epoch: 2 [2960/4752 (62%)]\tLoss: 0.036266\n",
      "Train Epoch: 2 [3000/4752 (63%)]\tLoss: 0.106235\n",
      "Train Epoch: 2 [3040/4752 (64%)]\tLoss: 1.037279\n",
      "Train Epoch: 2 [3080/4752 (65%)]\tLoss: 0.062106\n",
      "Train Epoch: 2 [3120/4752 (66%)]\tLoss: 0.001383\n",
      "Train Epoch: 2 [3160/4752 (66%)]\tLoss: 0.002778\n",
      "Train Epoch: 2 [3200/4752 (67%)]\tLoss: 0.011522\n",
      "Train Epoch: 2 [3240/4752 (68%)]\tLoss: 0.228201\n",
      "Train Epoch: 2 [3280/4752 (69%)]\tLoss: 0.353238\n",
      "Train Epoch: 2 [3320/4752 (70%)]\tLoss: 0.563002\n",
      "Train Epoch: 2 [3360/4752 (71%)]\tLoss: 0.086737\n",
      "Train Epoch: 2 [3400/4752 (72%)]\tLoss: 0.460889\n",
      "Train Epoch: 2 [3440/4752 (72%)]\tLoss: 0.328991\n",
      "Train Epoch: 2 [3480/4752 (73%)]\tLoss: 1.427855\n",
      "Train Epoch: 2 [3520/4752 (74%)]\tLoss: 0.388260\n",
      "Train Epoch: 2 [3560/4752 (75%)]\tLoss: 0.050662\n",
      "Train Epoch: 2 [3600/4752 (76%)]\tLoss: 0.002175\n",
      "Train Epoch: 2 [3640/4752 (77%)]\tLoss: 0.093815\n",
      "Train Epoch: 2 [3680/4752 (77%)]\tLoss: 0.201067\n",
      "Train Epoch: 2 [3720/4752 (78%)]\tLoss: 0.205034\n",
      "Train Epoch: 2 [3760/4752 (79%)]\tLoss: 0.088956\n",
      "Train Epoch: 2 [3800/4752 (80%)]\tLoss: 0.460174\n",
      "Train Epoch: 2 [3840/4752 (81%)]\tLoss: 1.318058\n",
      "Train Epoch: 2 [3880/4752 (82%)]\tLoss: 0.855231\n",
      "Train Epoch: 2 [3920/4752 (82%)]\tLoss: 0.040935\n",
      "Train Epoch: 2 [3960/4752 (83%)]\tLoss: 0.092622\n",
      "Train Epoch: 2 [4000/4752 (84%)]\tLoss: 1.424030\n",
      "Train Epoch: 2 [4040/4752 (85%)]\tLoss: 0.117169\n",
      "Train Epoch: 2 [4080/4752 (86%)]\tLoss: 0.053472\n",
      "Train Epoch: 2 [4120/4752 (87%)]\tLoss: 0.029008\n",
      "Train Epoch: 2 [4160/4752 (88%)]\tLoss: 0.107234\n",
      "Train Epoch: 2 [4200/4752 (88%)]\tLoss: 0.037897\n",
      "Train Epoch: 2 [4240/4752 (89%)]\tLoss: 0.120798\n",
      "Train Epoch: 2 [4280/4752 (90%)]\tLoss: 0.386866\n",
      "Train Epoch: 2 [4320/4752 (91%)]\tLoss: 0.125876\n",
      "Train Epoch: 2 [4360/4752 (92%)]\tLoss: 0.204112\n",
      "Train Epoch: 2 [4400/4752 (93%)]\tLoss: 0.112216\n",
      "Train Epoch: 2 [4440/4752 (93%)]\tLoss: 0.380031\n",
      "Train Epoch: 2 [4480/4752 (94%)]\tLoss: 0.252829\n",
      "Train Epoch: 2 [4520/4752 (95%)]\tLoss: 0.061967\n",
      "Train Epoch: 2 [4560/4752 (96%)]\tLoss: 0.057001\n",
      "Train Epoch: 2 [4600/4752 (97%)]\tLoss: 0.086823\n",
      "Train Epoch: 2 [4640/4752 (98%)]\tLoss: 0.194637\n",
      "Train Epoch: 2 [4680/4752 (98%)]\tLoss: 0.483415\n",
      "Train Epoch: 2 [4720/4752 (99%)]\tLoss: 0.006917\n",
      "\n",
      "Validation set: Average loss: 0.3082, Accuracy: 1115/1584 (70%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 0.128765\n",
      "Train Epoch: 3 [40/4752 (1%)]\tLoss: 0.846398\n",
      "Train Epoch: 3 [80/4752 (2%)]\tLoss: 0.012604\n",
      "Train Epoch: 3 [120/4752 (3%)]\tLoss: 0.002617\n",
      "Train Epoch: 3 [160/4752 (3%)]\tLoss: 0.025812\n",
      "Train Epoch: 3 [200/4752 (4%)]\tLoss: 0.080409\n",
      "Train Epoch: 3 [240/4752 (5%)]\tLoss: 0.084334\n",
      "Train Epoch: 3 [280/4752 (6%)]\tLoss: 0.256815\n",
      "Train Epoch: 3 [320/4752 (7%)]\tLoss: 0.091765\n",
      "Train Epoch: 3 [360/4752 (8%)]\tLoss: 0.000263\n",
      "Train Epoch: 3 [400/4752 (8%)]\tLoss: 0.026423\n",
      "Train Epoch: 3 [440/4752 (9%)]\tLoss: 0.030166\n",
      "Train Epoch: 3 [480/4752 (10%)]\tLoss: 0.087546\n",
      "Train Epoch: 3 [520/4752 (11%)]\tLoss: 0.085559\n",
      "Train Epoch: 3 [560/4752 (12%)]\tLoss: 0.036820\n",
      "Train Epoch: 3 [600/4752 (13%)]\tLoss: 0.005368\n",
      "Train Epoch: 3 [640/4752 (13%)]\tLoss: 0.009541\n",
      "Train Epoch: 3 [680/4752 (14%)]\tLoss: 0.000184\n",
      "Train Epoch: 3 [720/4752 (15%)]\tLoss: 0.018322\n",
      "Train Epoch: 3 [760/4752 (16%)]\tLoss: 0.029166\n",
      "Train Epoch: 3 [800/4752 (17%)]\tLoss: 0.000241\n",
      "Train Epoch: 3 [840/4752 (18%)]\tLoss: 0.000107\n",
      "Train Epoch: 3 [880/4752 (19%)]\tLoss: 0.000534\n",
      "Train Epoch: 3 [920/4752 (19%)]\tLoss: 0.006913\n",
      "Train Epoch: 3 [960/4752 (20%)]\tLoss: 0.429287\n",
      "Train Epoch: 3 [1000/4752 (21%)]\tLoss: 0.038909\n",
      "Train Epoch: 3 [1040/4752 (22%)]\tLoss: 0.182054\n",
      "Train Epoch: 3 [1080/4752 (23%)]\tLoss: 0.016609\n",
      "Train Epoch: 3 [1120/4752 (24%)]\tLoss: 0.230205\n",
      "Train Epoch: 3 [1160/4752 (24%)]\tLoss: 0.006611\n",
      "Train Epoch: 3 [1200/4752 (25%)]\tLoss: 0.004333\n",
      "Train Epoch: 3 [1240/4752 (26%)]\tLoss: 0.000716\n",
      "Train Epoch: 3 [1280/4752 (27%)]\tLoss: 0.021698\n",
      "Train Epoch: 3 [1320/4752 (28%)]\tLoss: 0.001722\n",
      "Train Epoch: 3 [1360/4752 (29%)]\tLoss: 0.859207\n",
      "Train Epoch: 3 [1400/4752 (29%)]\tLoss: 0.056668\n",
      "Train Epoch: 3 [1440/4752 (30%)]\tLoss: 0.109949\n",
      "Train Epoch: 3 [1480/4752 (31%)]\tLoss: 0.167312\n",
      "Train Epoch: 3 [1520/4752 (32%)]\tLoss: 0.107636\n",
      "Train Epoch: 3 [1560/4752 (33%)]\tLoss: 0.056721\n",
      "Train Epoch: 3 [1600/4752 (34%)]\tLoss: 0.097289\n",
      "Train Epoch: 3 [1640/4752 (35%)]\tLoss: 0.054122\n",
      "Train Epoch: 3 [1680/4752 (35%)]\tLoss: 0.079339\n",
      "Train Epoch: 3 [1720/4752 (36%)]\tLoss: 0.694650\n",
      "Train Epoch: 3 [1760/4752 (37%)]\tLoss: 0.000319\n",
      "Train Epoch: 3 [1800/4752 (38%)]\tLoss: 0.234633\n",
      "Train Epoch: 3 [1840/4752 (39%)]\tLoss: 0.237903\n",
      "Train Epoch: 3 [1880/4752 (40%)]\tLoss: 0.007332\n",
      "Train Epoch: 3 [1920/4752 (40%)]\tLoss: 0.220158\n",
      "Train Epoch: 3 [1960/4752 (41%)]\tLoss: 0.145072\n",
      "Train Epoch: 3 [2000/4752 (42%)]\tLoss: 0.002150\n",
      "Train Epoch: 3 [2040/4752 (43%)]\tLoss: 0.173994\n",
      "Train Epoch: 3 [2080/4752 (44%)]\tLoss: 0.008305\n",
      "Train Epoch: 3 [2120/4752 (45%)]\tLoss: 0.764685\n",
      "Train Epoch: 3 [2160/4752 (45%)]\tLoss: 0.016302\n",
      "Train Epoch: 3 [2200/4752 (46%)]\tLoss: 0.195507\n",
      "Train Epoch: 3 [2240/4752 (47%)]\tLoss: 0.006328\n",
      "Train Epoch: 3 [2280/4752 (48%)]\tLoss: 0.034959\n",
      "Train Epoch: 3 [2320/4752 (49%)]\tLoss: 0.250087\n",
      "Train Epoch: 3 [2360/4752 (50%)]\tLoss: 0.074035\n",
      "Train Epoch: 3 [2400/4752 (51%)]\tLoss: 0.000972\n",
      "Train Epoch: 3 [2440/4752 (51%)]\tLoss: 0.398243\n",
      "Train Epoch: 3 [2480/4752 (52%)]\tLoss: 0.117228\n",
      "Train Epoch: 3 [2520/4752 (53%)]\tLoss: 0.102554\n",
      "Train Epoch: 3 [2560/4752 (54%)]\tLoss: 0.172541\n",
      "Train Epoch: 3 [2600/4752 (55%)]\tLoss: 0.364721\n",
      "Train Epoch: 3 [2640/4752 (56%)]\tLoss: 0.000072\n",
      "Train Epoch: 3 [2680/4752 (56%)]\tLoss: 0.018179\n",
      "Train Epoch: 3 [2720/4752 (57%)]\tLoss: 0.007672\n",
      "Train Epoch: 3 [2760/4752 (58%)]\tLoss: 0.149013\n",
      "Train Epoch: 3 [2800/4752 (59%)]\tLoss: 0.011145\n",
      "Train Epoch: 3 [2840/4752 (60%)]\tLoss: 0.018798\n",
      "Train Epoch: 3 [2880/4752 (61%)]\tLoss: 0.281625\n",
      "Train Epoch: 3 [2920/4752 (61%)]\tLoss: 0.001768\n",
      "Train Epoch: 3 [2960/4752 (62%)]\tLoss: 0.294527\n",
      "Train Epoch: 3 [3000/4752 (63%)]\tLoss: 0.008893\n",
      "Train Epoch: 3 [3040/4752 (64%)]\tLoss: 0.277016\n",
      "Train Epoch: 3 [3080/4752 (65%)]\tLoss: 0.026027\n",
      "Train Epoch: 3 [3120/4752 (66%)]\tLoss: 0.037298\n",
      "Train Epoch: 3 [3160/4752 (66%)]\tLoss: 0.157182\n",
      "Train Epoch: 3 [3200/4752 (67%)]\tLoss: 0.006706\n",
      "Train Epoch: 3 [3240/4752 (68%)]\tLoss: 0.011750\n",
      "Train Epoch: 3 [3280/4752 (69%)]\tLoss: 0.031094\n",
      "Train Epoch: 3 [3320/4752 (70%)]\tLoss: 0.002181\n",
      "Train Epoch: 3 [3360/4752 (71%)]\tLoss: 0.039779\n",
      "Train Epoch: 3 [3400/4752 (72%)]\tLoss: 0.010121\n",
      "Train Epoch: 3 [3440/4752 (72%)]\tLoss: 0.498048\n",
      "Train Epoch: 3 [3480/4752 (73%)]\tLoss: 0.001204\n",
      "Train Epoch: 3 [3520/4752 (74%)]\tLoss: 0.007585\n",
      "Train Epoch: 3 [3560/4752 (75%)]\tLoss: 0.079905\n",
      "Train Epoch: 3 [3600/4752 (76%)]\tLoss: 0.000071\n",
      "Train Epoch: 3 [3640/4752 (77%)]\tLoss: 0.063766\n",
      "Train Epoch: 3 [3680/4752 (77%)]\tLoss: 0.071246\n",
      "Train Epoch: 3 [3720/4752 (78%)]\tLoss: 0.015146\n",
      "Train Epoch: 3 [3760/4752 (79%)]\tLoss: 0.195760\n",
      "Train Epoch: 3 [3800/4752 (80%)]\tLoss: 0.060739\n",
      "Train Epoch: 3 [3840/4752 (81%)]\tLoss: 0.363610\n",
      "Train Epoch: 3 [3880/4752 (82%)]\tLoss: 0.063301\n",
      "Train Epoch: 3 [3920/4752 (82%)]\tLoss: 0.002224\n",
      "Train Epoch: 3 [3960/4752 (83%)]\tLoss: 0.042452\n",
      "Train Epoch: 3 [4000/4752 (84%)]\tLoss: 0.033530\n",
      "Train Epoch: 3 [4040/4752 (85%)]\tLoss: 0.111745\n",
      "Train Epoch: 3 [4080/4752 (86%)]\tLoss: 0.047638\n",
      "Train Epoch: 3 [4120/4752 (87%)]\tLoss: 0.059051\n",
      "Train Epoch: 3 [4160/4752 (88%)]\tLoss: 0.419661\n",
      "Train Epoch: 3 [4200/4752 (88%)]\tLoss: 0.000200\n",
      "Train Epoch: 3 [4240/4752 (89%)]\tLoss: 0.090588\n",
      "Train Epoch: 3 [4280/4752 (90%)]\tLoss: 0.040557\n",
      "Train Epoch: 3 [4320/4752 (91%)]\tLoss: 0.066673\n",
      "Train Epoch: 3 [4360/4752 (92%)]\tLoss: 0.111638\n",
      "Train Epoch: 3 [4400/4752 (93%)]\tLoss: 0.000129\n",
      "Train Epoch: 3 [4440/4752 (93%)]\tLoss: 0.034055\n",
      "Train Epoch: 3 [4480/4752 (94%)]\tLoss: 0.182475\n",
      "Train Epoch: 3 [4520/4752 (95%)]\tLoss: 0.062560\n",
      "Train Epoch: 3 [4560/4752 (96%)]\tLoss: 0.035135\n",
      "Train Epoch: 3 [4600/4752 (97%)]\tLoss: 0.011787\n",
      "Train Epoch: 3 [4640/4752 (98%)]\tLoss: 0.006394\n",
      "Train Epoch: 3 [4680/4752 (98%)]\tLoss: 0.000205\n",
      "Train Epoch: 3 [4720/4752 (99%)]\tLoss: 0.001024\n",
      "\n",
      "Validation set: Average loss: 0.4872, Accuracy: 1130/1584 (71%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 0.030386\n",
      "Train Epoch: 4 [40/4752 (1%)]\tLoss: 0.009573\n",
      "Train Epoch: 4 [80/4752 (2%)]\tLoss: 0.006550\n",
      "Train Epoch: 4 [120/4752 (3%)]\tLoss: 0.689822\n",
      "Train Epoch: 4 [160/4752 (3%)]\tLoss: 0.000105\n",
      "Train Epoch: 4 [200/4752 (4%)]\tLoss: 0.004038\n",
      "Train Epoch: 4 [240/4752 (5%)]\tLoss: 0.003328\n",
      "Train Epoch: 4 [280/4752 (6%)]\tLoss: 0.001233\n",
      "Train Epoch: 4 [320/4752 (7%)]\tLoss: 0.052048\n",
      "Train Epoch: 4 [360/4752 (8%)]\tLoss: 0.003219\n",
      "Train Epoch: 4 [400/4752 (8%)]\tLoss: 0.204033\n",
      "Train Epoch: 4 [440/4752 (9%)]\tLoss: 0.006973\n",
      "Train Epoch: 4 [480/4752 (10%)]\tLoss: 0.007546\n",
      "Train Epoch: 4 [520/4752 (11%)]\tLoss: 0.000161\n",
      "Train Epoch: 4 [560/4752 (12%)]\tLoss: 0.000286\n",
      "Train Epoch: 4 [600/4752 (13%)]\tLoss: 0.002873\n",
      "Train Epoch: 4 [640/4752 (13%)]\tLoss: 0.037421\n",
      "Train Epoch: 4 [680/4752 (14%)]\tLoss: 0.005776\n",
      "Train Epoch: 4 [720/4752 (15%)]\tLoss: 0.000029\n",
      "Train Epoch: 4 [760/4752 (16%)]\tLoss: 0.007069\n",
      "Train Epoch: 4 [800/4752 (17%)]\tLoss: 0.018500\n",
      "Train Epoch: 4 [840/4752 (18%)]\tLoss: 0.059619\n",
      "Train Epoch: 4 [880/4752 (19%)]\tLoss: 0.000608\n",
      "Train Epoch: 4 [920/4752 (19%)]\tLoss: 0.000301\n",
      "Train Epoch: 4 [960/4752 (20%)]\tLoss: 0.009023\n",
      "Train Epoch: 4 [1000/4752 (21%)]\tLoss: 0.029163\n",
      "Train Epoch: 4 [1040/4752 (22%)]\tLoss: 0.012311\n",
      "Train Epoch: 4 [1080/4752 (23%)]\tLoss: 0.000550\n",
      "Train Epoch: 4 [1120/4752 (24%)]\tLoss: 0.015243\n",
      "Train Epoch: 4 [1160/4752 (24%)]\tLoss: 0.007195\n",
      "Train Epoch: 4 [1200/4752 (25%)]\tLoss: 0.029767\n",
      "Train Epoch: 4 [1240/4752 (26%)]\tLoss: 0.009722\n",
      "Train Epoch: 4 [1280/4752 (27%)]\tLoss: 0.043494\n",
      "Train Epoch: 4 [1320/4752 (28%)]\tLoss: 0.280840\n",
      "Train Epoch: 4 [1360/4752 (29%)]\tLoss: 0.101214\n",
      "Train Epoch: 4 [1400/4752 (29%)]\tLoss: 0.007188\n",
      "Train Epoch: 4 [1440/4752 (30%)]\tLoss: 0.000023\n",
      "Train Epoch: 4 [1480/4752 (31%)]\tLoss: 0.000397\n",
      "Train Epoch: 4 [1520/4752 (32%)]\tLoss: 0.009024\n",
      "Train Epoch: 4 [1560/4752 (33%)]\tLoss: 0.000122\n",
      "Train Epoch: 4 [1600/4752 (34%)]\tLoss: 0.000020\n",
      "Train Epoch: 4 [1640/4752 (35%)]\tLoss: 0.026961\n",
      "Train Epoch: 4 [1680/4752 (35%)]\tLoss: 0.000079\n",
      "Train Epoch: 4 [1720/4752 (36%)]\tLoss: 0.029732\n",
      "Train Epoch: 4 [1760/4752 (37%)]\tLoss: 0.007878\n",
      "Train Epoch: 4 [1800/4752 (38%)]\tLoss: 0.011581\n",
      "Train Epoch: 4 [1840/4752 (39%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [1880/4752 (40%)]\tLoss: 0.002401\n",
      "Train Epoch: 4 [1920/4752 (40%)]\tLoss: 0.000258\n",
      "Train Epoch: 4 [1960/4752 (41%)]\tLoss: 0.109579\n",
      "Train Epoch: 4 [2000/4752 (42%)]\tLoss: 0.001311\n",
      "Train Epoch: 4 [2040/4752 (43%)]\tLoss: 0.000163\n",
      "Train Epoch: 4 [2080/4752 (44%)]\tLoss: 0.001150\n",
      "Train Epoch: 4 [2120/4752 (45%)]\tLoss: 0.017313\n",
      "Train Epoch: 4 [2160/4752 (45%)]\tLoss: 0.022671\n",
      "Train Epoch: 4 [2200/4752 (46%)]\tLoss: 0.197929\n",
      "Train Epoch: 4 [2240/4752 (47%)]\tLoss: 0.005453\n",
      "Train Epoch: 4 [2280/4752 (48%)]\tLoss: 0.004009\n",
      "Train Epoch: 4 [2320/4752 (49%)]\tLoss: 0.050060\n",
      "Train Epoch: 4 [2360/4752 (50%)]\tLoss: 0.000257\n",
      "Train Epoch: 4 [2400/4752 (51%)]\tLoss: 0.000101\n",
      "Train Epoch: 4 [2440/4752 (51%)]\tLoss: 0.000005\n",
      "Train Epoch: 4 [2480/4752 (52%)]\tLoss: 0.011056\n",
      "Train Epoch: 4 [2520/4752 (53%)]\tLoss: 0.106467\n",
      "Train Epoch: 4 [2560/4752 (54%)]\tLoss: 0.194539\n",
      "Train Epoch: 4 [2600/4752 (55%)]\tLoss: 0.000102\n",
      "Train Epoch: 4 [2640/4752 (56%)]\tLoss: 0.007852\n",
      "Train Epoch: 4 [2680/4752 (56%)]\tLoss: 0.000724\n",
      "Train Epoch: 4 [2720/4752 (57%)]\tLoss: 0.000471\n",
      "Train Epoch: 4 [2760/4752 (58%)]\tLoss: 0.000102\n",
      "Train Epoch: 4 [2800/4752 (59%)]\tLoss: 0.010443\n",
      "Train Epoch: 4 [2840/4752 (60%)]\tLoss: 0.019214\n",
      "Train Epoch: 4 [2880/4752 (61%)]\tLoss: 0.002793\n",
      "Train Epoch: 4 [2920/4752 (61%)]\tLoss: 0.000009\n",
      "Train Epoch: 4 [2960/4752 (62%)]\tLoss: 0.000448\n",
      "Train Epoch: 4 [3000/4752 (63%)]\tLoss: 0.140546\n",
      "Train Epoch: 4 [3040/4752 (64%)]\tLoss: 0.071662\n",
      "Train Epoch: 4 [3080/4752 (65%)]\tLoss: 0.001237\n",
      "Train Epoch: 4 [3120/4752 (66%)]\tLoss: 0.023873\n",
      "Train Epoch: 4 [3160/4752 (66%)]\tLoss: 0.002626\n",
      "Train Epoch: 4 [3200/4752 (67%)]\tLoss: 0.601839\n",
      "Train Epoch: 4 [3240/4752 (68%)]\tLoss: 1.059999\n",
      "Train Epoch: 4 [3280/4752 (69%)]\tLoss: 0.036083\n",
      "Train Epoch: 4 [3320/4752 (70%)]\tLoss: 0.000043\n",
      "Train Epoch: 4 [3360/4752 (71%)]\tLoss: 0.004697\n",
      "Train Epoch: 4 [3400/4752 (72%)]\tLoss: 0.016848\n",
      "Train Epoch: 4 [3440/4752 (72%)]\tLoss: 0.000150\n",
      "Train Epoch: 4 [3480/4752 (73%)]\tLoss: 0.000069\n",
      "Train Epoch: 4 [3520/4752 (74%)]\tLoss: 0.135713\n",
      "Train Epoch: 4 [3560/4752 (75%)]\tLoss: 0.000113\n",
      "Train Epoch: 4 [3600/4752 (76%)]\tLoss: 0.018676\n",
      "Train Epoch: 4 [3640/4752 (77%)]\tLoss: 0.014015\n",
      "Train Epoch: 4 [3680/4752 (77%)]\tLoss: 0.001115\n",
      "Train Epoch: 4 [3720/4752 (78%)]\tLoss: 1.087766\n",
      "Train Epoch: 4 [3760/4752 (79%)]\tLoss: 0.764719\n",
      "Train Epoch: 4 [3800/4752 (80%)]\tLoss: 0.013626\n",
      "Train Epoch: 4 [3840/4752 (81%)]\tLoss: 0.004488\n",
      "Train Epoch: 4 [3880/4752 (82%)]\tLoss: 0.000024\n",
      "Train Epoch: 4 [3920/4752 (82%)]\tLoss: 0.175361\n",
      "Train Epoch: 4 [3960/4752 (83%)]\tLoss: 0.000298\n",
      "Train Epoch: 4 [4000/4752 (84%)]\tLoss: 0.002356\n",
      "Train Epoch: 4 [4040/4752 (85%)]\tLoss: 0.677810\n",
      "Train Epoch: 4 [4080/4752 (86%)]\tLoss: 0.000103\n",
      "Train Epoch: 4 [4120/4752 (87%)]\tLoss: 0.005399\n",
      "Train Epoch: 4 [4160/4752 (88%)]\tLoss: 0.107633\n",
      "Train Epoch: 4 [4200/4752 (88%)]\tLoss: 0.025962\n",
      "Train Epoch: 4 [4240/4752 (89%)]\tLoss: 0.075039\n",
      "Train Epoch: 4 [4280/4752 (90%)]\tLoss: 0.484889\n",
      "Train Epoch: 4 [4320/4752 (91%)]\tLoss: 0.013634\n",
      "Train Epoch: 4 [4360/4752 (92%)]\tLoss: 0.017361\n",
      "Train Epoch: 4 [4400/4752 (93%)]\tLoss: 0.020306\n",
      "Train Epoch: 4 [4440/4752 (93%)]\tLoss: 0.451842\n",
      "Train Epoch: 4 [4480/4752 (94%)]\tLoss: 0.028040\n",
      "Train Epoch: 4 [4520/4752 (95%)]\tLoss: 0.008024\n",
      "Train Epoch: 4 [4560/4752 (96%)]\tLoss: 0.003741\n",
      "Train Epoch: 4 [4600/4752 (97%)]\tLoss: 0.000059\n",
      "Train Epoch: 4 [4640/4752 (98%)]\tLoss: 0.220036\n",
      "Train Epoch: 4 [4680/4752 (98%)]\tLoss: 0.000242\n",
      "Train Epoch: 4 [4720/4752 (99%)]\tLoss: 0.004794\n",
      "\n",
      "Validation set: Average loss: 0.4887, Accuracy: 1059/1584 (67%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 0.001520\n",
      "Train Epoch: 5 [40/4752 (1%)]\tLoss: 0.004350\n",
      "Train Epoch: 5 [80/4752 (2%)]\tLoss: 0.003081\n",
      "Train Epoch: 5 [120/4752 (3%)]\tLoss: 0.002111\n",
      "Train Epoch: 5 [160/4752 (3%)]\tLoss: 0.000035\n",
      "Train Epoch: 5 [200/4752 (4%)]\tLoss: 0.000079\n",
      "Train Epoch: 5 [240/4752 (5%)]\tLoss: 0.169220\n",
      "Train Epoch: 5 [280/4752 (6%)]\tLoss: 0.101651\n",
      "Train Epoch: 5 [320/4752 (7%)]\tLoss: 0.010925\n",
      "Train Epoch: 5 [360/4752 (8%)]\tLoss: 0.002023\n",
      "Train Epoch: 5 [400/4752 (8%)]\tLoss: 0.003561\n",
      "Train Epoch: 5 [440/4752 (9%)]\tLoss: 0.002686\n",
      "Train Epoch: 5 [480/4752 (10%)]\tLoss: 0.002592\n",
      "Train Epoch: 5 [520/4752 (11%)]\tLoss: 0.002384\n",
      "Train Epoch: 5 [560/4752 (12%)]\tLoss: 0.012093\n",
      "Train Epoch: 5 [600/4752 (13%)]\tLoss: 0.002499\n",
      "Train Epoch: 5 [640/4752 (13%)]\tLoss: 0.153573\n",
      "Train Epoch: 5 [680/4752 (14%)]\tLoss: 0.024728\n",
      "Train Epoch: 5 [720/4752 (15%)]\tLoss: 0.004620\n",
      "Train Epoch: 5 [760/4752 (16%)]\tLoss: 0.027727\n",
      "Train Epoch: 5 [800/4752 (17%)]\tLoss: 0.019435\n",
      "Train Epoch: 5 [840/4752 (18%)]\tLoss: 0.028621\n",
      "Train Epoch: 5 [880/4752 (19%)]\tLoss: 0.000743\n",
      "Train Epoch: 5 [920/4752 (19%)]\tLoss: 0.000279\n",
      "Train Epoch: 5 [960/4752 (20%)]\tLoss: 0.010655\n",
      "Train Epoch: 5 [1000/4752 (21%)]\tLoss: 0.000728\n",
      "Train Epoch: 5 [1040/4752 (22%)]\tLoss: 0.036628\n",
      "Train Epoch: 5 [1080/4752 (23%)]\tLoss: 0.007070\n",
      "Train Epoch: 5 [1120/4752 (24%)]\tLoss: 0.001426\n",
      "Train Epoch: 5 [1160/4752 (24%)]\tLoss: 0.000044\n",
      "Train Epoch: 5 [1200/4752 (25%)]\tLoss: 0.001398\n",
      "Train Epoch: 5 [1240/4752 (26%)]\tLoss: 0.067523\n",
      "Train Epoch: 5 [1280/4752 (27%)]\tLoss: 0.000054\n",
      "Train Epoch: 5 [1320/4752 (28%)]\tLoss: 0.001376\n",
      "Train Epoch: 5 [1360/4752 (29%)]\tLoss: 0.000043\n",
      "Train Epoch: 5 [1400/4752 (29%)]\tLoss: 0.000289\n",
      "Train Epoch: 5 [1440/4752 (30%)]\tLoss: 0.147512\n",
      "Train Epoch: 5 [1480/4752 (31%)]\tLoss: 0.113545\n",
      "Train Epoch: 5 [1520/4752 (32%)]\tLoss: 0.001689\n",
      "Train Epoch: 5 [1560/4752 (33%)]\tLoss: 0.000169\n",
      "Train Epoch: 5 [1600/4752 (34%)]\tLoss: 0.008127\n",
      "Train Epoch: 5 [1640/4752 (35%)]\tLoss: 0.000029\n",
      "Train Epoch: 5 [1680/4752 (35%)]\tLoss: 0.002591\n",
      "Train Epoch: 5 [1720/4752 (36%)]\tLoss: 0.004847\n",
      "Train Epoch: 5 [1760/4752 (37%)]\tLoss: 0.000382\n",
      "Train Epoch: 5 [1800/4752 (38%)]\tLoss: 0.000064\n",
      "Train Epoch: 5 [1840/4752 (39%)]\tLoss: 0.001198\n",
      "Train Epoch: 5 [1880/4752 (40%)]\tLoss: 0.003558\n",
      "Train Epoch: 5 [1920/4752 (40%)]\tLoss: 0.006301\n",
      "Train Epoch: 5 [1960/4752 (41%)]\tLoss: 0.012287\n",
      "Train Epoch: 5 [2000/4752 (42%)]\tLoss: 0.115258\n",
      "Train Epoch: 5 [2040/4752 (43%)]\tLoss: 0.000559\n",
      "Train Epoch: 5 [2080/4752 (44%)]\tLoss: 0.009872\n",
      "Train Epoch: 5 [2120/4752 (45%)]\tLoss: 0.040868\n",
      "Train Epoch: 5 [2160/4752 (45%)]\tLoss: 0.000392\n",
      "Train Epoch: 5 [2200/4752 (46%)]\tLoss: 0.083382\n",
      "Train Epoch: 5 [2240/4752 (47%)]\tLoss: 0.000278\n",
      "Train Epoch: 5 [2280/4752 (48%)]\tLoss: 0.003461\n",
      "Train Epoch: 5 [2320/4752 (49%)]\tLoss: 0.000229\n",
      "Train Epoch: 5 [2360/4752 (50%)]\tLoss: 0.000199\n",
      "Train Epoch: 5 [2400/4752 (51%)]\tLoss: 0.029981\n",
      "Train Epoch: 5 [2440/4752 (51%)]\tLoss: 0.001774\n",
      "Train Epoch: 5 [2480/4752 (52%)]\tLoss: 0.035767\n",
      "Train Epoch: 5 [2520/4752 (53%)]\tLoss: 0.000099\n",
      "Train Epoch: 5 [2560/4752 (54%)]\tLoss: 0.009008\n",
      "Train Epoch: 5 [2600/4752 (55%)]\tLoss: 0.001387\n",
      "Train Epoch: 5 [2640/4752 (56%)]\tLoss: 0.003217\n",
      "Train Epoch: 5 [2680/4752 (56%)]\tLoss: 0.004293\n",
      "Train Epoch: 5 [2720/4752 (57%)]\tLoss: 0.004075\n",
      "Train Epoch: 5 [2760/4752 (58%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [2800/4752 (59%)]\tLoss: 0.000356\n",
      "Train Epoch: 5 [2840/4752 (60%)]\tLoss: 0.000118\n",
      "Train Epoch: 5 [2880/4752 (61%)]\tLoss: 0.000180\n",
      "Train Epoch: 5 [2920/4752 (61%)]\tLoss: 0.000113\n",
      "Train Epoch: 5 [2960/4752 (62%)]\tLoss: 0.006679\n",
      "Train Epoch: 5 [3000/4752 (63%)]\tLoss: 0.000214\n",
      "Train Epoch: 5 [3040/4752 (64%)]\tLoss: 0.000119\n",
      "Train Epoch: 5 [3080/4752 (65%)]\tLoss: 0.000036\n",
      "Train Epoch: 5 [3120/4752 (66%)]\tLoss: 0.011191\n",
      "Train Epoch: 5 [3160/4752 (66%)]\tLoss: 0.000438\n",
      "Train Epoch: 5 [3200/4752 (67%)]\tLoss: 0.000025\n",
      "Train Epoch: 5 [3240/4752 (68%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [3280/4752 (69%)]\tLoss: 0.590602\n",
      "Train Epoch: 5 [3320/4752 (70%)]\tLoss: 0.000043\n",
      "Train Epoch: 5 [3360/4752 (71%)]\tLoss: 0.008609\n",
      "Train Epoch: 5 [3400/4752 (72%)]\tLoss: 0.000052\n",
      "Train Epoch: 5 [3440/4752 (72%)]\tLoss: 0.000004\n",
      "Train Epoch: 5 [3480/4752 (73%)]\tLoss: 0.202715\n",
      "Train Epoch: 5 [3520/4752 (74%)]\tLoss: 0.003704\n",
      "Train Epoch: 5 [3560/4752 (75%)]\tLoss: 0.000079\n",
      "Train Epoch: 5 [3600/4752 (76%)]\tLoss: 0.088048\n",
      "Train Epoch: 5 [3640/4752 (77%)]\tLoss: 0.000026\n",
      "Train Epoch: 5 [3680/4752 (77%)]\tLoss: 0.010828\n",
      "Train Epoch: 5 [3720/4752 (78%)]\tLoss: 0.000019\n",
      "Train Epoch: 5 [3760/4752 (79%)]\tLoss: 0.000113\n",
      "Train Epoch: 5 [3800/4752 (80%)]\tLoss: 0.001093\n",
      "Train Epoch: 5 [3840/4752 (81%)]\tLoss: 0.000338\n",
      "Train Epoch: 5 [3880/4752 (82%)]\tLoss: 0.000080\n",
      "Train Epoch: 5 [3920/4752 (82%)]\tLoss: 0.014802\n",
      "Train Epoch: 5 [3960/4752 (83%)]\tLoss: 0.018391\n",
      "Train Epoch: 5 [4000/4752 (84%)]\tLoss: 0.643226\n",
      "Train Epoch: 5 [4040/4752 (85%)]\tLoss: 0.000047\n",
      "Train Epoch: 5 [4080/4752 (86%)]\tLoss: 0.001182\n",
      "Train Epoch: 5 [4120/4752 (87%)]\tLoss: 0.027682\n",
      "Train Epoch: 5 [4160/4752 (88%)]\tLoss: 0.026324\n",
      "Train Epoch: 5 [4200/4752 (88%)]\tLoss: 0.000666\n",
      "Train Epoch: 5 [4240/4752 (89%)]\tLoss: 0.015764\n",
      "Train Epoch: 5 [4280/4752 (90%)]\tLoss: 0.035406\n",
      "Train Epoch: 5 [4320/4752 (91%)]\tLoss: 0.002618\n",
      "Train Epoch: 5 [4360/4752 (92%)]\tLoss: 0.003471\n",
      "Train Epoch: 5 [4400/4752 (93%)]\tLoss: 0.126874\n",
      "Train Epoch: 5 [4440/4752 (93%)]\tLoss: 0.000381\n",
      "Train Epoch: 5 [4480/4752 (94%)]\tLoss: 0.000011\n",
      "Train Epoch: 5 [4520/4752 (95%)]\tLoss: 0.016857\n",
      "Train Epoch: 5 [4560/4752 (96%)]\tLoss: 0.003930\n",
      "Train Epoch: 5 [4600/4752 (97%)]\tLoss: 0.036736\n",
      "Train Epoch: 5 [4640/4752 (98%)]\tLoss: 0.000189\n",
      "Train Epoch: 5 [4680/4752 (98%)]\tLoss: 1.750939\n",
      "Train Epoch: 5 [4720/4752 (99%)]\tLoss: 0.000010\n",
      "\n",
      "Validation set: Average loss: 0.6927, Accuracy: 1099/1584 (69%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6822, Accuracy: 1094/1584 (69%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters and Hyperparameters\n",
    "vocabulary_size = 100000  # to adjust \n",
    "sequence_length = 50  # to adjust \n",
    "embedding_dim = 128\n",
    "hidden_dim = 64\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, optimizer, and device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN(vocabulary_size, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    validate(model, device, validation_loader)\n",
    "\n",
    "# After training, evaluate on the test set\n",
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-03 09:46:29,731] A new study created in memory with name: no-name-c39a1b7e-8f1a-4ba3-bfb5-4fa0c529b750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.673930\n",
      "Train Epoch: 1 [320/4752 (7%)]\tLoss: 0.643635\n",
      "Train Epoch: 1 [640/4752 (13%)]\tLoss: 0.434858\n",
      "Train Epoch: 1 [960/4752 (20%)]\tLoss: 0.308697\n",
      "Train Epoch: 1 [1280/4752 (27%)]\tLoss: 0.477517\n",
      "Train Epoch: 1 [1600/4752 (34%)]\tLoss: 0.464989\n",
      "Train Epoch: 1 [1920/4752 (40%)]\tLoss: 0.341352\n",
      "Train Epoch: 1 [2240/4752 (47%)]\tLoss: 0.323668\n",
      "Train Epoch: 1 [2560/4752 (54%)]\tLoss: 0.212230\n",
      "Train Epoch: 1 [2880/4752 (60%)]\tLoss: 0.249452\n",
      "Train Epoch: 1 [3200/4752 (67%)]\tLoss: 0.474155\n",
      "Train Epoch: 1 [3520/4752 (74%)]\tLoss: 0.287952\n",
      "Train Epoch: 1 [3840/4752 (81%)]\tLoss: 0.261109\n",
      "Train Epoch: 1 [4160/4752 (87%)]\tLoss: 0.261582\n",
      "Train Epoch: 1 [4480/4752 (94%)]\tLoss: 0.387405\n",
      "\n",
      "Validation set: Average loss: 0.0194, Accuracy: 1161/1584 (73%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 0.292701\n",
      "Train Epoch: 2 [320/4752 (7%)]\tLoss: 0.154585\n",
      "Train Epoch: 2 [640/4752 (13%)]\tLoss: 0.195275\n",
      "Train Epoch: 2 [960/4752 (20%)]\tLoss: 0.222274\n",
      "Train Epoch: 2 [1280/4752 (27%)]\tLoss: 0.286767\n",
      "Train Epoch: 2 [1600/4752 (34%)]\tLoss: 0.411844\n",
      "Train Epoch: 2 [1920/4752 (40%)]\tLoss: 0.525804\n",
      "Train Epoch: 2 [2240/4752 (47%)]\tLoss: 0.240545\n",
      "Train Epoch: 2 [2560/4752 (54%)]\tLoss: 0.310881\n",
      "Train Epoch: 2 [2880/4752 (60%)]\tLoss: 0.223943\n",
      "Train Epoch: 2 [3200/4752 (67%)]\tLoss: 0.172460\n",
      "Train Epoch: 2 [3520/4752 (74%)]\tLoss: 0.247100\n",
      "Train Epoch: 2 [3840/4752 (81%)]\tLoss: 0.346743\n",
      "Train Epoch: 2 [4160/4752 (87%)]\tLoss: 0.293795\n",
      "Train Epoch: 2 [4480/4752 (94%)]\tLoss: 0.120082\n",
      "\n",
      "Validation set: Average loss: 0.0303, Accuracy: 1160/1584 (73%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 0.129683\n",
      "Train Epoch: 3 [320/4752 (7%)]\tLoss: 0.138883\n",
      "Train Epoch: 3 [640/4752 (13%)]\tLoss: 0.173323\n",
      "Train Epoch: 3 [960/4752 (20%)]\tLoss: 0.160399\n",
      "Train Epoch: 3 [1280/4752 (27%)]\tLoss: 0.076437\n",
      "Train Epoch: 3 [1600/4752 (34%)]\tLoss: 0.105902\n",
      "Train Epoch: 3 [1920/4752 (40%)]\tLoss: 0.073023\n",
      "Train Epoch: 3 [2240/4752 (47%)]\tLoss: 0.190078\n",
      "Train Epoch: 3 [2560/4752 (54%)]\tLoss: 0.025119\n",
      "Train Epoch: 3 [2880/4752 (60%)]\tLoss: 0.096467\n",
      "Train Epoch: 3 [3200/4752 (67%)]\tLoss: 0.204638\n",
      "Train Epoch: 3 [3520/4752 (74%)]\tLoss: 0.063817\n",
      "Train Epoch: 3 [3840/4752 (81%)]\tLoss: 0.309163\n",
      "Train Epoch: 3 [4160/4752 (87%)]\tLoss: 0.214184\n",
      "Train Epoch: 3 [4480/4752 (94%)]\tLoss: 0.043798\n",
      "\n",
      "Validation set: Average loss: 0.0413, Accuracy: 1109/1584 (70%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 0.023840\n",
      "Train Epoch: 4 [320/4752 (7%)]\tLoss: 0.004943\n",
      "Train Epoch: 4 [640/4752 (13%)]\tLoss: 0.222510\n",
      "Train Epoch: 4 [960/4752 (20%)]\tLoss: 0.067361\n",
      "Train Epoch: 4 [1280/4752 (27%)]\tLoss: 0.008664\n",
      "Train Epoch: 4 [1600/4752 (34%)]\tLoss: 0.009538\n",
      "Train Epoch: 4 [1920/4752 (40%)]\tLoss: 0.102816\n",
      "Train Epoch: 4 [2240/4752 (47%)]\tLoss: 0.017865\n",
      "Train Epoch: 4 [2560/4752 (54%)]\tLoss: 0.031878\n",
      "Train Epoch: 4 [2880/4752 (60%)]\tLoss: 0.091762\n",
      "Train Epoch: 4 [3200/4752 (67%)]\tLoss: 0.073293\n",
      "Train Epoch: 4 [3520/4752 (74%)]\tLoss: 0.031223\n",
      "Train Epoch: 4 [3840/4752 (81%)]\tLoss: 0.043493\n",
      "Train Epoch: 4 [4160/4752 (87%)]\tLoss: 0.078887\n",
      "Train Epoch: 4 [4480/4752 (94%)]\tLoss: 0.008805\n",
      "\n",
      "Validation set: Average loss: 0.0538, Accuracy: 1139/1584 (72%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 0.013516\n",
      "Train Epoch: 5 [320/4752 (7%)]\tLoss: 0.010971\n",
      "Train Epoch: 5 [640/4752 (13%)]\tLoss: 0.004868\n",
      "Train Epoch: 5 [960/4752 (20%)]\tLoss: 0.111072\n",
      "Train Epoch: 5 [1280/4752 (27%)]\tLoss: 0.013443\n",
      "Train Epoch: 5 [1600/4752 (34%)]\tLoss: 0.001923\n",
      "Train Epoch: 5 [1920/4752 (40%)]\tLoss: 0.002811\n",
      "Train Epoch: 5 [2240/4752 (47%)]\tLoss: 0.010180\n",
      "Train Epoch: 5 [2560/4752 (54%)]\tLoss: 0.001968\n",
      "Train Epoch: 5 [2880/4752 (60%)]\tLoss: 0.001864\n",
      "Train Epoch: 5 [3200/4752 (67%)]\tLoss: 0.017261\n",
      "Train Epoch: 5 [3520/4752 (74%)]\tLoss: 0.001546\n",
      "Train Epoch: 5 [3840/4752 (81%)]\tLoss: 0.003208\n",
      "Train Epoch: 5 [4160/4752 (87%)]\tLoss: 0.405886\n",
      "Train Epoch: 5 [4480/4752 (94%)]\tLoss: 0.002644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-03 09:47:06,480] Trial 0 finished with value: -0.6641414141414141 and parameters: {'batch_size': 32, 'learning_rate': 0.0040841757800848715, 'embedding_dim': 64, 'hidden_dim': 128, 'optimizer': 'RMSprop', 'dropout_rate': 0.14956987091624135, 'step_size': 65, 'gamma': 0.697399587361881, 'sequence_length': 100}. Best is trial 0 with value: -0.6641414141414141.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.0616, Accuracy: 1052/1584 (66%)\n",
      "\n",
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.713973\n",
      "Train Epoch: 1 [2560/4752 (53%)]\tLoss: 0.425443\n",
      "\n",
      "Validation set: Average loss: 0.0037, Accuracy: 1076/1584 (68%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 0.255112\n",
      "Train Epoch: 2 [2560/4752 (53%)]\tLoss: 0.258602\n",
      "\n",
      "Validation set: Average loss: 0.0044, Accuracy: 1175/1584 (74%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 0.134875\n",
      "Train Epoch: 3 [2560/4752 (53%)]\tLoss: 0.111272\n",
      "\n",
      "Validation set: Average loss: 0.0085, Accuracy: 1129/1584 (71%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 0.032842\n",
      "Train Epoch: 4 [2560/4752 (53%)]\tLoss: 0.055920\n",
      "\n",
      "Validation set: Average loss: 0.0106, Accuracy: 1112/1584 (70%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 0.022660\n",
      "Train Epoch: 5 [2560/4752 (53%)]\tLoss: 0.015619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-03 09:47:18,104] Trial 1 finished with value: -0.7070707070707071 and parameters: {'batch_size': 256, 'learning_rate': 0.04459353601444581, 'embedding_dim': 128, 'hidden_dim': 32, 'optimizer': 'Adam', 'dropout_rate': 0.4216681752268144, 'step_size': 75, 'gamma': 0.7140159487425279, 'sequence_length': 400}. Best is trial 1 with value: -0.7070707070707071.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.0126, Accuracy: 1120/1584 (71%)\n",
      "\n",
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.679411\n",
      "Train Epoch: 1 [320/4752 (7%)]\tLoss: 0.688376\n",
      "Train Epoch: 1 [640/4752 (13%)]\tLoss: 0.676539\n",
      "Train Epoch: 1 [960/4752 (20%)]\tLoss: 0.679445\n",
      "Train Epoch: 1 [1280/4752 (27%)]\tLoss: 0.687975\n",
      "Train Epoch: 1 [1600/4752 (34%)]\tLoss: 0.669257\n",
      "Train Epoch: 1 [1920/4752 (40%)]\tLoss: 0.673290\n",
      "Train Epoch: 1 [2240/4752 (47%)]\tLoss: 0.682977\n",
      "Train Epoch: 1 [2560/4752 (54%)]\tLoss: 0.671448\n",
      "Train Epoch: 1 [2880/4752 (60%)]\tLoss: 0.671334\n",
      "Train Epoch: 1 [3200/4752 (67%)]\tLoss: 0.665516\n",
      "Train Epoch: 1 [3520/4752 (74%)]\tLoss: 0.658782\n",
      "Train Epoch: 1 [3840/4752 (81%)]\tLoss: 0.655443\n",
      "Train Epoch: 1 [4160/4752 (87%)]\tLoss: 0.668579\n",
      "Train Epoch: 1 [4480/4752 (94%)]\tLoss: 0.671686\n",
      "\n",
      "Validation set: Average loss: 0.0210, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 0.687995\n",
      "Train Epoch: 2 [320/4752 (7%)]\tLoss: 0.682627\n",
      "Train Epoch: 2 [640/4752 (13%)]\tLoss: 0.646507\n",
      "Train Epoch: 2 [960/4752 (20%)]\tLoss: 0.658427\n",
      "Train Epoch: 2 [1280/4752 (27%)]\tLoss: 0.640018\n",
      "Train Epoch: 2 [1600/4752 (34%)]\tLoss: 0.633417\n",
      "Train Epoch: 2 [1920/4752 (40%)]\tLoss: 0.643585\n",
      "Train Epoch: 2 [2240/4752 (47%)]\tLoss: 0.677426\n",
      "Train Epoch: 2 [2560/4752 (54%)]\tLoss: 0.649632\n",
      "Train Epoch: 2 [2880/4752 (60%)]\tLoss: 0.643641\n",
      "Train Epoch: 2 [3200/4752 (67%)]\tLoss: 0.657758\n",
      "Train Epoch: 2 [3520/4752 (74%)]\tLoss: 0.656205\n",
      "Train Epoch: 2 [3840/4752 (81%)]\tLoss: 0.652441\n",
      "Train Epoch: 2 [4160/4752 (87%)]\tLoss: 0.662355\n",
      "Train Epoch: 2 [4480/4752 (94%)]\tLoss: 0.658802\n",
      "\n",
      "Validation set: Average loss: 0.0205, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 0.661200\n",
      "Train Epoch: 3 [320/4752 (7%)]\tLoss: 0.672840\n",
      "Train Epoch: 3 [640/4752 (13%)]\tLoss: 0.640566\n",
      "Train Epoch: 3 [960/4752 (20%)]\tLoss: 0.637260\n",
      "Train Epoch: 3 [1280/4752 (27%)]\tLoss: 0.652051\n",
      "Train Epoch: 3 [1600/4752 (34%)]\tLoss: 0.642364\n",
      "Train Epoch: 3 [1920/4752 (40%)]\tLoss: 0.639215\n",
      "Train Epoch: 3 [2240/4752 (47%)]\tLoss: 0.619691\n",
      "Train Epoch: 3 [2560/4752 (54%)]\tLoss: 0.623005\n",
      "Train Epoch: 3 [2880/4752 (60%)]\tLoss: 0.640121\n",
      "Train Epoch: 3 [3200/4752 (67%)]\tLoss: 0.630117\n",
      "Train Epoch: 3 [3520/4752 (74%)]\tLoss: 0.660528\n",
      "Train Epoch: 3 [3840/4752 (81%)]\tLoss: 0.620274\n",
      "Train Epoch: 3 [4160/4752 (87%)]\tLoss: 0.627137\n",
      "Train Epoch: 3 [4480/4752 (94%)]\tLoss: 0.652472\n",
      "\n",
      "Validation set: Average loss: 0.0201, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 0.610913\n",
      "Train Epoch: 4 [320/4752 (7%)]\tLoss: 0.663053\n",
      "Train Epoch: 4 [640/4752 (13%)]\tLoss: 0.641992\n",
      "Train Epoch: 4 [960/4752 (20%)]\tLoss: 0.596796\n",
      "Train Epoch: 4 [1280/4752 (27%)]\tLoss: 0.668497\n",
      "Train Epoch: 4 [1600/4752 (34%)]\tLoss: 0.638891\n",
      "Train Epoch: 4 [1920/4752 (40%)]\tLoss: 0.661416\n",
      "Train Epoch: 4 [2240/4752 (47%)]\tLoss: 0.674667\n",
      "Train Epoch: 4 [2560/4752 (54%)]\tLoss: 0.647723\n",
      "Train Epoch: 4 [2880/4752 (60%)]\tLoss: 0.625004\n",
      "Train Epoch: 4 [3200/4752 (67%)]\tLoss: 0.637179\n",
      "Train Epoch: 4 [3520/4752 (74%)]\tLoss: 0.634925\n",
      "Train Epoch: 4 [3840/4752 (81%)]\tLoss: 0.583260\n",
      "Train Epoch: 4 [4160/4752 (87%)]\tLoss: 0.657183\n",
      "Train Epoch: 4 [4480/4752 (94%)]\tLoss: 0.638838\n",
      "\n",
      "Validation set: Average loss: 0.0197, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 0.580923\n",
      "Train Epoch: 5 [320/4752 (7%)]\tLoss: 0.607929\n",
      "Train Epoch: 5 [640/4752 (13%)]\tLoss: 0.624135\n",
      "Train Epoch: 5 [960/4752 (20%)]\tLoss: 0.596888\n",
      "Train Epoch: 5 [1280/4752 (27%)]\tLoss: 0.563614\n",
      "Train Epoch: 5 [1600/4752 (34%)]\tLoss: 0.580874\n",
      "Train Epoch: 5 [1920/4752 (40%)]\tLoss: 0.632430\n",
      "Train Epoch: 5 [2240/4752 (47%)]\tLoss: 0.594872\n",
      "Train Epoch: 5 [2560/4752 (54%)]\tLoss: 0.630939\n",
      "Train Epoch: 5 [2880/4752 (60%)]\tLoss: 0.593459\n",
      "Train Epoch: 5 [3200/4752 (67%)]\tLoss: 0.643192\n",
      "Train Epoch: 5 [3520/4752 (74%)]\tLoss: 0.661372\n",
      "Train Epoch: 5 [3840/4752 (81%)]\tLoss: 0.642143\n",
      "Train Epoch: 5 [4160/4752 (87%)]\tLoss: 0.591360\n",
      "Train Epoch: 5 [4480/4752 (94%)]\tLoss: 0.615016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-03 09:47:28,616] Trial 2 finished with value: -0.7462121212121212 and parameters: {'batch_size': 32, 'learning_rate': 0.0011275449657103711, 'embedding_dim': 64, 'hidden_dim': 32, 'optimizer': 'SGD', 'dropout_rate': 0.1602001763971111, 'step_size': 50, 'gamma': 0.20441284569584917, 'sequence_length': 50}. Best is trial 2 with value: -0.7462121212121212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.0194, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.700930\n",
      "Train Epoch: 1 [1280/4752 (26%)]\tLoss: 0.373853\n",
      "Train Epoch: 1 [2560/4752 (53%)]\tLoss: 0.429301\n",
      "Train Epoch: 1 [3840/4752 (79%)]\tLoss: 0.331182\n",
      "\n",
      "Validation set: Average loss: 0.0083, Accuracy: 1171/1584 (74%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 0.292256\n",
      "Train Epoch: 2 [1280/4752 (26%)]\tLoss: 0.283287\n",
      "Train Epoch: 2 [2560/4752 (53%)]\tLoss: 0.213158\n",
      "Train Epoch: 2 [3840/4752 (79%)]\tLoss: 0.305714\n",
      "\n",
      "Validation set: Average loss: 0.0089, Accuracy: 1132/1584 (71%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 0.080787\n",
      "Train Epoch: 3 [1280/4752 (26%)]\tLoss: 0.108782\n",
      "Train Epoch: 3 [2560/4752 (53%)]\tLoss: 0.175132\n",
      "Train Epoch: 3 [3840/4752 (79%)]\tLoss: 0.080312\n",
      "\n",
      "Validation set: Average loss: 0.0127, Accuracy: 1141/1584 (72%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 0.052149\n",
      "Train Epoch: 4 [1280/4752 (26%)]\tLoss: 0.058075\n",
      "Train Epoch: 4 [2560/4752 (53%)]\tLoss: 0.048793\n",
      "Train Epoch: 4 [3840/4752 (79%)]\tLoss: 0.036634\n",
      "\n",
      "Validation set: Average loss: 0.0192, Accuracy: 1151/1584 (73%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 0.011342\n",
      "Train Epoch: 5 [1280/4752 (26%)]\tLoss: 0.009673\n",
      "Train Epoch: 5 [2560/4752 (53%)]\tLoss: 0.010235\n",
      "Train Epoch: 5 [3840/4752 (79%)]\tLoss: 0.003925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-03 09:47:38,352] Trial 3 finished with value: -0.7146464646464646 and parameters: {'batch_size': 128, 'learning_rate': 0.007099906584103223, 'embedding_dim': 64, 'hidden_dim': 64, 'optimizer': 'RMSprop', 'dropout_rate': 0.04148977871021642, 'step_size': 40, 'gamma': 0.2859282500341737, 'sequence_length': 200}. Best is trial 2 with value: -0.7462121212121212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.0221, Accuracy: 1132/1584 (71%)\n",
      "\n",
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.684922\n",
      "Train Epoch: 1 [2560/4752 (53%)]\tLoss: 0.680966\n",
      "\n",
      "Validation set: Average loss: 0.0030, Accuracy: 1183/1584 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 0.665744\n",
      "Train Epoch: 2 [2560/4752 (53%)]\tLoss: 0.659528\n",
      "\n",
      "Validation set: Average loss: 0.0029, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 0.640974\n",
      "Train Epoch: 3 [2560/4752 (53%)]\tLoss: 0.636886\n",
      "\n",
      "Validation set: Average loss: 0.0028, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 0.635670\n",
      "Train Epoch: 4 [2560/4752 (53%)]\tLoss: 0.598922\n",
      "\n",
      "Validation set: Average loss: 0.0028, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 0.617016\n",
      "Train Epoch: 5 [2560/4752 (53%)]\tLoss: 0.597555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-03 09:47:52,344] Trial 4 finished with value: -0.7462121212121212 and parameters: {'batch_size': 256, 'learning_rate': 0.00015019108257404234, 'embedding_dim': 128, 'hidden_dim': 64, 'optimizer': 'Adam', 'dropout_rate': 0.47275307451036497, 'step_size': 82, 'gamma': 0.14813048695866554, 'sequence_length': 100}. Best is trial 2 with value: -0.7462121212121212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.0026, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Number of finished trials: 5\n",
      "Best trial:\n",
      "Best Validation Loss: 0.0194210239281558\n",
      "Best Validation Accuracy: 0.7462121212121212\n",
      "Best Trial Parameters:\n",
      "    batch_size: 32\n",
      "    learning_rate: 0.0011275449657103711\n",
      "    embedding_dim: 64\n",
      "    hidden_dim: 32\n",
      "    optimizer: SGD\n",
      "    dropout_rate: 0.1602001763971111\n",
      "    step_size: 50\n",
      "    gamma: 0.20441284569584917\n",
      "    sequence_length: 50\n"
     ]
    }
   ],
   "source": [
    "# Parameters and Hyperparameters\n",
    "n_trials=5\n",
    "num_classes = 2\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space\n",
    "    # vocabulary_size = trial.suggest_categorical('vocabulary_size', [5000, 10000, 20000, 40000])\n",
    "    vocabulary_size = 100000\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    embedding_dim = trial.suggest_categorical('embedding_dim', [64, 128, 256])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [32, 64, 128])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    step_size = trial.suggest_int('step_size', 1, 100)\n",
    "    gamma = trial.suggest_float('gamma', 0.1, 1.0, log=True)\n",
    "    sequence_length = trial.suggest_categorical('sequence_length', [50, 100, 200, 400])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Model setup with trial suggestions\n",
    "    model = DQN(vocabulary_size, embedding_dim, hidden_dim, num_classes, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 5  # Reduced for faster optimization cycles\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        val_loss, val_accuracy = validate(model, device, validation_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Set custom attributes for the trial\n",
    "    trial.set_user_attr(\"val_loss\", val_loss)\n",
    "    trial.set_user_attr(\"val_accuracy\", val_accuracy)\n",
    "    \n",
    "    # print(f\"Returning from validate: val_loss={val_loss}, val_accuracy={val_accuracy}\")\n",
    "    # return val_loss\n",
    "\n",
    "    # Objective: maximize validation accuracy by minimizing its negative value\n",
    "    return -val_accuracy  # Return the negative accuracy\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials)  # Number of trials can be adjusted\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "# Retrieve the validation loss and accuracy from the best trial\n",
    "best_val_loss = trial.user_attrs[\"val_loss\"]\n",
    "best_val_accuracy = trial.user_attrs[\"val_accuracy\"]\n",
    "\n",
    "print(f'Best Validation Loss: {best_val_loss}')\n",
    "print(f'Best Validation Accuracy: {best_val_accuracy}')\n",
    "print('Best Trial Parameters:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
