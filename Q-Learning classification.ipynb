{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock dataset for demonstration\n",
    "class TextDataset(Dataset):\n",
    "    # def __init__(self, vocabulary_size, sequence_length, num_samples):\n",
    "    #     self.data = torch.randint(0, vocabulary_size, (num_samples, sequence_length))\n",
    "    #     self.labels = torch.randint(0, 2, (num_samples,))\n",
    "    def __init__(self, texts, labels, sequence_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab = self.build_vocab(texts)\n",
    "        self.encoded_texts = [self.encode_text(text) for text in texts]\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        unique_words = set(word for text in texts for word in text.lower().split())\n",
    "        vocab = {word: i + 1 for i, word in enumerate(unique_words)}  # +1 for padding token at index 0\n",
    "        return vocab\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        return [self.vocab.get(word, 0) for word in text.lower().split()][:self.sequence_length] + [0] * (self.sequence_length - len(text.split()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.encoded_texts[idx]), torch.tensor(self.labels[idx])\n",
    "    \n",
    "\n",
    "# Define the Q-network model\n",
    "class QLearning(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim, hidden_dim, dropout_rate=0.5):\n",
    "        super(QLearning, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # Two actions: 0 or 1\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.dropout(lstm_out[:, -1])\n",
    "        q_values = self.fc(out)\n",
    "        return q_values\n",
    "\n",
    "def update_q_values(model, optimizer, states, actions, rewards, next_states, gamma=0.99):\n",
    "    # Ensure actions tensor is in the correct shape for .gather()\n",
    "    actions = actions.long().unsqueeze(-1)  # Adding a dimension to match q_values dimensions for gather\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get max Q value for next states, for all next states in the batch\n",
    "        q_values_next = model(next_states).max(1)[0]\n",
    "    model.train()\n",
    "\n",
    "    # Get the predicted Q-values for the chosen actions. \n",
    "    # As actions are now correctly unsqueezed, this should work without issues.\n",
    "    # q_values = model(states).gather(1, actions).squeeze(-1)  # squeeze(-1) to remove the extra dimension after gather\n",
    "    q_values = model(states).gather(1, actions.unsqueeze(-1))\n",
    "    \n",
    "    # Calculate the target Q values for the current states\n",
    "    # This is the immediate reward plus the discounted max future Q value\n",
    "    # Note: We detach q_values_next from the graph as it's a target, not a variable we need gradients for\n",
    "    target = rewards + gamma * q_values_next.detach()\n",
    "    \n",
    "    # Compute the loss between current Q values (for the chosen actions) and the target Q values\n",
    "    loss = nn.functional.mse_loss(q_values, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, gamma=0.99, log_interval=10):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict Q-values for current states (data)\n",
    "        current_q_values = model(data)\n",
    "\n",
    "        # Simulate taking actions (classifying) and receiving rewards\n",
    "        # In a real RL scenario, actions are taken based on a policy. Here, we simplify.\n",
    "        _, predicted_actions = torch.max(current_q_values, 1)\n",
    "        rewards = (predicted_actions == target).float()  # Reward is 1 for correct classification, 0 otherwise\n",
    "\n",
    "        # Simulate next states (in practice, you might use a different strategy)\n",
    "        # For simplicity, let's just roll data to simulate \"next states\"\n",
    "        next_data = torch.roll(data, -1, 0)\n",
    "        with torch.no_grad():\n",
    "            future_q_values = model(next_data).max(1)[0]  # Use max Q-value for simplicity\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        target_q_values = rewards + gamma * future_q_values\n",
    "\n",
    "        # Update model based on the difference between current Q-values and target Q-values\n",
    "        # Assuming binary classification, gather Q-values for the taken actions\n",
    "        action_indices = target.view(-1, 1).long()\n",
    "        gathered_q_values = current_q_values.gather(1, action_indices).squeeze()\n",
    "\n",
    "        loss = nn.functional.mse_loss(gathered_q_values, target_q_values.detach())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
    "                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# def train(model, device, train_loader, optimizer, epoch, gamma=0.99, log_interval=10):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Predict Q-values for current states (data)\n",
    "#         current_q_values = model(data)\n",
    "\n",
    "#         # Simulate taking actions (classifying) and receiving rewards\n",
    "#         _, predicted_actions = torch.max(current_q_values, 1)\n",
    "#         rewards = (predicted_actions == target).float()  # Reward is 1 for correct classification, 0 otherwise\n",
    "\n",
    "#         # Simulate next states (in practice, you might use a different strategy)\n",
    "#         next_data = torch.roll(data, -1, 0)\n",
    "#         with torch.no_grad():\n",
    "#             future_q_values = model(next_data).max(1)[0]\n",
    "\n",
    "#         # Here we manually construct what we need for update_q_values\n",
    "#         actions = predicted_actions.view(-1, 1)\n",
    "#         rewards = rewards.unsqueeze(-1)\n",
    "#         next_states = next_data\n",
    "\n",
    "#         # Update Q-values using the custom function, adapted to fit our setup\n",
    "#         # Note: update_q_values function is assumed to be adapted for our context\n",
    "#         loss = update_q_values(model, optimizer, data, actions, rewards, next_states, gamma)\n",
    "#         total_loss += loss\n",
    "\n",
    "#         if batch_idx % log_interval == 0:\n",
    "#             print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
    "#                   f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {total_loss / (batch_idx + 1):.6f}')\n",
    "\n",
    "def validate(model, device, validation_loader):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in validation_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            validation_loss += nn.CrossEntropyLoss()(output, target).item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    validation_loss /= len(validation_loader.dataset)\n",
    "    validation_acc = correct / len(validation_loader.dataset)\n",
    "    print(f'\\nValidation set: Average loss: {validation_loss:.4f}, Accuracy: {correct}/{len(validation_loader.dataset)} ({100. * correct / len(validation_loader.dataset):.0f}%)\\n')\n",
    "    return validation_loss, validation_acc\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Encoded text: tensor([[20655, 18571, 13064,  3125, 19348, 13014, 16230, 13924, 20400, 11550],\n",
      "        [14625, 10428,  3125, 12991, 15537, 10920, 17104, 13213, 11874, 19899],\n",
      "        [ 8654, 14999, 20606,  8966, 18599,  9186, 10363, 10770,   925,  4179],\n",
      "        [ 1111,  5026, 12025, 13896, 19397,  8211,  3350, 18458, 15633, 17776]])\n",
      "Validation Label: tensor([0, 1, 1, 1])\n",
      "Validation Encoded text: tensor([[2026, 5854, 5043, 5005, 1440,  150, 5999, 5785, 8586,  248],\n",
      "        [5434, 7852, 1896, 5684, 4545, 3561, 5345, 3043,   45,  924],\n",
      "        [6821,  293, 7617, 1422, 8655, 2273, 2014, 4048, 2961, 7476],\n",
      "        [5434, 1531, 1896,  411, 7369, 6574,  285, 1568, 3410, 6618]])\n",
      "Validation Label: tensor([1, 1, 1, 1])\n",
      "Test Encoded text: tensor([[6332, 6864, 6903, 2554, 6857, 7206, 3527, 4826, 8675, 8933],\n",
      "        [3233, 5512, 5310, 5005, 4409, 2170, 5859, 1313, 4221, 1449],\n",
      "        [6673, 3527, 8940, 4598, 5074, 5172, 7242, 6581, 2468, 2550],\n",
      "        [9078, 4781, 8736, 3098, 5001, 2220, 8464, 7799, 6855, 7430]])\n",
      "Test Label: tensor([1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/sentiment_analysis.csv')\n",
    "\n",
    "# Extracting texts and labels\n",
    "texts = df['tweet'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Splitting dataset into train+val and test\n",
    "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting train+val into train and val\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(train_val_texts, train_val_labels, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Creating datasets\n",
    "sequence_length = 10  # Max number of words in a text\n",
    "train_dataset = TextDataset(train_texts, train_labels, sequence_length)\n",
    "validation_dataset = TextDataset(validation_texts, validation_labels, sequence_length)\n",
    "test_dataset = TextDataset(test_texts, test_labels, sequence_length)\n",
    "\n",
    "# Creating DataLoaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for data, label in train_loader:\n",
    "    print(f\"Validation Encoded text: {data}\")\n",
    "    print(f\"Validation Label: {label}\")\n",
    "    break  # Just show one batch for brevity\n",
    "\n",
    "for data, label in validation_loader:\n",
    "    print(f\"Validation Encoded text: {data}\")\n",
    "    print(f\"Validation Label: {label}\")\n",
    "    break  # Just show one batch for brevity\n",
    "\n",
    "for data, label in test_loader:\n",
    "    print(f\"Test Encoded text: {data}\")\n",
    "    print(f\"Test Label: {label}\")\n",
    "    break  # Just show one batch for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 1.008337\n",
      "Train Epoch: 1 [40/4752 (1%)]\tLoss: 0.580762\n",
      "Train Epoch: 1 [80/4752 (2%)]\tLoss: 0.562247\n",
      "Train Epoch: 1 [120/4752 (3%)]\tLoss: 1.281242\n",
      "Train Epoch: 1 [160/4752 (3%)]\tLoss: 2.083549\n",
      "Train Epoch: 1 [200/4752 (4%)]\tLoss: 5.031292\n",
      "Train Epoch: 1 [240/4752 (5%)]\tLoss: 2.756545\n",
      "Train Epoch: 1 [280/4752 (6%)]\tLoss: 4.799285\n",
      "Train Epoch: 1 [320/4752 (7%)]\tLoss: 2.274063\n",
      "Train Epoch: 1 [360/4752 (8%)]\tLoss: 6.669448\n",
      "Train Epoch: 1 [400/4752 (8%)]\tLoss: 0.528102\n",
      "Train Epoch: 1 [440/4752 (9%)]\tLoss: 6.749612\n",
      "Train Epoch: 1 [480/4752 (10%)]\tLoss: 6.881298\n",
      "Train Epoch: 1 [520/4752 (11%)]\tLoss: 5.604421\n",
      "Train Epoch: 1 [560/4752 (12%)]\tLoss: 6.070611\n",
      "Train Epoch: 1 [600/4752 (13%)]\tLoss: 8.927024\n",
      "Train Epoch: 1 [640/4752 (13%)]\tLoss: 6.643185\n",
      "Train Epoch: 1 [680/4752 (14%)]\tLoss: 7.773912\n",
      "Train Epoch: 1 [720/4752 (15%)]\tLoss: 6.712416\n",
      "Train Epoch: 1 [760/4752 (16%)]\tLoss: 3.121124\n",
      "Train Epoch: 1 [800/4752 (17%)]\tLoss: 9.757664\n",
      "Train Epoch: 1 [840/4752 (18%)]\tLoss: 5.813485\n",
      "Train Epoch: 1 [880/4752 (19%)]\tLoss: 0.344507\n",
      "Train Epoch: 1 [920/4752 (19%)]\tLoss: 4.835443\n",
      "Train Epoch: 1 [960/4752 (20%)]\tLoss: 9.172982\n",
      "Train Epoch: 1 [1000/4752 (21%)]\tLoss: 19.097273\n",
      "Train Epoch: 1 [1040/4752 (22%)]\tLoss: 28.573944\n",
      "Train Epoch: 1 [1080/4752 (23%)]\tLoss: 3.913527\n",
      "Train Epoch: 1 [1120/4752 (24%)]\tLoss: 6.673308\n",
      "Train Epoch: 1 [1160/4752 (24%)]\tLoss: 9.614304\n",
      "Train Epoch: 1 [1200/4752 (25%)]\tLoss: 6.607699\n",
      "Train Epoch: 1 [1240/4752 (26%)]\tLoss: 5.960547\n",
      "Train Epoch: 1 [1280/4752 (27%)]\tLoss: 2.606318\n",
      "Train Epoch: 1 [1320/4752 (28%)]\tLoss: 2.154159\n",
      "Train Epoch: 1 [1360/4752 (29%)]\tLoss: 17.224115\n",
      "Train Epoch: 1 [1400/4752 (29%)]\tLoss: 11.789189\n",
      "Train Epoch: 1 [1440/4752 (30%)]\tLoss: 8.965328\n",
      "Train Epoch: 1 [1480/4752 (31%)]\tLoss: 3.887361\n",
      "Train Epoch: 1 [1520/4752 (32%)]\tLoss: 9.256582\n",
      "Train Epoch: 1 [1560/4752 (33%)]\tLoss: 6.356354\n",
      "Train Epoch: 1 [1600/4752 (34%)]\tLoss: 1.113008\n",
      "Train Epoch: 1 [1640/4752 (35%)]\tLoss: 13.225555\n",
      "Train Epoch: 1 [1680/4752 (35%)]\tLoss: 9.778565\n",
      "Train Epoch: 1 [1720/4752 (36%)]\tLoss: 8.317844\n",
      "Train Epoch: 1 [1760/4752 (37%)]\tLoss: 25.236809\n",
      "Train Epoch: 1 [1800/4752 (38%)]\tLoss: 3.481315\n",
      "Train Epoch: 1 [1840/4752 (39%)]\tLoss: 11.835793\n",
      "Train Epoch: 1 [1880/4752 (40%)]\tLoss: 3.843005\n",
      "Train Epoch: 1 [1920/4752 (40%)]\tLoss: 17.210222\n",
      "Train Epoch: 1 [1960/4752 (41%)]\tLoss: 15.176480\n",
      "Train Epoch: 1 [2000/4752 (42%)]\tLoss: 13.351665\n",
      "Train Epoch: 1 [2040/4752 (43%)]\tLoss: 8.555377\n",
      "Train Epoch: 1 [2080/4752 (44%)]\tLoss: 8.339290\n",
      "Train Epoch: 1 [2120/4752 (45%)]\tLoss: 20.300835\n",
      "Train Epoch: 1 [2160/4752 (45%)]\tLoss: 11.927398\n",
      "Train Epoch: 1 [2200/4752 (46%)]\tLoss: 2.356193\n",
      "Train Epoch: 1 [2240/4752 (47%)]\tLoss: 3.947331\n",
      "Train Epoch: 1 [2280/4752 (48%)]\tLoss: 7.530653\n",
      "Train Epoch: 1 [2320/4752 (49%)]\tLoss: 3.244903\n",
      "Train Epoch: 1 [2360/4752 (50%)]\tLoss: 7.279273\n",
      "Train Epoch: 1 [2400/4752 (51%)]\tLoss: 12.674449\n",
      "Train Epoch: 1 [2440/4752 (51%)]\tLoss: 14.772092\n",
      "Train Epoch: 1 [2480/4752 (52%)]\tLoss: 4.770010\n",
      "Train Epoch: 1 [2520/4752 (53%)]\tLoss: 9.930414\n",
      "Train Epoch: 1 [2560/4752 (54%)]\tLoss: 2.540865\n",
      "Train Epoch: 1 [2600/4752 (55%)]\tLoss: 13.508429\n",
      "Train Epoch: 1 [2640/4752 (56%)]\tLoss: 30.002972\n",
      "Train Epoch: 1 [2680/4752 (56%)]\tLoss: 18.191679\n",
      "Train Epoch: 1 [2720/4752 (57%)]\tLoss: 4.088359\n",
      "Train Epoch: 1 [2760/4752 (58%)]\tLoss: 4.468138\n",
      "Train Epoch: 1 [2800/4752 (59%)]\tLoss: 24.835714\n",
      "Train Epoch: 1 [2840/4752 (60%)]\tLoss: 6.148623\n",
      "Train Epoch: 1 [2880/4752 (61%)]\tLoss: 8.331707\n",
      "Train Epoch: 1 [2920/4752 (61%)]\tLoss: 0.575318\n",
      "Train Epoch: 1 [2960/4752 (62%)]\tLoss: 10.226839\n",
      "Train Epoch: 1 [3000/4752 (63%)]\tLoss: 4.452544\n",
      "Train Epoch: 1 [3040/4752 (64%)]\tLoss: 10.555446\n",
      "Train Epoch: 1 [3080/4752 (65%)]\tLoss: 5.417419\n",
      "Train Epoch: 1 [3120/4752 (66%)]\tLoss: 34.751274\n",
      "Train Epoch: 1 [3160/4752 (66%)]\tLoss: 22.836992\n",
      "Train Epoch: 1 [3200/4752 (67%)]\tLoss: 29.055637\n",
      "Train Epoch: 1 [3240/4752 (68%)]\tLoss: 18.363043\n",
      "Train Epoch: 1 [3280/4752 (69%)]\tLoss: 39.230442\n",
      "Train Epoch: 1 [3320/4752 (70%)]\tLoss: 22.462433\n",
      "Train Epoch: 1 [3360/4752 (71%)]\tLoss: 4.002605\n",
      "Train Epoch: 1 [3400/4752 (72%)]\tLoss: 5.381008\n",
      "Train Epoch: 1 [3440/4752 (72%)]\tLoss: 12.977551\n",
      "Train Epoch: 1 [3480/4752 (73%)]\tLoss: 4.336849\n",
      "Train Epoch: 1 [3520/4752 (74%)]\tLoss: 7.046525\n",
      "Train Epoch: 1 [3560/4752 (75%)]\tLoss: 13.260431\n",
      "Train Epoch: 1 [3600/4752 (76%)]\tLoss: 2.036836\n",
      "Train Epoch: 1 [3640/4752 (77%)]\tLoss: 3.943769\n",
      "Train Epoch: 1 [3680/4752 (77%)]\tLoss: 8.787275\n",
      "Train Epoch: 1 [3720/4752 (78%)]\tLoss: 21.614708\n",
      "Train Epoch: 1 [3760/4752 (79%)]\tLoss: 14.196395\n",
      "Train Epoch: 1 [3800/4752 (80%)]\tLoss: 17.240784\n",
      "Train Epoch: 1 [3840/4752 (81%)]\tLoss: 9.803734\n",
      "Train Epoch: 1 [3880/4752 (82%)]\tLoss: 2.353424\n",
      "Train Epoch: 1 [3920/4752 (82%)]\tLoss: 14.372921\n",
      "Train Epoch: 1 [3960/4752 (83%)]\tLoss: 8.521437\n",
      "Train Epoch: 1 [4000/4752 (84%)]\tLoss: 3.885699\n",
      "Train Epoch: 1 [4040/4752 (85%)]\tLoss: 17.101265\n",
      "Train Epoch: 1 [4080/4752 (86%)]\tLoss: 10.704502\n",
      "Train Epoch: 1 [4120/4752 (87%)]\tLoss: 8.054832\n",
      "Train Epoch: 1 [4160/4752 (88%)]\tLoss: 18.576733\n",
      "Train Epoch: 1 [4200/4752 (88%)]\tLoss: 59.312065\n",
      "Train Epoch: 1 [4240/4752 (89%)]\tLoss: 21.771341\n",
      "Train Epoch: 1 [4280/4752 (90%)]\tLoss: 14.659847\n",
      "Train Epoch: 1 [4320/4752 (91%)]\tLoss: 2.558585\n",
      "Train Epoch: 1 [4360/4752 (92%)]\tLoss: 15.300242\n",
      "Train Epoch: 1 [4400/4752 (93%)]\tLoss: 34.189625\n",
      "Train Epoch: 1 [4440/4752 (93%)]\tLoss: 19.217737\n",
      "Train Epoch: 1 [4480/4752 (94%)]\tLoss: 30.071854\n",
      "Train Epoch: 1 [4520/4752 (95%)]\tLoss: 12.166332\n",
      "Train Epoch: 1 [4560/4752 (96%)]\tLoss: 8.300145\n",
      "Train Epoch: 1 [4600/4752 (97%)]\tLoss: 8.490704\n",
      "Train Epoch: 1 [4640/4752 (98%)]\tLoss: 7.069530\n",
      "Train Epoch: 1 [4680/4752 (98%)]\tLoss: 8.808022\n",
      "Train Epoch: 1 [4720/4752 (99%)]\tLoss: 3.171493\n",
      "\n",
      "Validation set: Average loss: 0.1455, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 32.105515\n",
      "Train Epoch: 2 [40/4752 (1%)]\tLoss: 14.272507\n",
      "Train Epoch: 2 [80/4752 (2%)]\tLoss: 8.161675\n",
      "Train Epoch: 2 [120/4752 (3%)]\tLoss: 39.141510\n",
      "Train Epoch: 2 [160/4752 (3%)]\tLoss: 11.696019\n",
      "Train Epoch: 2 [200/4752 (4%)]\tLoss: 29.205183\n",
      "Train Epoch: 2 [240/4752 (5%)]\tLoss: 4.319836\n",
      "Train Epoch: 2 [280/4752 (6%)]\tLoss: 47.994595\n",
      "Train Epoch: 2 [320/4752 (7%)]\tLoss: 7.894051\n",
      "Train Epoch: 2 [360/4752 (8%)]\tLoss: 16.902887\n",
      "Train Epoch: 2 [400/4752 (8%)]\tLoss: 15.176476\n",
      "Train Epoch: 2 [440/4752 (9%)]\tLoss: 23.758081\n",
      "Train Epoch: 2 [480/4752 (10%)]\tLoss: 27.080196\n",
      "Train Epoch: 2 [520/4752 (11%)]\tLoss: 62.451488\n",
      "Train Epoch: 2 [560/4752 (12%)]\tLoss: 25.518644\n",
      "Train Epoch: 2 [600/4752 (13%)]\tLoss: 24.826435\n",
      "Train Epoch: 2 [640/4752 (13%)]\tLoss: 23.972687\n",
      "Train Epoch: 2 [680/4752 (14%)]\tLoss: 23.250334\n",
      "Train Epoch: 2 [720/4752 (15%)]\tLoss: 14.254625\n",
      "Train Epoch: 2 [760/4752 (16%)]\tLoss: 16.538176\n",
      "Train Epoch: 2 [800/4752 (17%)]\tLoss: 26.995232\n",
      "Train Epoch: 2 [840/4752 (18%)]\tLoss: 18.472023\n",
      "Train Epoch: 2 [880/4752 (19%)]\tLoss: 5.504439\n",
      "Train Epoch: 2 [920/4752 (19%)]\tLoss: 21.712481\n",
      "Train Epoch: 2 [960/4752 (20%)]\tLoss: 45.371689\n",
      "Train Epoch: 2 [1000/4752 (21%)]\tLoss: 29.668320\n",
      "Train Epoch: 2 [1040/4752 (22%)]\tLoss: 7.982760\n",
      "Train Epoch: 2 [1080/4752 (23%)]\tLoss: 4.643836\n",
      "Train Epoch: 2 [1120/4752 (24%)]\tLoss: 23.669529\n",
      "Train Epoch: 2 [1160/4752 (24%)]\tLoss: 24.374569\n",
      "Train Epoch: 2 [1200/4752 (25%)]\tLoss: 14.337403\n",
      "Train Epoch: 2 [1240/4752 (26%)]\tLoss: 12.575720\n",
      "Train Epoch: 2 [1280/4752 (27%)]\tLoss: 23.046991\n",
      "Train Epoch: 2 [1320/4752 (28%)]\tLoss: 13.113440\n",
      "Train Epoch: 2 [1360/4752 (29%)]\tLoss: 1.966180\n",
      "Train Epoch: 2 [1400/4752 (29%)]\tLoss: 59.232262\n",
      "Train Epoch: 2 [1440/4752 (30%)]\tLoss: 4.213341\n",
      "Train Epoch: 2 [1480/4752 (31%)]\tLoss: 8.643332\n",
      "Train Epoch: 2 [1520/4752 (32%)]\tLoss: 34.909538\n",
      "Train Epoch: 2 [1560/4752 (33%)]\tLoss: 49.132126\n",
      "Train Epoch: 2 [1600/4752 (34%)]\tLoss: 40.571934\n",
      "Train Epoch: 2 [1640/4752 (35%)]\tLoss: 8.813353\n",
      "Train Epoch: 2 [1680/4752 (35%)]\tLoss: 1.353805\n",
      "Train Epoch: 2 [1720/4752 (36%)]\tLoss: 51.786362\n",
      "Train Epoch: 2 [1760/4752 (37%)]\tLoss: 29.736956\n",
      "Train Epoch: 2 [1800/4752 (38%)]\tLoss: 10.596390\n",
      "Train Epoch: 2 [1840/4752 (39%)]\tLoss: 25.845781\n",
      "Train Epoch: 2 [1880/4752 (40%)]\tLoss: 6.184899\n",
      "Train Epoch: 2 [1920/4752 (40%)]\tLoss: 40.225040\n",
      "Train Epoch: 2 [1960/4752 (41%)]\tLoss: 24.863689\n",
      "Train Epoch: 2 [2000/4752 (42%)]\tLoss: 39.410034\n",
      "Train Epoch: 2 [2040/4752 (43%)]\tLoss: 10.337126\n",
      "Train Epoch: 2 [2080/4752 (44%)]\tLoss: 14.888774\n",
      "Train Epoch: 2 [2120/4752 (45%)]\tLoss: 31.557743\n",
      "Train Epoch: 2 [2160/4752 (45%)]\tLoss: 6.519318\n",
      "Train Epoch: 2 [2200/4752 (46%)]\tLoss: 61.261887\n",
      "Train Epoch: 2 [2240/4752 (47%)]\tLoss: 25.024841\n",
      "Train Epoch: 2 [2280/4752 (48%)]\tLoss: 19.385860\n",
      "Train Epoch: 2 [2320/4752 (49%)]\tLoss: 18.757595\n",
      "Train Epoch: 2 [2360/4752 (50%)]\tLoss: 98.308189\n",
      "Train Epoch: 2 [2400/4752 (51%)]\tLoss: 16.612244\n",
      "Train Epoch: 2 [2440/4752 (51%)]\tLoss: 20.804340\n",
      "Train Epoch: 2 [2480/4752 (52%)]\tLoss: 66.040161\n",
      "Train Epoch: 2 [2520/4752 (53%)]\tLoss: 25.969706\n",
      "Train Epoch: 2 [2560/4752 (54%)]\tLoss: 14.869332\n",
      "Train Epoch: 2 [2600/4752 (55%)]\tLoss: 5.304613\n",
      "Train Epoch: 2 [2640/4752 (56%)]\tLoss: 31.983246\n",
      "Train Epoch: 2 [2680/4752 (56%)]\tLoss: 13.318212\n",
      "Train Epoch: 2 [2720/4752 (57%)]\tLoss: 28.960169\n",
      "Train Epoch: 2 [2760/4752 (58%)]\tLoss: 36.815342\n",
      "Train Epoch: 2 [2800/4752 (59%)]\tLoss: 10.915335\n",
      "Train Epoch: 2 [2840/4752 (60%)]\tLoss: 29.833691\n",
      "Train Epoch: 2 [2880/4752 (61%)]\tLoss: 28.056927\n",
      "Train Epoch: 2 [2920/4752 (61%)]\tLoss: 85.521286\n",
      "Train Epoch: 2 [2960/4752 (62%)]\tLoss: 34.460922\n",
      "Train Epoch: 2 [3000/4752 (63%)]\tLoss: 71.903618\n",
      "Train Epoch: 2 [3040/4752 (64%)]\tLoss: 87.175331\n",
      "Train Epoch: 2 [3080/4752 (65%)]\tLoss: 31.155067\n",
      "Train Epoch: 2 [3120/4752 (66%)]\tLoss: 19.926374\n",
      "Train Epoch: 2 [3160/4752 (66%)]\tLoss: 44.812012\n",
      "Train Epoch: 2 [3200/4752 (67%)]\tLoss: 30.770901\n",
      "Train Epoch: 2 [3240/4752 (68%)]\tLoss: 22.532440\n",
      "Train Epoch: 2 [3280/4752 (69%)]\tLoss: 17.325672\n",
      "Train Epoch: 2 [3320/4752 (70%)]\tLoss: 17.663754\n",
      "Train Epoch: 2 [3360/4752 (71%)]\tLoss: 23.566898\n",
      "Train Epoch: 2 [3400/4752 (72%)]\tLoss: 6.200444\n",
      "Train Epoch: 2 [3440/4752 (72%)]\tLoss: 11.234882\n",
      "Train Epoch: 2 [3480/4752 (73%)]\tLoss: 4.356580\n",
      "Train Epoch: 2 [3520/4752 (74%)]\tLoss: 15.582604\n",
      "Train Epoch: 2 [3560/4752 (75%)]\tLoss: 47.704914\n",
      "Train Epoch: 2 [3600/4752 (76%)]\tLoss: 23.256262\n",
      "Train Epoch: 2 [3640/4752 (77%)]\tLoss: 18.409893\n",
      "Train Epoch: 2 [3680/4752 (77%)]\tLoss: 15.232662\n",
      "Train Epoch: 2 [3720/4752 (78%)]\tLoss: 23.606096\n",
      "Train Epoch: 2 [3760/4752 (79%)]\tLoss: 50.392376\n",
      "Train Epoch: 2 [3800/4752 (80%)]\tLoss: 12.362337\n",
      "Train Epoch: 2 [3840/4752 (81%)]\tLoss: 41.300713\n",
      "Train Epoch: 2 [3880/4752 (82%)]\tLoss: 38.794140\n",
      "Train Epoch: 2 [3920/4752 (82%)]\tLoss: 67.105087\n",
      "Train Epoch: 2 [3960/4752 (83%)]\tLoss: 10.856514\n",
      "Train Epoch: 2 [4000/4752 (84%)]\tLoss: 44.683937\n",
      "Train Epoch: 2 [4040/4752 (85%)]\tLoss: 16.843428\n",
      "Train Epoch: 2 [4080/4752 (86%)]\tLoss: 9.440625\n",
      "Train Epoch: 2 [4120/4752 (87%)]\tLoss: 17.641317\n",
      "Train Epoch: 2 [4160/4752 (88%)]\tLoss: 23.732687\n",
      "Train Epoch: 2 [4200/4752 (88%)]\tLoss: 16.878857\n",
      "Train Epoch: 2 [4240/4752 (89%)]\tLoss: 29.270102\n",
      "Train Epoch: 2 [4280/4752 (90%)]\tLoss: 9.613811\n",
      "Train Epoch: 2 [4320/4752 (91%)]\tLoss: 49.039230\n",
      "Train Epoch: 2 [4360/4752 (92%)]\tLoss: 18.213530\n",
      "Train Epoch: 2 [4400/4752 (93%)]\tLoss: 56.828484\n",
      "Train Epoch: 2 [4440/4752 (93%)]\tLoss: 27.237991\n",
      "Train Epoch: 2 [4480/4752 (94%)]\tLoss: 21.490788\n",
      "Train Epoch: 2 [4520/4752 (95%)]\tLoss: 1.327893\n",
      "Train Epoch: 2 [4560/4752 (96%)]\tLoss: 19.630674\n",
      "Train Epoch: 2 [4600/4752 (97%)]\tLoss: 7.208342\n",
      "Train Epoch: 2 [4640/4752 (98%)]\tLoss: 12.996049\n",
      "Train Epoch: 2 [4680/4752 (98%)]\tLoss: 58.614639\n",
      "Train Epoch: 2 [4720/4752 (99%)]\tLoss: 56.773323\n",
      "\n",
      "Validation set: Average loss: 0.1564, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 23.599018\n",
      "Train Epoch: 3 [40/4752 (1%)]\tLoss: 75.907387\n",
      "Train Epoch: 3 [80/4752 (2%)]\tLoss: 45.171032\n",
      "Train Epoch: 3 [120/4752 (3%)]\tLoss: 39.659565\n",
      "Train Epoch: 3 [160/4752 (3%)]\tLoss: 85.045769\n",
      "Train Epoch: 3 [200/4752 (4%)]\tLoss: 8.725401\n",
      "Train Epoch: 3 [240/4752 (5%)]\tLoss: 6.449831\n",
      "Train Epoch: 3 [280/4752 (6%)]\tLoss: 33.735016\n",
      "Train Epoch: 3 [320/4752 (7%)]\tLoss: 13.811781\n",
      "Train Epoch: 3 [360/4752 (8%)]\tLoss: 8.350194\n",
      "Train Epoch: 3 [400/4752 (8%)]\tLoss: 53.429562\n",
      "Train Epoch: 3 [440/4752 (9%)]\tLoss: 2.769948\n",
      "Train Epoch: 3 [480/4752 (10%)]\tLoss: 37.168148\n",
      "Train Epoch: 3 [520/4752 (11%)]\tLoss: 26.872162\n",
      "Train Epoch: 3 [560/4752 (12%)]\tLoss: 12.401396\n",
      "Train Epoch: 3 [600/4752 (13%)]\tLoss: 18.709345\n",
      "Train Epoch: 3 [640/4752 (13%)]\tLoss: 41.472572\n",
      "Train Epoch: 3 [680/4752 (14%)]\tLoss: 41.671078\n",
      "Train Epoch: 3 [720/4752 (15%)]\tLoss: 7.098635\n",
      "Train Epoch: 3 [760/4752 (16%)]\tLoss: 26.481518\n",
      "Train Epoch: 3 [800/4752 (17%)]\tLoss: 45.696678\n",
      "Train Epoch: 3 [840/4752 (18%)]\tLoss: 28.389044\n",
      "Train Epoch: 3 [880/4752 (19%)]\tLoss: 7.663697\n",
      "Train Epoch: 3 [920/4752 (19%)]\tLoss: 25.175732\n",
      "Train Epoch: 3 [960/4752 (20%)]\tLoss: 14.654410\n",
      "Train Epoch: 3 [1000/4752 (21%)]\tLoss: 26.317900\n",
      "Train Epoch: 3 [1040/4752 (22%)]\tLoss: 15.400983\n",
      "Train Epoch: 3 [1080/4752 (23%)]\tLoss: 8.378281\n",
      "Train Epoch: 3 [1120/4752 (24%)]\tLoss: 23.842901\n",
      "Train Epoch: 3 [1160/4752 (24%)]\tLoss: 3.399242\n",
      "Train Epoch: 3 [1200/4752 (25%)]\tLoss: 75.511948\n",
      "Train Epoch: 3 [1240/4752 (26%)]\tLoss: 35.586979\n",
      "Train Epoch: 3 [1280/4752 (27%)]\tLoss: 39.001907\n",
      "Train Epoch: 3 [1320/4752 (28%)]\tLoss: 34.602570\n",
      "Train Epoch: 3 [1360/4752 (29%)]\tLoss: 34.761078\n",
      "Train Epoch: 3 [1400/4752 (29%)]\tLoss: 42.401440\n",
      "Train Epoch: 3 [1440/4752 (30%)]\tLoss: 26.372061\n",
      "Train Epoch: 3 [1480/4752 (31%)]\tLoss: 28.347416\n",
      "Train Epoch: 3 [1520/4752 (32%)]\tLoss: 28.385063\n",
      "Train Epoch: 3 [1560/4752 (33%)]\tLoss: 11.934746\n",
      "Train Epoch: 3 [1600/4752 (34%)]\tLoss: 6.608976\n",
      "Train Epoch: 3 [1640/4752 (35%)]\tLoss: 39.595909\n",
      "Train Epoch: 3 [1680/4752 (35%)]\tLoss: 42.770569\n",
      "Train Epoch: 3 [1720/4752 (36%)]\tLoss: 36.300297\n",
      "Train Epoch: 3 [1760/4752 (37%)]\tLoss: 41.367226\n",
      "Train Epoch: 3 [1800/4752 (38%)]\tLoss: 64.142944\n",
      "Train Epoch: 3 [1840/4752 (39%)]\tLoss: 49.192436\n",
      "Train Epoch: 3 [1880/4752 (40%)]\tLoss: 75.211586\n",
      "Train Epoch: 3 [1920/4752 (40%)]\tLoss: 42.495449\n",
      "Train Epoch: 3 [1960/4752 (41%)]\tLoss: 30.183086\n",
      "Train Epoch: 3 [2000/4752 (42%)]\tLoss: 22.145706\n",
      "Train Epoch: 3 [2040/4752 (43%)]\tLoss: 24.900105\n",
      "Train Epoch: 3 [2080/4752 (44%)]\tLoss: 16.505554\n",
      "Train Epoch: 3 [2120/4752 (45%)]\tLoss: 2.154248\n",
      "Train Epoch: 3 [2160/4752 (45%)]\tLoss: 18.911236\n",
      "Train Epoch: 3 [2200/4752 (46%)]\tLoss: 35.544693\n",
      "Train Epoch: 3 [2240/4752 (47%)]\tLoss: 55.293198\n",
      "Train Epoch: 3 [2280/4752 (48%)]\tLoss: 10.976514\n",
      "Train Epoch: 3 [2320/4752 (49%)]\tLoss: 26.173025\n",
      "Train Epoch: 3 [2360/4752 (50%)]\tLoss: 103.073547\n",
      "Train Epoch: 3 [2400/4752 (51%)]\tLoss: 46.918682\n",
      "Train Epoch: 3 [2440/4752 (51%)]\tLoss: 48.566887\n",
      "Train Epoch: 3 [2480/4752 (52%)]\tLoss: 23.825335\n",
      "Train Epoch: 3 [2520/4752 (53%)]\tLoss: 12.750814\n",
      "Train Epoch: 3 [2560/4752 (54%)]\tLoss: 53.635242\n",
      "Train Epoch: 3 [2600/4752 (55%)]\tLoss: 29.172634\n",
      "Train Epoch: 3 [2640/4752 (56%)]\tLoss: 1.684385\n",
      "Train Epoch: 3 [2680/4752 (56%)]\tLoss: 6.896436\n",
      "Train Epoch: 3 [2720/4752 (57%)]\tLoss: 57.553860\n",
      "Train Epoch: 3 [2760/4752 (58%)]\tLoss: 51.158936\n",
      "Train Epoch: 3 [2800/4752 (59%)]\tLoss: 63.802788\n",
      "Train Epoch: 3 [2840/4752 (60%)]\tLoss: 70.331207\n",
      "Train Epoch: 3 [2880/4752 (61%)]\tLoss: 53.154644\n",
      "Train Epoch: 3 [2920/4752 (61%)]\tLoss: 106.597954\n",
      "Train Epoch: 3 [2960/4752 (62%)]\tLoss: 89.596840\n",
      "Train Epoch: 3 [3000/4752 (63%)]\tLoss: 37.113861\n",
      "Train Epoch: 3 [3040/4752 (64%)]\tLoss: 21.438107\n",
      "Train Epoch: 3 [3080/4752 (65%)]\tLoss: 6.673808\n",
      "Train Epoch: 3 [3120/4752 (66%)]\tLoss: 31.488174\n",
      "Train Epoch: 3 [3160/4752 (66%)]\tLoss: 91.900330\n",
      "Train Epoch: 3 [3200/4752 (67%)]\tLoss: 57.253357\n",
      "Train Epoch: 3 [3240/4752 (68%)]\tLoss: 53.466225\n",
      "Train Epoch: 3 [3280/4752 (69%)]\tLoss: 20.482367\n",
      "Train Epoch: 3 [3320/4752 (70%)]\tLoss: 10.151414\n",
      "Train Epoch: 3 [3360/4752 (71%)]\tLoss: 22.684097\n",
      "Train Epoch: 3 [3400/4752 (72%)]\tLoss: 20.617716\n",
      "Train Epoch: 3 [3440/4752 (72%)]\tLoss: 10.122803\n",
      "Train Epoch: 3 [3480/4752 (73%)]\tLoss: 17.666103\n",
      "Train Epoch: 3 [3520/4752 (74%)]\tLoss: 39.240776\n",
      "Train Epoch: 3 [3560/4752 (75%)]\tLoss: 51.686447\n",
      "Train Epoch: 3 [3600/4752 (76%)]\tLoss: 72.067444\n",
      "Train Epoch: 3 [3640/4752 (77%)]\tLoss: 39.592010\n",
      "Train Epoch: 3 [3680/4752 (77%)]\tLoss: 22.829056\n",
      "Train Epoch: 3 [3720/4752 (78%)]\tLoss: 87.189438\n",
      "Train Epoch: 3 [3760/4752 (79%)]\tLoss: 18.796520\n",
      "Train Epoch: 3 [3800/4752 (80%)]\tLoss: 25.220287\n",
      "Train Epoch: 3 [3840/4752 (81%)]\tLoss: 9.169930\n",
      "Train Epoch: 3 [3880/4752 (82%)]\tLoss: 46.758511\n",
      "Train Epoch: 3 [3920/4752 (82%)]\tLoss: 54.974148\n",
      "Train Epoch: 3 [3960/4752 (83%)]\tLoss: 110.383575\n",
      "Train Epoch: 3 [4000/4752 (84%)]\tLoss: 52.180767\n",
      "Train Epoch: 3 [4040/4752 (85%)]\tLoss: 58.645920\n",
      "Train Epoch: 3 [4080/4752 (86%)]\tLoss: 134.719635\n",
      "Train Epoch: 3 [4120/4752 (87%)]\tLoss: 45.238342\n",
      "Train Epoch: 3 [4160/4752 (88%)]\tLoss: 63.827675\n",
      "Train Epoch: 3 [4200/4752 (88%)]\tLoss: 98.235954\n",
      "Train Epoch: 3 [4240/4752 (89%)]\tLoss: 7.735827\n",
      "Train Epoch: 3 [4280/4752 (90%)]\tLoss: 89.340431\n",
      "Train Epoch: 3 [4320/4752 (91%)]\tLoss: 12.190821\n",
      "Train Epoch: 3 [4360/4752 (92%)]\tLoss: 43.933716\n",
      "Train Epoch: 3 [4400/4752 (93%)]\tLoss: 2.264428\n",
      "Train Epoch: 3 [4440/4752 (93%)]\tLoss: 50.084866\n",
      "Train Epoch: 3 [4480/4752 (94%)]\tLoss: 45.567776\n",
      "Train Epoch: 3 [4520/4752 (95%)]\tLoss: 76.091522\n",
      "Train Epoch: 3 [4560/4752 (96%)]\tLoss: 5.341137\n",
      "Train Epoch: 3 [4600/4752 (97%)]\tLoss: 20.669325\n",
      "Train Epoch: 3 [4640/4752 (98%)]\tLoss: 35.842892\n",
      "Train Epoch: 3 [4680/4752 (98%)]\tLoss: 36.528610\n",
      "Train Epoch: 3 [4720/4752 (99%)]\tLoss: 15.152729\n",
      "\n",
      "Validation set: Average loss: 0.3445, Accuracy: 402/1584 (25%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 81.957382\n",
      "Train Epoch: 4 [40/4752 (1%)]\tLoss: 26.044317\n",
      "Train Epoch: 4 [80/4752 (2%)]\tLoss: 17.643429\n",
      "Train Epoch: 4 [120/4752 (3%)]\tLoss: 37.193436\n",
      "Train Epoch: 4 [160/4752 (3%)]\tLoss: 28.915783\n",
      "Train Epoch: 4 [200/4752 (4%)]\tLoss: 45.001244\n",
      "Train Epoch: 4 [240/4752 (5%)]\tLoss: 13.841635\n",
      "Train Epoch: 4 [280/4752 (6%)]\tLoss: 43.361305\n",
      "Train Epoch: 4 [320/4752 (7%)]\tLoss: 23.223600\n",
      "Train Epoch: 4 [360/4752 (8%)]\tLoss: 88.442467\n",
      "Train Epoch: 4 [400/4752 (8%)]\tLoss: 27.137043\n",
      "Train Epoch: 4 [440/4752 (9%)]\tLoss: 72.890579\n",
      "Train Epoch: 4 [480/4752 (10%)]\tLoss: 10.020912\n",
      "Train Epoch: 4 [520/4752 (11%)]\tLoss: 22.973412\n",
      "Train Epoch: 4 [560/4752 (12%)]\tLoss: 21.609964\n",
      "Train Epoch: 4 [600/4752 (13%)]\tLoss: 44.507114\n",
      "Train Epoch: 4 [640/4752 (13%)]\tLoss: 29.971397\n",
      "Train Epoch: 4 [680/4752 (14%)]\tLoss: 63.297348\n",
      "Train Epoch: 4 [720/4752 (15%)]\tLoss: 60.279129\n",
      "Train Epoch: 4 [760/4752 (16%)]\tLoss: 105.162437\n",
      "Train Epoch: 4 [800/4752 (17%)]\tLoss: 10.175278\n",
      "Train Epoch: 4 [840/4752 (18%)]\tLoss: 52.856575\n",
      "Train Epoch: 4 [880/4752 (19%)]\tLoss: 55.995010\n",
      "Train Epoch: 4 [920/4752 (19%)]\tLoss: 75.009468\n",
      "Train Epoch: 4 [960/4752 (20%)]\tLoss: 7.488785\n",
      "Train Epoch: 4 [1000/4752 (21%)]\tLoss: 70.954933\n",
      "Train Epoch: 4 [1040/4752 (22%)]\tLoss: 56.411270\n",
      "Train Epoch: 4 [1080/4752 (23%)]\tLoss: 99.615768\n",
      "Train Epoch: 4 [1120/4752 (24%)]\tLoss: 58.969265\n",
      "Train Epoch: 4 [1160/4752 (24%)]\tLoss: 38.738251\n",
      "Train Epoch: 4 [1200/4752 (25%)]\tLoss: 49.411766\n",
      "Train Epoch: 4 [1240/4752 (26%)]\tLoss: 27.678661\n",
      "Train Epoch: 4 [1280/4752 (27%)]\tLoss: 29.139133\n",
      "Train Epoch: 4 [1320/4752 (28%)]\tLoss: 87.754494\n",
      "Train Epoch: 4 [1360/4752 (29%)]\tLoss: 21.070646\n",
      "Train Epoch: 4 [1400/4752 (29%)]\tLoss: 92.450302\n",
      "Train Epoch: 4 [1440/4752 (30%)]\tLoss: 29.035168\n",
      "Train Epoch: 4 [1480/4752 (31%)]\tLoss: 39.380196\n",
      "Train Epoch: 4 [1520/4752 (32%)]\tLoss: 16.492714\n",
      "Train Epoch: 4 [1560/4752 (33%)]\tLoss: 38.432579\n",
      "Train Epoch: 4 [1600/4752 (34%)]\tLoss: 29.041597\n",
      "Train Epoch: 4 [1640/4752 (35%)]\tLoss: 35.124237\n",
      "Train Epoch: 4 [1680/4752 (35%)]\tLoss: 49.524254\n",
      "Train Epoch: 4 [1720/4752 (36%)]\tLoss: 37.983223\n",
      "Train Epoch: 4 [1760/4752 (37%)]\tLoss: 83.769760\n",
      "Train Epoch: 4 [1800/4752 (38%)]\tLoss: 103.439651\n",
      "Train Epoch: 4 [1840/4752 (39%)]\tLoss: 21.147160\n",
      "Train Epoch: 4 [1880/4752 (40%)]\tLoss: 41.308350\n",
      "Train Epoch: 4 [1920/4752 (40%)]\tLoss: 10.758837\n",
      "Train Epoch: 4 [1960/4752 (41%)]\tLoss: 14.347963\n",
      "Train Epoch: 4 [2000/4752 (42%)]\tLoss: 36.348824\n",
      "Train Epoch: 4 [2040/4752 (43%)]\tLoss: 95.964020\n",
      "Train Epoch: 4 [2080/4752 (44%)]\tLoss: 76.062019\n",
      "Train Epoch: 4 [2120/4752 (45%)]\tLoss: 51.548660\n",
      "Train Epoch: 4 [2160/4752 (45%)]\tLoss: 54.056686\n",
      "Train Epoch: 4 [2200/4752 (46%)]\tLoss: 60.669785\n",
      "Train Epoch: 4 [2240/4752 (47%)]\tLoss: 45.884701\n",
      "Train Epoch: 4 [2280/4752 (48%)]\tLoss: 129.595352\n",
      "Train Epoch: 4 [2320/4752 (49%)]\tLoss: 36.843578\n",
      "Train Epoch: 4 [2360/4752 (50%)]\tLoss: 41.344017\n",
      "Train Epoch: 4 [2400/4752 (51%)]\tLoss: 9.900565\n",
      "Train Epoch: 4 [2440/4752 (51%)]\tLoss: 25.681225\n",
      "Train Epoch: 4 [2480/4752 (52%)]\tLoss: 79.216087\n",
      "Train Epoch: 4 [2520/4752 (53%)]\tLoss: 7.068549\n",
      "Train Epoch: 4 [2560/4752 (54%)]\tLoss: 2.903504\n",
      "Train Epoch: 4 [2600/4752 (55%)]\tLoss: 13.293620\n",
      "Train Epoch: 4 [2640/4752 (56%)]\tLoss: 28.962393\n",
      "Train Epoch: 4 [2680/4752 (56%)]\tLoss: 111.133720\n",
      "Train Epoch: 4 [2720/4752 (57%)]\tLoss: 4.565439\n",
      "Train Epoch: 4 [2760/4752 (58%)]\tLoss: 73.081581\n",
      "Train Epoch: 4 [2800/4752 (59%)]\tLoss: 41.272556\n",
      "Train Epoch: 4 [2840/4752 (60%)]\tLoss: 17.562628\n",
      "Train Epoch: 4 [2880/4752 (61%)]\tLoss: 33.668587\n",
      "Train Epoch: 4 [2920/4752 (61%)]\tLoss: 64.352005\n",
      "Train Epoch: 4 [2960/4752 (62%)]\tLoss: 11.220641\n",
      "Train Epoch: 4 [3000/4752 (63%)]\tLoss: 29.723511\n",
      "Train Epoch: 4 [3040/4752 (64%)]\tLoss: 77.665298\n",
      "Train Epoch: 4 [3080/4752 (65%)]\tLoss: 12.342952\n",
      "Train Epoch: 4 [3120/4752 (66%)]\tLoss: 26.419325\n",
      "Train Epoch: 4 [3160/4752 (66%)]\tLoss: 15.241871\n",
      "Train Epoch: 4 [3200/4752 (67%)]\tLoss: 12.464512\n",
      "Train Epoch: 4 [3240/4752 (68%)]\tLoss: 66.926971\n",
      "Train Epoch: 4 [3280/4752 (69%)]\tLoss: 16.571905\n",
      "Train Epoch: 4 [3320/4752 (70%)]\tLoss: 64.508781\n",
      "Train Epoch: 4 [3360/4752 (71%)]\tLoss: 105.722961\n",
      "Train Epoch: 4 [3400/4752 (72%)]\tLoss: 51.134346\n",
      "Train Epoch: 4 [3440/4752 (72%)]\tLoss: 74.981430\n",
      "Train Epoch: 4 [3480/4752 (73%)]\tLoss: 14.014714\n",
      "Train Epoch: 4 [3520/4752 (74%)]\tLoss: 56.153549\n",
      "Train Epoch: 4 [3560/4752 (75%)]\tLoss: 6.501777\n",
      "Train Epoch: 4 [3600/4752 (76%)]\tLoss: 32.764446\n",
      "Train Epoch: 4 [3640/4752 (77%)]\tLoss: 46.706741\n",
      "Train Epoch: 4 [3680/4752 (77%)]\tLoss: 66.414253\n",
      "Train Epoch: 4 [3720/4752 (78%)]\tLoss: 42.797482\n",
      "Train Epoch: 4 [3760/4752 (79%)]\tLoss: 25.485023\n",
      "Train Epoch: 4 [3800/4752 (80%)]\tLoss: 44.400154\n",
      "Train Epoch: 4 [3840/4752 (81%)]\tLoss: 41.262417\n",
      "Train Epoch: 4 [3880/4752 (82%)]\tLoss: 56.757812\n",
      "Train Epoch: 4 [3920/4752 (82%)]\tLoss: 32.521591\n",
      "Train Epoch: 4 [3960/4752 (83%)]\tLoss: 18.127563\n",
      "Train Epoch: 4 [4000/4752 (84%)]\tLoss: 122.391991\n",
      "Train Epoch: 4 [4040/4752 (85%)]\tLoss: 61.521423\n",
      "Train Epoch: 4 [4080/4752 (86%)]\tLoss: 74.568031\n",
      "Train Epoch: 4 [4120/4752 (87%)]\tLoss: 97.393829\n",
      "Train Epoch: 4 [4160/4752 (88%)]\tLoss: 10.286155\n",
      "Train Epoch: 4 [4200/4752 (88%)]\tLoss: 86.658569\n",
      "Train Epoch: 4 [4240/4752 (89%)]\tLoss: 16.233416\n",
      "Train Epoch: 4 [4280/4752 (90%)]\tLoss: 85.980118\n",
      "Train Epoch: 4 [4320/4752 (91%)]\tLoss: 19.584040\n",
      "Train Epoch: 4 [4360/4752 (92%)]\tLoss: 12.896069\n",
      "Train Epoch: 4 [4400/4752 (93%)]\tLoss: 49.715714\n",
      "Train Epoch: 4 [4440/4752 (93%)]\tLoss: 76.821602\n",
      "Train Epoch: 4 [4480/4752 (94%)]\tLoss: 78.699768\n",
      "Train Epoch: 4 [4520/4752 (95%)]\tLoss: 21.673670\n",
      "Train Epoch: 4 [4560/4752 (96%)]\tLoss: 57.655251\n",
      "Train Epoch: 4 [4600/4752 (97%)]\tLoss: 15.195575\n",
      "Train Epoch: 4 [4640/4752 (98%)]\tLoss: 38.604462\n",
      "Train Epoch: 4 [4680/4752 (98%)]\tLoss: 45.739548\n",
      "Train Epoch: 4 [4720/4752 (99%)]\tLoss: 54.789894\n",
      "\n",
      "Validation set: Average loss: 0.3304, Accuracy: 402/1584 (25%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 64.494087\n",
      "Train Epoch: 5 [40/4752 (1%)]\tLoss: 31.903000\n",
      "Train Epoch: 5 [80/4752 (2%)]\tLoss: 16.908226\n",
      "Train Epoch: 5 [120/4752 (3%)]\tLoss: 54.678017\n",
      "Train Epoch: 5 [160/4752 (3%)]\tLoss: 18.522749\n",
      "Train Epoch: 5 [200/4752 (4%)]\tLoss: 89.309738\n",
      "Train Epoch: 5 [240/4752 (5%)]\tLoss: 33.840084\n",
      "Train Epoch: 5 [280/4752 (6%)]\tLoss: 28.513885\n",
      "Train Epoch: 5 [320/4752 (7%)]\tLoss: 38.556435\n",
      "Train Epoch: 5 [360/4752 (8%)]\tLoss: 18.720469\n",
      "Train Epoch: 5 [400/4752 (8%)]\tLoss: 47.668064\n",
      "Train Epoch: 5 [440/4752 (9%)]\tLoss: 49.407730\n",
      "Train Epoch: 5 [480/4752 (10%)]\tLoss: 98.710388\n",
      "Train Epoch: 5 [520/4752 (11%)]\tLoss: 24.064465\n",
      "Train Epoch: 5 [560/4752 (12%)]\tLoss: 18.137247\n",
      "Train Epoch: 5 [600/4752 (13%)]\tLoss: 63.013779\n",
      "Train Epoch: 5 [640/4752 (13%)]\tLoss: 121.358505\n",
      "Train Epoch: 5 [680/4752 (14%)]\tLoss: 37.379440\n",
      "Train Epoch: 5 [720/4752 (15%)]\tLoss: 69.902275\n",
      "Train Epoch: 5 [760/4752 (16%)]\tLoss: 12.111481\n",
      "Train Epoch: 5 [800/4752 (17%)]\tLoss: 35.261387\n",
      "Train Epoch: 5 [840/4752 (18%)]\tLoss: 50.403152\n",
      "Train Epoch: 5 [880/4752 (19%)]\tLoss: 26.704426\n",
      "Train Epoch: 5 [920/4752 (19%)]\tLoss: 33.553738\n",
      "Train Epoch: 5 [960/4752 (20%)]\tLoss: 72.697197\n",
      "Train Epoch: 5 [1000/4752 (21%)]\tLoss: 25.070808\n",
      "Train Epoch: 5 [1040/4752 (22%)]\tLoss: 15.139056\n",
      "Train Epoch: 5 [1080/4752 (23%)]\tLoss: 40.049828\n",
      "Train Epoch: 5 [1120/4752 (24%)]\tLoss: 44.885902\n",
      "Train Epoch: 5 [1160/4752 (24%)]\tLoss: 7.903892\n",
      "Train Epoch: 5 [1200/4752 (25%)]\tLoss: 15.166672\n",
      "Train Epoch: 5 [1240/4752 (26%)]\tLoss: 19.870129\n",
      "Train Epoch: 5 [1280/4752 (27%)]\tLoss: 5.147717\n",
      "Train Epoch: 5 [1320/4752 (28%)]\tLoss: 11.335119\n",
      "Train Epoch: 5 [1360/4752 (29%)]\tLoss: 41.155609\n",
      "Train Epoch: 5 [1400/4752 (29%)]\tLoss: 52.881786\n",
      "Train Epoch: 5 [1440/4752 (30%)]\tLoss: 58.960442\n",
      "Train Epoch: 5 [1480/4752 (31%)]\tLoss: 71.755478\n",
      "Train Epoch: 5 [1520/4752 (32%)]\tLoss: 118.070625\n",
      "Train Epoch: 5 [1560/4752 (33%)]\tLoss: 26.919626\n",
      "Train Epoch: 5 [1600/4752 (34%)]\tLoss: 23.591911\n",
      "Train Epoch: 5 [1640/4752 (35%)]\tLoss: 7.501522\n",
      "Train Epoch: 5 [1680/4752 (35%)]\tLoss: 92.490906\n",
      "Train Epoch: 5 [1720/4752 (36%)]\tLoss: 13.153842\n",
      "Train Epoch: 5 [1760/4752 (37%)]\tLoss: 50.975243\n",
      "Train Epoch: 5 [1800/4752 (38%)]\tLoss: 39.791683\n",
      "Train Epoch: 5 [1840/4752 (39%)]\tLoss: 107.893959\n",
      "Train Epoch: 5 [1880/4752 (40%)]\tLoss: 74.554016\n",
      "Train Epoch: 5 [1920/4752 (40%)]\tLoss: 6.817047\n",
      "Train Epoch: 5 [1960/4752 (41%)]\tLoss: 20.651468\n",
      "Train Epoch: 5 [2000/4752 (42%)]\tLoss: 110.646828\n",
      "Train Epoch: 5 [2040/4752 (43%)]\tLoss: 77.955627\n",
      "Train Epoch: 5 [2080/4752 (44%)]\tLoss: 4.351376\n",
      "Train Epoch: 5 [2120/4752 (45%)]\tLoss: 19.264618\n",
      "Train Epoch: 5 [2160/4752 (45%)]\tLoss: 10.969814\n",
      "Train Epoch: 5 [2200/4752 (46%)]\tLoss: 129.844116\n",
      "Train Epoch: 5 [2240/4752 (47%)]\tLoss: 49.045212\n",
      "Train Epoch: 5 [2280/4752 (48%)]\tLoss: 202.039444\n",
      "Train Epoch: 5 [2320/4752 (49%)]\tLoss: 44.705185\n",
      "Train Epoch: 5 [2360/4752 (50%)]\tLoss: 29.423038\n",
      "Train Epoch: 5 [2400/4752 (51%)]\tLoss: 38.751762\n",
      "Train Epoch: 5 [2440/4752 (51%)]\tLoss: 52.080425\n",
      "Train Epoch: 5 [2480/4752 (52%)]\tLoss: 76.572693\n",
      "Train Epoch: 5 [2520/4752 (53%)]\tLoss: 25.125051\n",
      "Train Epoch: 5 [2560/4752 (54%)]\tLoss: 34.347752\n",
      "Train Epoch: 5 [2600/4752 (55%)]\tLoss: 51.641979\n",
      "Train Epoch: 5 [2640/4752 (56%)]\tLoss: 63.699554\n",
      "Train Epoch: 5 [2680/4752 (56%)]\tLoss: 38.703701\n",
      "Train Epoch: 5 [2720/4752 (57%)]\tLoss: 127.681656\n",
      "Train Epoch: 5 [2760/4752 (58%)]\tLoss: 51.664398\n",
      "Train Epoch: 5 [2800/4752 (59%)]\tLoss: 59.146240\n",
      "Train Epoch: 5 [2840/4752 (60%)]\tLoss: 40.665142\n",
      "Train Epoch: 5 [2880/4752 (61%)]\tLoss: 33.600925\n",
      "Train Epoch: 5 [2920/4752 (61%)]\tLoss: 58.380135\n",
      "Train Epoch: 5 [2960/4752 (62%)]\tLoss: 54.935802\n",
      "Train Epoch: 5 [3000/4752 (63%)]\tLoss: 33.459503\n",
      "Train Epoch: 5 [3040/4752 (64%)]\tLoss: 19.281067\n",
      "Train Epoch: 5 [3080/4752 (65%)]\tLoss: 57.064808\n",
      "Train Epoch: 5 [3120/4752 (66%)]\tLoss: 26.885288\n",
      "Train Epoch: 5 [3160/4752 (66%)]\tLoss: 54.779327\n",
      "Train Epoch: 5 [3200/4752 (67%)]\tLoss: 17.726093\n",
      "Train Epoch: 5 [3240/4752 (68%)]\tLoss: 46.594803\n",
      "Train Epoch: 5 [3280/4752 (69%)]\tLoss: 29.877657\n",
      "Train Epoch: 5 [3320/4752 (70%)]\tLoss: 153.791351\n",
      "Train Epoch: 5 [3360/4752 (71%)]\tLoss: 15.756893\n",
      "Train Epoch: 5 [3400/4752 (72%)]\tLoss: 22.440533\n",
      "Train Epoch: 5 [3440/4752 (72%)]\tLoss: 61.952805\n",
      "Train Epoch: 5 [3480/4752 (73%)]\tLoss: 35.292919\n",
      "Train Epoch: 5 [3520/4752 (74%)]\tLoss: 10.770723\n",
      "Train Epoch: 5 [3560/4752 (75%)]\tLoss: 30.197760\n",
      "Train Epoch: 5 [3600/4752 (76%)]\tLoss: 20.595226\n",
      "Train Epoch: 5 [3640/4752 (77%)]\tLoss: 49.461773\n",
      "Train Epoch: 5 [3680/4752 (77%)]\tLoss: 63.168766\n",
      "Train Epoch: 5 [3720/4752 (78%)]\tLoss: 16.225201\n",
      "Train Epoch: 5 [3760/4752 (79%)]\tLoss: 17.435907\n",
      "Train Epoch: 5 [3800/4752 (80%)]\tLoss: 57.839737\n",
      "Train Epoch: 5 [3840/4752 (81%)]\tLoss: 93.316422\n",
      "Train Epoch: 5 [3880/4752 (82%)]\tLoss: 27.712404\n",
      "Train Epoch: 5 [3920/4752 (82%)]\tLoss: 4.953912\n",
      "Train Epoch: 5 [3960/4752 (83%)]\tLoss: 39.259850\n",
      "Train Epoch: 5 [4000/4752 (84%)]\tLoss: 20.210314\n",
      "Train Epoch: 5 [4040/4752 (85%)]\tLoss: 46.102604\n",
      "Train Epoch: 5 [4080/4752 (86%)]\tLoss: 59.459064\n",
      "Train Epoch: 5 [4120/4752 (87%)]\tLoss: 47.049458\n",
      "Train Epoch: 5 [4160/4752 (88%)]\tLoss: 11.113792\n",
      "Train Epoch: 5 [4200/4752 (88%)]\tLoss: 43.818771\n",
      "Train Epoch: 5 [4240/4752 (89%)]\tLoss: 86.291794\n",
      "Train Epoch: 5 [4280/4752 (90%)]\tLoss: 43.241310\n",
      "Train Epoch: 5 [4320/4752 (91%)]\tLoss: 31.949852\n",
      "Train Epoch: 5 [4360/4752 (92%)]\tLoss: 18.455137\n",
      "Train Epoch: 5 [4400/4752 (93%)]\tLoss: 28.864100\n",
      "Train Epoch: 5 [4440/4752 (93%)]\tLoss: 72.448280\n",
      "Train Epoch: 5 [4480/4752 (94%)]\tLoss: 24.845881\n",
      "Train Epoch: 5 [4520/4752 (95%)]\tLoss: 74.815735\n",
      "Train Epoch: 5 [4560/4752 (96%)]\tLoss: 103.495407\n",
      "Train Epoch: 5 [4600/4752 (97%)]\tLoss: 13.387619\n",
      "Train Epoch: 5 [4640/4752 (98%)]\tLoss: 41.594894\n",
      "Train Epoch: 5 [4680/4752 (98%)]\tLoss: 19.175636\n",
      "Train Epoch: 5 [4720/4752 (99%)]\tLoss: 49.190903\n",
      "\n",
      "Validation set: Average loss: 0.2281, Accuracy: 402/1584 (25%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2249, Accuracy: 432/1584 (27%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters and Hyperparameters\n",
    "vocabulary_size = 100000  # to adjust \n",
    "sequence_length = 50  # to adjust \n",
    "embedding_dim = 128\n",
    "hidden_dim = 64\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, optimizer, and device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = QLearning(vocabulary_size, embedding_dim, hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    validate(model, device, validation_loader)\n",
    "\n",
    "# After training, evaluate on the test set\n",
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-02 20:57:17,521] A new study created in memory with name: no-name-ab7b57ca-6751-4f57-b2f4-0c88f8f12761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.451875\n",
      "Train Epoch: 1 [1280/4752 (26%)]\tLoss: 0.630663\n",
      "Train Epoch: 1 [2560/4752 (53%)]\tLoss: 0.802966\n",
      "Train Epoch: 1 [3840/4752 (79%)]\tLoss: 0.967167\n",
      "\n",
      "Validation set: Average loss: 0.0048, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 1.119895\n",
      "Train Epoch: 2 [1280/4752 (26%)]\tLoss: 1.439611\n",
      "Train Epoch: 2 [2560/4752 (53%)]\tLoss: 1.566463\n",
      "Train Epoch: 2 [3840/4752 (79%)]\tLoss: 1.529376\n",
      "\n",
      "Validation set: Average loss: 0.0053, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 1.707562\n",
      "Train Epoch: 3 [1280/4752 (26%)]\tLoss: 1.497642\n",
      "Train Epoch: 3 [2560/4752 (53%)]\tLoss: 1.365630\n",
      "Train Epoch: 3 [3840/4752 (79%)]\tLoss: 1.563856\n",
      "\n",
      "Validation set: Average loss: 0.0052, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 1.384591\n",
      "Train Epoch: 4 [1280/4752 (26%)]\tLoss: 1.494386\n",
      "Train Epoch: 4 [2560/4752 (53%)]\tLoss: 1.314216\n",
      "Train Epoch: 4 [3840/4752 (79%)]\tLoss: 1.594736\n",
      "\n",
      "Validation set: Average loss: 0.0052, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 1.310798\n",
      "Train Epoch: 5 [1280/4752 (26%)]\tLoss: 1.358270\n",
      "Train Epoch: 5 [2560/4752 (53%)]\tLoss: 1.539686\n",
      "Train Epoch: 5 [3840/4752 (79%)]\tLoss: 1.237773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-02 20:57:35,047] Trial 0 finished with value: -0.7462121212121212 and parameters: {'batch_size': 128, 'learning_rate': 0.0012591182670210136, 'embedding_dim': 128, 'hidden_dim': 32, 'optimizer': 'Adam', 'dropout_rate': 0.013259984128779212, 'step_size': 78, 'gamma': 0.2509806262222176, 'sequence_length': 200}. Best is trial 0 with value: -0.7462121212121212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.0051, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.099648\n",
      "Train Epoch: 1 [320/4752 (7%)]\tLoss: 2.190719\n",
      "Train Epoch: 1 [640/4752 (13%)]\tLoss: 2.069269\n",
      "Train Epoch: 1 [960/4752 (20%)]\tLoss: 1.208082\n",
      "Train Epoch: 1 [1280/4752 (27%)]\tLoss: 1.722150\n",
      "Train Epoch: 1 [1600/4752 (34%)]\tLoss: 1.708508\n",
      "Train Epoch: 1 [1920/4752 (40%)]\tLoss: 1.806677\n",
      "Train Epoch: 1 [2240/4752 (47%)]\tLoss: 2.199113\n",
      "Train Epoch: 1 [2560/4752 (54%)]\tLoss: 1.947798\n",
      "Train Epoch: 1 [2880/4752 (60%)]\tLoss: 1.594264\n",
      "Train Epoch: 1 [3200/4752 (67%)]\tLoss: 2.475135\n",
      "Train Epoch: 1 [3520/4752 (74%)]\tLoss: 1.382063\n",
      "Train Epoch: 1 [3840/4752 (81%)]\tLoss: 2.513667\n",
      "Train Epoch: 1 [4160/4752 (87%)]\tLoss: 2.608904\n",
      "Train Epoch: 1 [4480/4752 (94%)]\tLoss: 1.496856\n",
      "\n",
      "Validation set: Average loss: 0.0186, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 2.519108\n",
      "Train Epoch: 2 [320/4752 (7%)]\tLoss: 2.247905\n",
      "Train Epoch: 2 [640/4752 (13%)]\tLoss: 1.910979\n",
      "Train Epoch: 2 [960/4752 (20%)]\tLoss: 1.430839\n",
      "Train Epoch: 2 [1280/4752 (27%)]\tLoss: 2.647101\n",
      "Train Epoch: 2 [1600/4752 (34%)]\tLoss: 2.065827\n",
      "Train Epoch: 2 [1920/4752 (40%)]\tLoss: 3.702032\n",
      "Train Epoch: 2 [2240/4752 (47%)]\tLoss: 1.367057\n",
      "Train Epoch: 2 [2560/4752 (54%)]\tLoss: 2.530380\n",
      "Train Epoch: 2 [2880/4752 (60%)]\tLoss: 3.582289\n",
      "Train Epoch: 2 [3200/4752 (67%)]\tLoss: 3.375871\n",
      "Train Epoch: 2 [3520/4752 (74%)]\tLoss: 4.299176\n",
      "Train Epoch: 2 [3840/4752 (81%)]\tLoss: 2.538762\n",
      "Train Epoch: 2 [4160/4752 (87%)]\tLoss: 3.528508\n",
      "Train Epoch: 2 [4480/4752 (94%)]\tLoss: 4.163267\n",
      "\n",
      "Validation set: Average loss: 0.0186, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 3.098292\n",
      "Train Epoch: 3 [320/4752 (7%)]\tLoss: 4.120806\n",
      "Train Epoch: 3 [640/4752 (13%)]\tLoss: 3.371127\n",
      "Train Epoch: 3 [960/4752 (20%)]\tLoss: 2.847241\n",
      "Train Epoch: 3 [1280/4752 (27%)]\tLoss: 2.878077\n",
      "Train Epoch: 3 [1600/4752 (34%)]\tLoss: 4.677570\n",
      "Train Epoch: 3 [1920/4752 (40%)]\tLoss: 4.741338\n",
      "Train Epoch: 3 [2240/4752 (47%)]\tLoss: 3.804934\n",
      "Train Epoch: 3 [2560/4752 (54%)]\tLoss: 4.927792\n",
      "Train Epoch: 3 [2880/4752 (60%)]\tLoss: 3.173437\n",
      "Train Epoch: 3 [3200/4752 (67%)]\tLoss: 3.983139\n",
      "Train Epoch: 3 [3520/4752 (74%)]\tLoss: 3.703444\n",
      "Train Epoch: 3 [3840/4752 (81%)]\tLoss: 4.282972\n",
      "Train Epoch: 3 [4160/4752 (87%)]\tLoss: 3.817913\n",
      "Train Epoch: 3 [4480/4752 (94%)]\tLoss: 3.843032\n",
      "\n",
      "Validation set: Average loss: 0.0181, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 4.110068\n",
      "Train Epoch: 4 [320/4752 (7%)]\tLoss: 3.179222\n",
      "Train Epoch: 4 [640/4752 (13%)]\tLoss: 3.243959\n",
      "Train Epoch: 4 [960/4752 (20%)]\tLoss: 4.218561\n",
      "Train Epoch: 4 [1280/4752 (27%)]\tLoss: 4.423982\n",
      "Train Epoch: 4 [1600/4752 (34%)]\tLoss: 4.099674\n",
      "Train Epoch: 4 [1920/4752 (40%)]\tLoss: 5.837824\n",
      "Train Epoch: 4 [2240/4752 (47%)]\tLoss: 5.444983\n",
      "Train Epoch: 4 [2560/4752 (54%)]\tLoss: 6.829030\n",
      "Train Epoch: 4 [2880/4752 (60%)]\tLoss: 3.897273\n",
      "Train Epoch: 4 [3200/4752 (67%)]\tLoss: 4.185780\n",
      "Train Epoch: 4 [3520/4752 (74%)]\tLoss: 9.224562\n",
      "Train Epoch: 4 [3840/4752 (81%)]\tLoss: 4.827735\n",
      "Train Epoch: 4 [4160/4752 (87%)]\tLoss: 6.048359\n",
      "Train Epoch: 4 [4480/4752 (94%)]\tLoss: 6.131577\n",
      "\n",
      "Validation set: Average loss: 0.0179, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 6.312903\n",
      "Train Epoch: 5 [320/4752 (7%)]\tLoss: 7.358935\n",
      "Train Epoch: 5 [640/4752 (13%)]\tLoss: 5.935745\n",
      "Train Epoch: 5 [960/4752 (20%)]\tLoss: 4.531621\n",
      "Train Epoch: 5 [1280/4752 (27%)]\tLoss: 6.116148\n",
      "Train Epoch: 5 [1600/4752 (34%)]\tLoss: 2.531613\n",
      "Train Epoch: 5 [1920/4752 (40%)]\tLoss: 4.890283\n",
      "Train Epoch: 5 [2240/4752 (47%)]\tLoss: 6.790654\n",
      "Train Epoch: 5 [2560/4752 (54%)]\tLoss: 9.131515\n",
      "Train Epoch: 5 [2880/4752 (60%)]\tLoss: 5.768075\n",
      "Train Epoch: 5 [3200/4752 (67%)]\tLoss: 6.157746\n",
      "Train Epoch: 5 [3520/4752 (74%)]\tLoss: 4.983541\n",
      "Train Epoch: 5 [3840/4752 (81%)]\tLoss: 6.170178\n",
      "Train Epoch: 5 [4160/4752 (87%)]\tLoss: 4.358707\n",
      "Train Epoch: 5 [4480/4752 (94%)]\tLoss: 4.217172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-02 20:59:05,105] Trial 1 finished with value: -0.7462121212121212 and parameters: {'batch_size': 32, 'learning_rate': 0.001296619982995175, 'embedding_dim': 256, 'hidden_dim': 64, 'optimizer': 'RMSprop', 'dropout_rate': 0.05082897612082027, 'step_size': 29, 'gamma': 0.8831017758467231, 'sequence_length': 50}. Best is trial 0 with value: -0.7462121212121212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.0181, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.359774\n",
      "Train Epoch: 1 [640/4752 (13%)]\tLoss: 1.033899\n",
      "Train Epoch: 1 [1280/4752 (27%)]\tLoss: 1.571849\n",
      "Train Epoch: 1 [1920/4752 (40%)]\tLoss: 2.922798\n",
      "Train Epoch: 1 [2560/4752 (53%)]\tLoss: 3.469686\n",
      "Train Epoch: 1 [3200/4752 (67%)]\tLoss: 2.507963\n",
      "Train Epoch: 1 [3840/4752 (80%)]\tLoss: 5.119108\n",
      "Train Epoch: 1 [4480/4752 (93%)]\tLoss: 3.769398\n",
      "\n",
      "Validation set: Average loss: 0.0101, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 3.572478\n",
      "Train Epoch: 2 [640/4752 (13%)]\tLoss: 3.892415\n",
      "Train Epoch: 2 [1280/4752 (27%)]\tLoss: 3.816474\n",
      "Train Epoch: 2 [1920/4752 (40%)]\tLoss: 4.601270\n",
      "Train Epoch: 2 [2560/4752 (53%)]\tLoss: 4.428186\n",
      "Train Epoch: 2 [3200/4752 (67%)]\tLoss: 4.301545\n",
      "Train Epoch: 2 [3840/4752 (80%)]\tLoss: 4.325041\n",
      "Train Epoch: 2 [4480/4752 (93%)]\tLoss: 4.698863\n",
      "\n",
      "Validation set: Average loss: 0.0099, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 4.386045\n",
      "Train Epoch: 3 [640/4752 (13%)]\tLoss: 5.855964\n",
      "Train Epoch: 3 [1280/4752 (27%)]\tLoss: 5.643104\n",
      "Train Epoch: 3 [1920/4752 (40%)]\tLoss: 4.291949\n",
      "Train Epoch: 3 [2560/4752 (53%)]\tLoss: 4.232966\n",
      "Train Epoch: 3 [3200/4752 (67%)]\tLoss: 6.666086\n",
      "Train Epoch: 3 [3840/4752 (80%)]\tLoss: 5.254091\n",
      "Train Epoch: 3 [4480/4752 (93%)]\tLoss: 5.949785\n",
      "\n",
      "Validation set: Average loss: 0.0100, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 7.955058\n",
      "Train Epoch: 4 [640/4752 (13%)]\tLoss: 5.795410\n",
      "Train Epoch: 4 [1280/4752 (27%)]\tLoss: 5.381888\n",
      "Train Epoch: 4 [1920/4752 (40%)]\tLoss: 5.781865\n",
      "Train Epoch: 4 [2560/4752 (53%)]\tLoss: 4.639564\n",
      "Train Epoch: 4 [3200/4752 (67%)]\tLoss: 5.207854\n",
      "Train Epoch: 4 [3840/4752 (80%)]\tLoss: 7.351658\n",
      "Train Epoch: 4 [4480/4752 (93%)]\tLoss: 6.116006\n",
      "\n",
      "Validation set: Average loss: 0.0097, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 8.481832\n",
      "Train Epoch: 5 [640/4752 (13%)]\tLoss: 7.925132\n",
      "Train Epoch: 5 [1280/4752 (27%)]\tLoss: 12.430093\n",
      "Train Epoch: 5 [1920/4752 (40%)]\tLoss: 7.757678\n",
      "Train Epoch: 5 [2560/4752 (53%)]\tLoss: 5.868748\n",
      "Train Epoch: 5 [3200/4752 (67%)]\tLoss: 7.753673\n",
      "Train Epoch: 5 [3840/4752 (80%)]\tLoss: 6.886885\n",
      "Train Epoch: 5 [4480/4752 (93%)]\tLoss: 4.942873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-02 20:59:18,526] Trial 2 finished with value: -0.7462121212121212 and parameters: {'batch_size': 64, 'learning_rate': 0.0004892930038408881, 'embedding_dim': 64, 'hidden_dim': 32, 'optimizer': 'RMSprop', 'dropout_rate': 0.4397029645972962, 'step_size': 4, 'gamma': 0.11731105218391365, 'sequence_length': 50}. Best is trial 0 with value: -0.7462121212121212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.0097, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.804019\n",
      "Train Epoch: 1 [1280/4752 (26%)]\tLoss: 54.106033\n",
      "Train Epoch: 1 [2560/4752 (53%)]\tLoss: 126.298843\n",
      "Train Epoch: 1 [3840/4752 (79%)]\tLoss: 129.424545\n",
      "\n",
      "Validation set: Average loss: 0.0141, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 97.408615\n",
      "Train Epoch: 2 [1280/4752 (26%)]\tLoss: 86.663681\n",
      "Train Epoch: 2 [2560/4752 (53%)]\tLoss: 100.727119\n",
      "Train Epoch: 2 [3840/4752 (79%)]\tLoss: 85.794983\n",
      "\n",
      "Validation set: Average loss: 0.0115, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 92.868546\n",
      "Train Epoch: 3 [1280/4752 (26%)]\tLoss: 87.552094\n",
      "Train Epoch: 3 [2560/4752 (53%)]\tLoss: 113.283531\n",
      "Train Epoch: 3 [3840/4752 (79%)]\tLoss: 80.832291\n",
      "\n",
      "Validation set: Average loss: 0.0564, Accuracy: 402/1584 (25%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 151.773117\n",
      "Train Epoch: 4 [1280/4752 (26%)]\tLoss: 76.278526\n",
      "Train Epoch: 4 [2560/4752 (53%)]\tLoss: 112.738075\n",
      "Train Epoch: 4 [3840/4752 (79%)]\tLoss: 107.840591\n",
      "\n",
      "Validation set: Average loss: 0.0139, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 109.400063\n",
      "Train Epoch: 5 [1280/4752 (26%)]\tLoss: 124.359192\n",
      "Train Epoch: 5 [2560/4752 (53%)]\tLoss: 114.159180\n",
      "Train Epoch: 5 [3840/4752 (79%)]\tLoss: 74.446121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-02 20:59:35,027] Trial 3 finished with value: -0.7462121212121212 and parameters: {'batch_size': 128, 'learning_rate': 0.07527339560856858, 'embedding_dim': 128, 'hidden_dim': 32, 'optimizer': 'RMSprop', 'dropout_rate': 0.10182729575093302, 'step_size': 45, 'gamma': 0.14614642750913567, 'sequence_length': 200}. Best is trial 0 with value: -0.7462121212121212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.0190, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.771361\n",
      "Train Epoch: 1 [320/4752 (7%)]\tLoss: 1.996622\n",
      "Train Epoch: 1 [640/4752 (13%)]\tLoss: 3.895391\n",
      "Train Epoch: 1 [960/4752 (20%)]\tLoss: 5.225539\n",
      "Train Epoch: 1 [1280/4752 (27%)]\tLoss: 4.462019\n",
      "Train Epoch: 1 [1600/4752 (34%)]\tLoss: 6.253251\n",
      "Train Epoch: 1 [1920/4752 (40%)]\tLoss: 4.687564\n",
      "Train Epoch: 1 [2240/4752 (47%)]\tLoss: 5.763823\n",
      "Train Epoch: 1 [2560/4752 (54%)]\tLoss: 4.725799\n",
      "Train Epoch: 1 [2880/4752 (60%)]\tLoss: 6.729166\n",
      "Train Epoch: 1 [3200/4752 (67%)]\tLoss: 8.529419\n",
      "Train Epoch: 1 [3520/4752 (74%)]\tLoss: 4.542541\n",
      "Train Epoch: 1 [3840/4752 (81%)]\tLoss: 4.423913\n",
      "Train Epoch: 1 [4160/4752 (87%)]\tLoss: 5.714040\n",
      "Train Epoch: 1 [4480/4752 (94%)]\tLoss: 6.579265\n",
      "\n",
      "Validation set: Average loss: 0.0343, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 6.041725\n",
      "Train Epoch: 2 [320/4752 (7%)]\tLoss: 6.092122\n",
      "Train Epoch: 2 [640/4752 (13%)]\tLoss: 6.341473\n",
      "Train Epoch: 2 [960/4752 (20%)]\tLoss: 6.176129\n",
      "Train Epoch: 2 [1280/4752 (27%)]\tLoss: 4.990542\n",
      "Train Epoch: 2 [1600/4752 (34%)]\tLoss: 4.809487\n",
      "Train Epoch: 2 [1920/4752 (40%)]\tLoss: 4.674689\n",
      "Train Epoch: 2 [2240/4752 (47%)]\tLoss: 7.546330\n",
      "Train Epoch: 2 [2560/4752 (54%)]\tLoss: 5.404240\n",
      "Train Epoch: 2 [2880/4752 (60%)]\tLoss: 6.960972\n",
      "Train Epoch: 2 [3200/4752 (67%)]\tLoss: 5.593790\n",
      "Train Epoch: 2 [3520/4752 (74%)]\tLoss: 5.524820\n",
      "Train Epoch: 2 [3840/4752 (81%)]\tLoss: 6.898894\n",
      "Train Epoch: 2 [4160/4752 (87%)]\tLoss: 9.455782\n",
      "Train Epoch: 2 [4480/4752 (94%)]\tLoss: 10.880423\n",
      "\n",
      "Validation set: Average loss: 0.0311, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 7.496222\n",
      "Train Epoch: 3 [320/4752 (7%)]\tLoss: 7.366348\n",
      "Train Epoch: 3 [640/4752 (13%)]\tLoss: 7.492166\n",
      "Train Epoch: 3 [960/4752 (20%)]\tLoss: 8.288831\n",
      "Train Epoch: 3 [1280/4752 (27%)]\tLoss: 8.954530\n",
      "Train Epoch: 3 [1600/4752 (34%)]\tLoss: 5.751611\n",
      "Train Epoch: 3 [1920/4752 (40%)]\tLoss: 5.868202\n",
      "Train Epoch: 3 [2240/4752 (47%)]\tLoss: 7.307952\n",
      "Train Epoch: 3 [2560/4752 (54%)]\tLoss: 9.776530\n",
      "Train Epoch: 3 [2880/4752 (60%)]\tLoss: 7.240533\n",
      "Train Epoch: 3 [3200/4752 (67%)]\tLoss: 9.229753\n",
      "Train Epoch: 3 [3520/4752 (74%)]\tLoss: 7.781091\n",
      "Train Epoch: 3 [3840/4752 (81%)]\tLoss: 7.723979\n",
      "Train Epoch: 3 [4160/4752 (87%)]\tLoss: 5.221014\n",
      "Train Epoch: 3 [4480/4752 (94%)]\tLoss: 9.197066\n",
      "\n",
      "Validation set: Average loss: 0.0296, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 5.709729\n",
      "Train Epoch: 4 [320/4752 (7%)]\tLoss: 8.072948\n",
      "Train Epoch: 4 [640/4752 (13%)]\tLoss: 8.158200\n",
      "Train Epoch: 4 [960/4752 (20%)]\tLoss: 10.225831\n",
      "Train Epoch: 4 [1280/4752 (27%)]\tLoss: 11.018324\n",
      "Train Epoch: 4 [1600/4752 (34%)]\tLoss: 7.030731\n",
      "Train Epoch: 4 [1920/4752 (40%)]\tLoss: 6.824150\n",
      "Train Epoch: 4 [2240/4752 (47%)]\tLoss: 10.244534\n",
      "Train Epoch: 4 [2560/4752 (54%)]\tLoss: 4.878850\n",
      "Train Epoch: 4 [2880/4752 (60%)]\tLoss: 10.639977\n",
      "Train Epoch: 4 [3200/4752 (67%)]\tLoss: 3.516302\n",
      "Train Epoch: 4 [3520/4752 (74%)]\tLoss: 6.914299\n",
      "Train Epoch: 4 [3840/4752 (81%)]\tLoss: 7.253694\n",
      "Train Epoch: 4 [4160/4752 (87%)]\tLoss: 9.174419\n",
      "Train Epoch: 4 [4480/4752 (94%)]\tLoss: 12.629290\n",
      "\n",
      "Validation set: Average loss: 0.0258, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 9.460485\n",
      "Train Epoch: 5 [320/4752 (7%)]\tLoss: 8.718154\n",
      "Train Epoch: 5 [640/4752 (13%)]\tLoss: 8.556634\n",
      "Train Epoch: 5 [960/4752 (20%)]\tLoss: 8.550965\n",
      "Train Epoch: 5 [1280/4752 (27%)]\tLoss: 11.297606\n",
      "Train Epoch: 5 [1600/4752 (34%)]\tLoss: 6.495461\n",
      "Train Epoch: 5 [1920/4752 (40%)]\tLoss: 9.942442\n",
      "Train Epoch: 5 [2240/4752 (47%)]\tLoss: 5.737331\n",
      "Train Epoch: 5 [2560/4752 (54%)]\tLoss: 8.736724\n",
      "Train Epoch: 5 [2880/4752 (60%)]\tLoss: 10.545625\n",
      "Train Epoch: 5 [3200/4752 (67%)]\tLoss: 8.767139\n",
      "Train Epoch: 5 [3520/4752 (74%)]\tLoss: 6.458152\n",
      "Train Epoch: 5 [3840/4752 (81%)]\tLoss: 7.554929\n",
      "Train Epoch: 5 [4160/4752 (87%)]\tLoss: 9.744472\n",
      "Train Epoch: 5 [4480/4752 (94%)]\tLoss: 8.833642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-02 21:00:25,901] Trial 4 finished with value: -0.7462121212121212 and parameters: {'batch_size': 32, 'learning_rate': 0.00017598356651484445, 'embedding_dim': 128, 'hidden_dim': 128, 'optimizer': 'RMSprop', 'dropout_rate': 0.4501877673997916, 'step_size': 68, 'gamma': 0.21208200504659475, 'sequence_length': 100}. Best is trial 0 with value: -0.7462121212121212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.0216, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Number of finished trials: 5\n",
      "Best trial:\n",
      "Best Validation Loss: 0.005118348532252842\n",
      "Best Validation Accuracy: 0.7462121212121212\n",
      "Best Trial Parameters:\n",
      "    batch_size: 128\n",
      "    learning_rate: 0.0012591182670210136\n",
      "    embedding_dim: 128\n",
      "    hidden_dim: 32\n",
      "    optimizer: Adam\n",
      "    dropout_rate: 0.013259984128779212\n",
      "    step_size: 78\n",
      "    gamma: 0.2509806262222176\n",
      "    sequence_length: 200\n"
     ]
    }
   ],
   "source": [
    "n_trials=5\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space\n",
    "    # vocabulary_size = trial.suggest_categorical('vocabulary_size', [5000, 10000, 20000, 40000])\n",
    "    vocabulary_size = 100000\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    embedding_dim = trial.suggest_categorical('embedding_dim', [64, 128, 256])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [32, 64, 128])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    step_size = trial.suggest_int('step_size', 1, 100)\n",
    "    gamma = trial.suggest_float('gamma', 0.1, 1.0, log=True)\n",
    "    sequence_length = trial.suggest_categorical('sequence_length', [50, 100, 200, 400])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Model setup with trial suggestions\n",
    "    model = QLearning(vocabulary_size, embedding_dim, hidden_dim, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 5 # Reduced for faster optimization cycles\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        val_loss, val_accuracy = validate(model, device, validation_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Set custom attributes for the trial\n",
    "    trial.set_user_attr(\"val_loss\", val_loss)\n",
    "    trial.set_user_attr(\"val_accuracy\", val_accuracy)\n",
    "    \n",
    "    # print(f\"Returning from validate: val_loss={val_loss}, val_accuracy={val_accuracy}\")\n",
    "    # return val_loss\n",
    "\n",
    "    # Objective: maximize validation accuracy by minimizing its negative value\n",
    "    return -val_accuracy  # Return the negative accuracy\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials) \n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "# Retrieve the validation loss and accuracy from the best trial\n",
    "best_val_loss = trial.user_attrs[\"val_loss\"]\n",
    "best_val_accuracy = trial.user_attrs[\"val_accuracy\"]\n",
    "\n",
    "print(f'Best Validation Loss: {best_val_loss}')\n",
    "print(f'Best Validation Accuracy: {best_val_accuracy}')\n",
    "print('Best Trial Parameters:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
