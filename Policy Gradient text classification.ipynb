{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock dataset for demonstration\n",
    "class TextDataset(Dataset):\n",
    "    # def __init__(self, vocabulary_size, sequence_length, num_samples):\n",
    "    #     self.data = torch.randint(0, vocabulary_size, (num_samples, sequence_length))\n",
    "    #     self.labels = torch.randint(0, 2, (num_samples,))\n",
    "    def __init__(self, texts, labels, sequence_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab = self.build_vocab(texts)\n",
    "        self.encoded_texts = [self.encode_text(text) for text in texts]\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        unique_words = set(word for text in texts for word in text.lower().split())\n",
    "        vocab = {word: i + 1 for i, word in enumerate(unique_words)}  # +1 for padding token at index 0\n",
    "        return vocab\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        return [self.vocab.get(word, 0) for word in text.lower().split()][:self.sequence_length] + [0] * (self.sequence_length - len(text.split()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.encoded_texts[idx]), torch.tensor(self.labels[idx])\n",
    "    \n",
    "\n",
    "# Define the Q-network model\n",
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, vocabulary_size, embedding_dim, hidden_dim, num_classes):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         embeds = self.embedding(x)\n",
    "#         lstm_out, _ = self.lstm(embeds)\n",
    "#         q_values = self.fc(lstm_out[:, -1])\n",
    "#         return q_values\n",
    "\n",
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, vocabulary_size, embedding_dim, hidden_dim, num_classes):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         embeds = self.embedding(x)\n",
    "#         lstm_out, _ = self.lstm(embeds)\n",
    "#         q_values = self.fc(lstm_out[:, -1])\n",
    "#         return q_values\n",
    "    \n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim, hidden_dim, num_classes, dropout_rate=0.5, pre_trained_embeddings=None):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        if pre_trained_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(pre_trained_embeddings)\n",
    "            self.embedding.weight.requires_grad = False  # Or True if you want to fine-tune\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.dropout(lstm_out[:, -1])\n",
    "        logits = self.fc(out)\n",
    "        probabilities = self.softmax(logits)\n",
    "        return probabilities\n",
    "\n",
    "def policy_gradient_loss(probabilities, actions, rewards):\n",
    "    # Negative log likelihood loss\n",
    "    log_prob = torch.log(probabilities[range(len(actions)), actions])\n",
    "    loss = -torch.mean(log_prob * rewards)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10):\n",
    "    model.train()\n",
    "    for batch_idx, (data, actions) in enumerate(train_loader):\n",
    "        data, actions = data.to(device), actions.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        probabilities = model(data)\n",
    "        \n",
    "        # Simulate rewards; in a real RL scenario, this would come from the environment\n",
    "        rewards = torch.rand(len(actions))  # Placeholder for actual reward calculation logic\n",
    "        \n",
    "        loss = policy_gradient_loss(probabilities, actions, rewards)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "def validate(model, device, validation_loader):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in validation_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            validation_loss += nn.CrossEntropyLoss()(output, target).item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    validation_loss /= len(validation_loader.dataset)\n",
    "    validation_acc = correct / len(validation_loader.dataset)\n",
    "    print(f'\\nValidation set: Average loss: {validation_loss:.4f}, Accuracy: {correct}/{len(validation_loader.dataset)} ({100. * correct / len(validation_loader.dataset):.0f}%)\\n')\n",
    "    return validation_loss, validation_acc\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Encoded text: tensor([[ 9249,   288,  9182,  6467, 15831, 16784, 14985,  3744,  2095,  2886],\n",
      "        [12493,  9746, 14026, 18670,  9588,  6219,  5640, 20155,  6219,  2317],\n",
      "        [15738,   124,  6283, 20232, 11152,  8098,  4936,  7763, 16950,    25],\n",
      "        [ 1674, 14415, 12894, 14680, 10873,  7407,  7368,  6192, 10002, 18437]])\n",
      "Train Label: tensor([0, 1, 0, 1])\n",
      "Validation Encoded text: tensor([[3925, 6913, 2028, 3912, 3591, 4467, 4922, 4487, 4383, 4484],\n",
      "        [7583, 5973, 1990, 3053, 6689,  191, 4206, 7471, 1204, 3157],\n",
      "        [8503, 6298, 2066, 3410, 1140, 3307, 6427,  718, 1589, 2886],\n",
      "        [7583, 6020, 1990,  770, 3571, 7393,  792,  735, 8797, 6473]])\n",
      "Validation Label: tensor([1, 1, 1, 1])\n",
      "Test Encoded text: tensor([[2863, 7953, 4415,  651,  626, 2169,  223, 2173, 7409, 6529],\n",
      "        [4909, 8420, 5283, 8077, 7102,   30, 1559, 5489, 2332, 3245],\n",
      "        [6123,  223, 6925, 2041, 3443, 1095, 5059, 4281,  934, 1505],\n",
      "        [7493, 7652, 6843,  453, 2045, 8095, 9025, 7654, 5651, 4429]])\n",
      "Test Label: tensor([1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/sentiment_analysis.csv')\n",
    "\n",
    "# Extracting texts and labels\n",
    "texts = df['tweet'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Splitting dataset into train+val and test\n",
    "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=RAND_STATE)\n",
    "\n",
    "# Splitting train+val into train and val\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(train_val_texts, train_val_labels, test_size=0.25, random_state=RAND_STATE)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Creating datasets\n",
    "sequence_length = 10  # Max number of words in a text\n",
    "train_dataset = TextDataset(train_texts, train_labels, sequence_length)\n",
    "validation_dataset = TextDataset(validation_texts, validation_labels, sequence_length)\n",
    "test_dataset = TextDataset(test_texts, test_labels, sequence_length)\n",
    "\n",
    "# Creating DataLoaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for data, label in train_loader:\n",
    "    print(f\"Train Encoded text: {data}\")\n",
    "    print(f\"Train Label: {label}\")\n",
    "    break  # Just show one batch for brevity\n",
    "\n",
    "for data, label in validation_loader:\n",
    "    print(f\"Validation Encoded text: {data}\")\n",
    "    print(f\"Validation Label: {label}\")\n",
    "    break  # Just show one batch for brevity\n",
    "\n",
    "for data, label in test_loader:\n",
    "    print(f\"Test Encoded text: {data}\")\n",
    "    print(f\"Test Label: {label}\")\n",
    "    break  # Just show one batch for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.362245\n",
      "Train Epoch: 1 [40/4752 (1%)]\tLoss: 0.125249\n",
      "Train Epoch: 1 [80/4752 (2%)]\tLoss: 0.312006\n",
      "Train Epoch: 1 [120/4752 (3%)]\tLoss: 0.245866\n",
      "Train Epoch: 1 [160/4752 (3%)]\tLoss: 0.249490\n",
      "Train Epoch: 1 [200/4752 (4%)]\tLoss: 0.199117\n",
      "Train Epoch: 1 [240/4752 (5%)]\tLoss: 0.147369\n",
      "Train Epoch: 1 [280/4752 (6%)]\tLoss: 0.136874\n",
      "Train Epoch: 1 [320/4752 (7%)]\tLoss: 0.087879\n",
      "Train Epoch: 1 [360/4752 (8%)]\tLoss: 0.074132\n",
      "Train Epoch: 1 [400/4752 (8%)]\tLoss: 0.344574\n",
      "Train Epoch: 1 [440/4752 (9%)]\tLoss: 0.221542\n",
      "Train Epoch: 1 [480/4752 (10%)]\tLoss: 0.147119\n",
      "Train Epoch: 1 [520/4752 (11%)]\tLoss: 0.155488\n",
      "Train Epoch: 1 [560/4752 (12%)]\tLoss: 0.544138\n",
      "Train Epoch: 1 [600/4752 (13%)]\tLoss: 0.103073\n",
      "Train Epoch: 1 [640/4752 (13%)]\tLoss: 0.267107\n",
      "Train Epoch: 1 [680/4752 (14%)]\tLoss: 0.231547\n",
      "Train Epoch: 1 [720/4752 (15%)]\tLoss: 0.075307\n",
      "Train Epoch: 1 [760/4752 (16%)]\tLoss: 0.073378\n",
      "Train Epoch: 1 [800/4752 (17%)]\tLoss: 0.940467\n",
      "Train Epoch: 1 [840/4752 (18%)]\tLoss: 0.213710\n",
      "Train Epoch: 1 [880/4752 (19%)]\tLoss: 0.140993\n",
      "Train Epoch: 1 [920/4752 (19%)]\tLoss: 0.451765\n",
      "Train Epoch: 1 [960/4752 (20%)]\tLoss: 0.178700\n",
      "Train Epoch: 1 [1000/4752 (21%)]\tLoss: 0.120242\n",
      "Train Epoch: 1 [1040/4752 (22%)]\tLoss: 0.762113\n",
      "Train Epoch: 1 [1080/4752 (23%)]\tLoss: 0.213310\n",
      "Train Epoch: 1 [1120/4752 (24%)]\tLoss: 0.185441\n",
      "Train Epoch: 1 [1160/4752 (24%)]\tLoss: 0.098531\n",
      "Train Epoch: 1 [1200/4752 (25%)]\tLoss: 0.355124\n",
      "Train Epoch: 1 [1240/4752 (26%)]\tLoss: 0.268348\n",
      "Train Epoch: 1 [1280/4752 (27%)]\tLoss: 0.139128\n",
      "Train Epoch: 1 [1320/4752 (28%)]\tLoss: 0.346782\n",
      "Train Epoch: 1 [1360/4752 (29%)]\tLoss: 0.058509\n",
      "Train Epoch: 1 [1400/4752 (29%)]\tLoss: 0.277242\n",
      "Train Epoch: 1 [1440/4752 (30%)]\tLoss: 0.305770\n",
      "Train Epoch: 1 [1480/4752 (31%)]\tLoss: 0.442452\n",
      "Train Epoch: 1 [1520/4752 (32%)]\tLoss: 0.039064\n",
      "Train Epoch: 1 [1560/4752 (33%)]\tLoss: 0.098196\n",
      "Train Epoch: 1 [1600/4752 (34%)]\tLoss: 0.349606\n",
      "Train Epoch: 1 [1640/4752 (35%)]\tLoss: 0.066045\n",
      "Train Epoch: 1 [1680/4752 (35%)]\tLoss: 0.237693\n",
      "Train Epoch: 1 [1720/4752 (36%)]\tLoss: 0.128516\n",
      "Train Epoch: 1 [1760/4752 (37%)]\tLoss: 0.263692\n",
      "Train Epoch: 1 [1800/4752 (38%)]\tLoss: 0.105977\n",
      "Train Epoch: 1 [1840/4752 (39%)]\tLoss: 0.107972\n",
      "Train Epoch: 1 [1880/4752 (40%)]\tLoss: 0.190215\n",
      "Train Epoch: 1 [1920/4752 (40%)]\tLoss: 0.103048\n",
      "Train Epoch: 1 [1960/4752 (41%)]\tLoss: 0.023754\n",
      "Train Epoch: 1 [2000/4752 (42%)]\tLoss: 0.256404\n",
      "Train Epoch: 1 [2040/4752 (43%)]\tLoss: 0.574050\n",
      "Train Epoch: 1 [2080/4752 (44%)]\tLoss: 0.262903\n",
      "Train Epoch: 1 [2120/4752 (45%)]\tLoss: 0.190341\n",
      "Train Epoch: 1 [2160/4752 (45%)]\tLoss: 0.398943\n",
      "Train Epoch: 1 [2200/4752 (46%)]\tLoss: 0.142512\n",
      "Train Epoch: 1 [2240/4752 (47%)]\tLoss: 0.158060\n",
      "Train Epoch: 1 [2280/4752 (48%)]\tLoss: 0.492969\n",
      "Train Epoch: 1 [2320/4752 (49%)]\tLoss: 0.116603\n",
      "Train Epoch: 1 [2360/4752 (50%)]\tLoss: 0.015360\n",
      "Train Epoch: 1 [2400/4752 (51%)]\tLoss: 0.037632\n",
      "Train Epoch: 1 [2440/4752 (51%)]\tLoss: 0.204193\n",
      "Train Epoch: 1 [2480/4752 (52%)]\tLoss: 0.137337\n",
      "Train Epoch: 1 [2520/4752 (53%)]\tLoss: 0.053679\n",
      "Train Epoch: 1 [2560/4752 (54%)]\tLoss: 0.072803\n",
      "Train Epoch: 1 [2600/4752 (55%)]\tLoss: 0.278696\n",
      "Train Epoch: 1 [2640/4752 (56%)]\tLoss: 0.157277\n",
      "Train Epoch: 1 [2680/4752 (56%)]\tLoss: 0.051853\n",
      "Train Epoch: 1 [2720/4752 (57%)]\tLoss: 0.150388\n",
      "Train Epoch: 1 [2760/4752 (58%)]\tLoss: 0.048106\n",
      "Train Epoch: 1 [2800/4752 (59%)]\tLoss: 0.195637\n",
      "Train Epoch: 1 [2840/4752 (60%)]\tLoss: 0.497016\n",
      "Train Epoch: 1 [2880/4752 (61%)]\tLoss: 0.156983\n",
      "Train Epoch: 1 [2920/4752 (61%)]\tLoss: 0.345031\n",
      "Train Epoch: 1 [2960/4752 (62%)]\tLoss: 0.224373\n",
      "Train Epoch: 1 [3000/4752 (63%)]\tLoss: 0.164616\n",
      "Train Epoch: 1 [3040/4752 (64%)]\tLoss: 0.163997\n",
      "Train Epoch: 1 [3080/4752 (65%)]\tLoss: 0.136988\n",
      "Train Epoch: 1 [3120/4752 (66%)]\tLoss: 0.223503\n",
      "Train Epoch: 1 [3160/4752 (66%)]\tLoss: 0.167623\n",
      "Train Epoch: 1 [3200/4752 (67%)]\tLoss: 0.136007\n",
      "Train Epoch: 1 [3240/4752 (68%)]\tLoss: 0.272998\n",
      "Train Epoch: 1 [3280/4752 (69%)]\tLoss: 0.095806\n",
      "Train Epoch: 1 [3320/4752 (70%)]\tLoss: 0.060669\n",
      "Train Epoch: 1 [3360/4752 (71%)]\tLoss: 0.093871\n",
      "Train Epoch: 1 [3400/4752 (72%)]\tLoss: 0.005543\n",
      "Train Epoch: 1 [3440/4752 (72%)]\tLoss: 1.329599\n",
      "Train Epoch: 1 [3480/4752 (73%)]\tLoss: 0.283307\n",
      "Train Epoch: 1 [3520/4752 (74%)]\tLoss: 0.090462\n",
      "Train Epoch: 1 [3560/4752 (75%)]\tLoss: 0.081786\n",
      "Train Epoch: 1 [3600/4752 (76%)]\tLoss: 0.691138\n",
      "Train Epoch: 1 [3640/4752 (77%)]\tLoss: 0.050903\n",
      "Train Epoch: 1 [3680/4752 (77%)]\tLoss: 0.021828\n",
      "Train Epoch: 1 [3720/4752 (78%)]\tLoss: 0.213329\n",
      "Train Epoch: 1 [3760/4752 (79%)]\tLoss: 0.133799\n",
      "Train Epoch: 1 [3800/4752 (80%)]\tLoss: 0.024896\n",
      "Train Epoch: 1 [3840/4752 (81%)]\tLoss: 0.042715\n",
      "Train Epoch: 1 [3880/4752 (82%)]\tLoss: 0.092143\n",
      "Train Epoch: 1 [3920/4752 (82%)]\tLoss: 0.166744\n",
      "Train Epoch: 1 [3960/4752 (83%)]\tLoss: 0.029460\n",
      "Train Epoch: 1 [4000/4752 (84%)]\tLoss: 0.128906\n",
      "Train Epoch: 1 [4040/4752 (85%)]\tLoss: 0.073373\n",
      "Train Epoch: 1 [4080/4752 (86%)]\tLoss: 0.213092\n",
      "Train Epoch: 1 [4120/4752 (87%)]\tLoss: 0.088632\n",
      "Train Epoch: 1 [4160/4752 (88%)]\tLoss: 0.276339\n",
      "Train Epoch: 1 [4200/4752 (88%)]\tLoss: 0.622392\n",
      "Train Epoch: 1 [4240/4752 (89%)]\tLoss: 0.015033\n",
      "Train Epoch: 1 [4280/4752 (90%)]\tLoss: 0.096810\n",
      "Train Epoch: 1 [4320/4752 (91%)]\tLoss: 0.196705\n",
      "Train Epoch: 1 [4360/4752 (92%)]\tLoss: 0.222731\n",
      "Train Epoch: 1 [4400/4752 (93%)]\tLoss: 0.055727\n",
      "Train Epoch: 1 [4440/4752 (93%)]\tLoss: 0.008843\n",
      "Train Epoch: 1 [4480/4752 (94%)]\tLoss: 0.163505\n",
      "Train Epoch: 1 [4520/4752 (95%)]\tLoss: 0.927820\n",
      "Train Epoch: 1 [4560/4752 (96%)]\tLoss: 0.060986\n",
      "Train Epoch: 1 [4600/4752 (97%)]\tLoss: 0.170409\n",
      "Train Epoch: 1 [4640/4752 (98%)]\tLoss: 0.081040\n",
      "Train Epoch: 1 [4680/4752 (98%)]\tLoss: 0.152109\n",
      "Train Epoch: 1 [4720/4752 (99%)]\tLoss: 0.106397\n",
      "\n",
      "Validation set: Average loss: 0.1492, Accuracy: 1135/1584 (72%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 0.047947\n",
      "Train Epoch: 2 [40/4752 (1%)]\tLoss: 0.050105\n",
      "Train Epoch: 2 [80/4752 (2%)]\tLoss: 0.643582\n",
      "Train Epoch: 2 [120/4752 (3%)]\tLoss: 0.056984\n",
      "Train Epoch: 2 [160/4752 (3%)]\tLoss: 0.289446\n",
      "Train Epoch: 2 [200/4752 (4%)]\tLoss: 0.118344\n",
      "Train Epoch: 2 [240/4752 (5%)]\tLoss: 0.061805\n",
      "Train Epoch: 2 [280/4752 (6%)]\tLoss: 0.003763\n",
      "Train Epoch: 2 [320/4752 (7%)]\tLoss: 0.033937\n",
      "Train Epoch: 2 [360/4752 (8%)]\tLoss: 0.058989\n",
      "Train Epoch: 2 [400/4752 (8%)]\tLoss: 0.018087\n",
      "Train Epoch: 2 [440/4752 (9%)]\tLoss: 0.046676\n",
      "Train Epoch: 2 [480/4752 (10%)]\tLoss: 0.110225\n",
      "Train Epoch: 2 [520/4752 (11%)]\tLoss: 0.173783\n",
      "Train Epoch: 2 [560/4752 (12%)]\tLoss: 0.065414\n",
      "Train Epoch: 2 [600/4752 (13%)]\tLoss: 0.473518\n",
      "Train Epoch: 2 [640/4752 (13%)]\tLoss: 0.136523\n",
      "Train Epoch: 2 [680/4752 (14%)]\tLoss: 0.006417\n",
      "Train Epoch: 2 [720/4752 (15%)]\tLoss: 0.046863\n",
      "Train Epoch: 2 [760/4752 (16%)]\tLoss: 0.283123\n",
      "Train Epoch: 2 [800/4752 (17%)]\tLoss: 0.028211\n",
      "Train Epoch: 2 [840/4752 (18%)]\tLoss: 0.045304\n",
      "Train Epoch: 2 [880/4752 (19%)]\tLoss: 0.052637\n",
      "Train Epoch: 2 [920/4752 (19%)]\tLoss: 0.206466\n",
      "Train Epoch: 2 [960/4752 (20%)]\tLoss: 0.035585\n",
      "Train Epoch: 2 [1000/4752 (21%)]\tLoss: 0.310788\n",
      "Train Epoch: 2 [1040/4752 (22%)]\tLoss: 0.201953\n",
      "Train Epoch: 2 [1080/4752 (23%)]\tLoss: 0.178289\n",
      "Train Epoch: 2 [1120/4752 (24%)]\tLoss: 0.015546\n",
      "Train Epoch: 2 [1160/4752 (24%)]\tLoss: 0.140180\n",
      "Train Epoch: 2 [1200/4752 (25%)]\tLoss: 0.168603\n",
      "Train Epoch: 2 [1240/4752 (26%)]\tLoss: 0.071907\n",
      "Train Epoch: 2 [1280/4752 (27%)]\tLoss: 0.165805\n",
      "Train Epoch: 2 [1320/4752 (28%)]\tLoss: 0.147972\n",
      "Train Epoch: 2 [1360/4752 (29%)]\tLoss: 0.006369\n",
      "Train Epoch: 2 [1400/4752 (29%)]\tLoss: 0.014527\n",
      "Train Epoch: 2 [1440/4752 (30%)]\tLoss: 0.252879\n",
      "Train Epoch: 2 [1480/4752 (31%)]\tLoss: 0.258601\n",
      "Train Epoch: 2 [1520/4752 (32%)]\tLoss: 0.345954\n",
      "Train Epoch: 2 [1560/4752 (33%)]\tLoss: 0.105207\n",
      "Train Epoch: 2 [1600/4752 (34%)]\tLoss: 0.000778\n",
      "Train Epoch: 2 [1640/4752 (35%)]\tLoss: 0.323058\n",
      "Train Epoch: 2 [1680/4752 (35%)]\tLoss: 0.275580\n",
      "Train Epoch: 2 [1720/4752 (36%)]\tLoss: 0.104651\n",
      "Train Epoch: 2 [1760/4752 (37%)]\tLoss: 0.288183\n",
      "Train Epoch: 2 [1800/4752 (38%)]\tLoss: 0.022352\n",
      "Train Epoch: 2 [1840/4752 (39%)]\tLoss: 0.011032\n",
      "Train Epoch: 2 [1880/4752 (40%)]\tLoss: 0.047491\n",
      "Train Epoch: 2 [1920/4752 (40%)]\tLoss: 0.101653\n",
      "Train Epoch: 2 [1960/4752 (41%)]\tLoss: 0.076633\n",
      "Train Epoch: 2 [2000/4752 (42%)]\tLoss: 0.223704\n",
      "Train Epoch: 2 [2040/4752 (43%)]\tLoss: 0.524844\n",
      "Train Epoch: 2 [2080/4752 (44%)]\tLoss: 0.293300\n",
      "Train Epoch: 2 [2120/4752 (45%)]\tLoss: 0.003621\n",
      "Train Epoch: 2 [2160/4752 (45%)]\tLoss: 0.001297\n",
      "Train Epoch: 2 [2200/4752 (46%)]\tLoss: 0.019894\n",
      "Train Epoch: 2 [2240/4752 (47%)]\tLoss: 0.235584\n",
      "Train Epoch: 2 [2280/4752 (48%)]\tLoss: 0.579505\n",
      "Train Epoch: 2 [2320/4752 (49%)]\tLoss: 0.262181\n",
      "Train Epoch: 2 [2360/4752 (50%)]\tLoss: 0.157280\n",
      "Train Epoch: 2 [2400/4752 (51%)]\tLoss: 0.070463\n",
      "Train Epoch: 2 [2440/4752 (51%)]\tLoss: 0.085830\n",
      "Train Epoch: 2 [2480/4752 (52%)]\tLoss: 0.002698\n",
      "Train Epoch: 2 [2520/4752 (53%)]\tLoss: 0.593849\n",
      "Train Epoch: 2 [2560/4752 (54%)]\tLoss: 0.088440\n",
      "Train Epoch: 2 [2600/4752 (55%)]\tLoss: 0.120988\n",
      "Train Epoch: 2 [2640/4752 (56%)]\tLoss: 0.006544\n",
      "Train Epoch: 2 [2680/4752 (56%)]\tLoss: 0.259840\n",
      "Train Epoch: 2 [2720/4752 (57%)]\tLoss: 0.053014\n",
      "Train Epoch: 2 [2760/4752 (58%)]\tLoss: 0.101521\n",
      "Train Epoch: 2 [2800/4752 (59%)]\tLoss: 0.115096\n",
      "Train Epoch: 2 [2840/4752 (60%)]\tLoss: 0.009207\n",
      "Train Epoch: 2 [2880/4752 (61%)]\tLoss: 0.273057\n",
      "Train Epoch: 2 [2920/4752 (61%)]\tLoss: 0.042805\n",
      "Train Epoch: 2 [2960/4752 (62%)]\tLoss: 0.530455\n",
      "Train Epoch: 2 [3000/4752 (63%)]\tLoss: 0.039366\n",
      "Train Epoch: 2 [3040/4752 (64%)]\tLoss: 0.170700\n",
      "Train Epoch: 2 [3080/4752 (65%)]\tLoss: 0.118436\n",
      "Train Epoch: 2 [3120/4752 (66%)]\tLoss: 0.127073\n",
      "Train Epoch: 2 [3160/4752 (66%)]\tLoss: 0.173269\n",
      "Train Epoch: 2 [3200/4752 (67%)]\tLoss: 0.213405\n",
      "Train Epoch: 2 [3240/4752 (68%)]\tLoss: 0.031122\n",
      "Train Epoch: 2 [3280/4752 (69%)]\tLoss: 0.021417\n",
      "Train Epoch: 2 [3320/4752 (70%)]\tLoss: 0.180297\n",
      "Train Epoch: 2 [3360/4752 (71%)]\tLoss: 0.436524\n",
      "Train Epoch: 2 [3400/4752 (72%)]\tLoss: 0.184048\n",
      "Train Epoch: 2 [3440/4752 (72%)]\tLoss: 0.032941\n",
      "Train Epoch: 2 [3480/4752 (73%)]\tLoss: 0.013768\n",
      "Train Epoch: 2 [3520/4752 (74%)]\tLoss: 0.003337\n",
      "Train Epoch: 2 [3560/4752 (75%)]\tLoss: 0.141795\n",
      "Train Epoch: 2 [3600/4752 (76%)]\tLoss: 0.150051\n",
      "Train Epoch: 2 [3640/4752 (77%)]\tLoss: 0.043870\n",
      "Train Epoch: 2 [3680/4752 (77%)]\tLoss: 0.014565\n",
      "Train Epoch: 2 [3720/4752 (78%)]\tLoss: 0.241899\n",
      "Train Epoch: 2 [3760/4752 (79%)]\tLoss: 0.046343\n",
      "Train Epoch: 2 [3800/4752 (80%)]\tLoss: 0.951332\n",
      "Train Epoch: 2 [3840/4752 (81%)]\tLoss: 0.088803\n",
      "Train Epoch: 2 [3880/4752 (82%)]\tLoss: 0.034007\n",
      "Train Epoch: 2 [3920/4752 (82%)]\tLoss: 0.037577\n",
      "Train Epoch: 2 [3960/4752 (83%)]\tLoss: 0.042770\n",
      "Train Epoch: 2 [4000/4752 (84%)]\tLoss: 0.227883\n",
      "Train Epoch: 2 [4040/4752 (85%)]\tLoss: 0.061224\n",
      "Train Epoch: 2 [4080/4752 (86%)]\tLoss: 0.044345\n",
      "Train Epoch: 2 [4120/4752 (87%)]\tLoss: 0.127501\n",
      "Train Epoch: 2 [4160/4752 (88%)]\tLoss: 0.260000\n",
      "Train Epoch: 2 [4200/4752 (88%)]\tLoss: 0.351447\n",
      "Train Epoch: 2 [4240/4752 (89%)]\tLoss: 0.046740\n",
      "Train Epoch: 2 [4280/4752 (90%)]\tLoss: 0.460893\n",
      "Train Epoch: 2 [4320/4752 (91%)]\tLoss: 0.525432\n",
      "Train Epoch: 2 [4360/4752 (92%)]\tLoss: 0.042458\n",
      "Train Epoch: 2 [4400/4752 (93%)]\tLoss: 0.110840\n",
      "Train Epoch: 2 [4440/4752 (93%)]\tLoss: 0.019114\n",
      "Train Epoch: 2 [4480/4752 (94%)]\tLoss: 0.086250\n",
      "Train Epoch: 2 [4520/4752 (95%)]\tLoss: 0.097373\n",
      "Train Epoch: 2 [4560/4752 (96%)]\tLoss: 0.173315\n",
      "Train Epoch: 2 [4600/4752 (97%)]\tLoss: 0.004729\n",
      "Train Epoch: 2 [4640/4752 (98%)]\tLoss: 0.002729\n",
      "Train Epoch: 2 [4680/4752 (98%)]\tLoss: 0.026763\n",
      "Train Epoch: 2 [4720/4752 (99%)]\tLoss: 0.032983\n",
      "\n",
      "Validation set: Average loss: 0.1500, Accuracy: 1103/1584 (70%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1526, Accuracy: 1095/1584 (69%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters and Hyperparameters\n",
    "vocabulary_size = 100000  # to adjust \n",
    "sequence_length = 50  # to adjust \n",
    "embedding_dim = 128\n",
    "hidden_dim = 64\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, optimizer, and device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PolicyNetwork(vocabulary_size, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    validate(model, device, validation_loader)\n",
    "\n",
    "# After training, evaluate on the test set\n",
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-02 21:31:12,844] A new study created in memory with name: no-name-6a524d41-e0ce-455f-a176-ab6a77a8383c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.347969\n",
      "Train Epoch: 1 [320/4752 (7%)]\tLoss: 0.403945\n",
      "Train Epoch: 1 [640/4752 (13%)]\tLoss: 0.445074\n",
      "Train Epoch: 1 [960/4752 (20%)]\tLoss: 0.310939\n",
      "Train Epoch: 1 [1280/4752 (27%)]\tLoss: 0.338621\n",
      "Train Epoch: 1 [1600/4752 (34%)]\tLoss: 0.400845\n",
      "Train Epoch: 1 [1920/4752 (40%)]\tLoss: 0.358307\n",
      "Train Epoch: 1 [2240/4752 (47%)]\tLoss: 0.306752\n",
      "Train Epoch: 1 [2560/4752 (54%)]\tLoss: 0.371696\n",
      "Train Epoch: 1 [2880/4752 (60%)]\tLoss: 0.372941\n",
      "Train Epoch: 1 [3200/4752 (67%)]\tLoss: 0.314638\n",
      "Train Epoch: 1 [3520/4752 (74%)]\tLoss: 0.348230\n",
      "Train Epoch: 1 [3840/4752 (81%)]\tLoss: 0.333738\n",
      "Train Epoch: 1 [4160/4752 (87%)]\tLoss: 0.291597\n",
      "Train Epoch: 1 [4480/4752 (94%)]\tLoss: 0.388255\n",
      "\n",
      "Validation set: Average loss: 0.0220, Accuracy: 505/1584 (32%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 0.373070\n",
      "Train Epoch: 2 [320/4752 (7%)]\tLoss: 0.356189\n",
      "Train Epoch: 2 [640/4752 (13%)]\tLoss: 0.411839\n",
      "Train Epoch: 2 [960/4752 (20%)]\tLoss: 0.370149\n",
      "Train Epoch: 2 [1280/4752 (27%)]\tLoss: 0.360297\n",
      "Train Epoch: 2 [1600/4752 (34%)]\tLoss: 0.375002\n",
      "Train Epoch: 2 [1920/4752 (40%)]\tLoss: 0.361001\n",
      "Train Epoch: 2 [2240/4752 (47%)]\tLoss: 0.382634\n",
      "Train Epoch: 2 [2560/4752 (54%)]\tLoss: 0.376028\n",
      "Train Epoch: 2 [2880/4752 (60%)]\tLoss: 0.331383\n",
      "Train Epoch: 2 [3200/4752 (67%)]\tLoss: 0.334575\n",
      "Train Epoch: 2 [3520/4752 (74%)]\tLoss: 0.349151\n",
      "Train Epoch: 2 [3840/4752 (81%)]\tLoss: 0.333764\n",
      "Train Epoch: 2 [4160/4752 (87%)]\tLoss: 0.370744\n",
      "Train Epoch: 2 [4480/4752 (94%)]\tLoss: 0.282638\n",
      "\n",
      "Validation set: Average loss: 0.0220, Accuracy: 504/1584 (32%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 0.372368\n",
      "Train Epoch: 3 [320/4752 (7%)]\tLoss: 0.377368\n",
      "Train Epoch: 3 [640/4752 (13%)]\tLoss: 0.371003\n",
      "Train Epoch: 3 [960/4752 (20%)]\tLoss: 0.282202\n",
      "Train Epoch: 3 [1280/4752 (27%)]\tLoss: 0.344433\n",
      "Train Epoch: 3 [1600/4752 (34%)]\tLoss: 0.404099\n",
      "Train Epoch: 3 [1920/4752 (40%)]\tLoss: 0.381533\n",
      "Train Epoch: 3 [2240/4752 (47%)]\tLoss: 0.377208\n",
      "Train Epoch: 3 [2560/4752 (54%)]\tLoss: 0.360837\n",
      "Train Epoch: 3 [2880/4752 (60%)]\tLoss: 0.286573\n",
      "Train Epoch: 3 [3200/4752 (67%)]\tLoss: 0.339204\n",
      "Train Epoch: 3 [3520/4752 (74%)]\tLoss: 0.329339\n",
      "Train Epoch: 3 [3840/4752 (81%)]\tLoss: 0.360254\n",
      "Train Epoch: 3 [4160/4752 (87%)]\tLoss: 0.393890\n",
      "Train Epoch: 3 [4480/4752 (94%)]\tLoss: 0.380107\n",
      "\n",
      "Validation set: Average loss: 0.0220, Accuracy: 506/1584 (32%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 0.347305\n",
      "Train Epoch: 4 [320/4752 (7%)]\tLoss: 0.408448\n",
      "Train Epoch: 4 [640/4752 (13%)]\tLoss: 0.343578\n",
      "Train Epoch: 4 [960/4752 (20%)]\tLoss: 0.282493\n",
      "Train Epoch: 4 [1280/4752 (27%)]\tLoss: 0.299687\n",
      "Train Epoch: 4 [1600/4752 (34%)]\tLoss: 0.330369\n",
      "Train Epoch: 4 [1920/4752 (40%)]\tLoss: 0.270226\n",
      "Train Epoch: 4 [2240/4752 (47%)]\tLoss: 0.399345\n",
      "Train Epoch: 4 [2560/4752 (54%)]\tLoss: 0.316847\n",
      "Train Epoch: 4 [2880/4752 (60%)]\tLoss: 0.330167\n",
      "Train Epoch: 4 [3200/4752 (67%)]\tLoss: 0.348217\n",
      "Train Epoch: 4 [3520/4752 (74%)]\tLoss: 0.333859\n",
      "Train Epoch: 4 [3840/4752 (81%)]\tLoss: 0.329086\n",
      "Train Epoch: 4 [4160/4752 (87%)]\tLoss: 0.356239\n",
      "Train Epoch: 4 [4480/4752 (94%)]\tLoss: 0.351538\n",
      "\n",
      "Validation set: Average loss: 0.0220, Accuracy: 510/1584 (32%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 0.370993\n",
      "Train Epoch: 5 [320/4752 (7%)]\tLoss: 0.351200\n",
      "Train Epoch: 5 [640/4752 (13%)]\tLoss: 0.274821\n",
      "Train Epoch: 5 [960/4752 (20%)]\tLoss: 0.413402\n",
      "Train Epoch: 5 [1280/4752 (27%)]\tLoss: 0.322865\n",
      "Train Epoch: 5 [1600/4752 (34%)]\tLoss: 0.329899\n",
      "Train Epoch: 5 [1920/4752 (40%)]\tLoss: 0.354396\n",
      "Train Epoch: 5 [2240/4752 (47%)]\tLoss: 0.369782\n",
      "Train Epoch: 5 [2560/4752 (54%)]\tLoss: 0.437448\n",
      "Train Epoch: 5 [2880/4752 (60%)]\tLoss: 0.251840\n",
      "Train Epoch: 5 [3200/4752 (67%)]\tLoss: 0.345563\n",
      "Train Epoch: 5 [3520/4752 (74%)]\tLoss: 0.318564\n",
      "Train Epoch: 5 [3840/4752 (81%)]\tLoss: 0.341884\n",
      "Train Epoch: 5 [4160/4752 (87%)]\tLoss: 0.358811\n",
      "Train Epoch: 5 [4480/4752 (94%)]\tLoss: 0.334387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-02 21:31:34,300] Trial 0 finished with value: -0.32323232323232326 and parameters: {'batch_size': 32, 'learning_rate': 1.0118171703582376e-05, 'embedding_dim': 256, 'hidden_dim': 64, 'optimizer': 'SGD', 'dropout_rate': 0.18519460843451108, 'step_size': 71, 'gamma': 0.17167883086941504, 'sequence_length': 50}. Best is trial 0 with value: -0.32323232323232326.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.0220, Accuracy: 512/1584 (32%)\n",
      "\n",
      "Train Epoch: 1 [0/4752 (0%)]\tLoss: 0.339870\n",
      "Train Epoch: 1 [640/4752 (13%)]\tLoss: 0.377715\n",
      "Train Epoch: 1 [1280/4752 (27%)]\tLoss: 0.329393\n",
      "Train Epoch: 1 [1920/4752 (40%)]\tLoss: 0.340592\n",
      "Train Epoch: 1 [2560/4752 (53%)]\tLoss: 0.303404\n",
      "Train Epoch: 1 [3200/4752 (67%)]\tLoss: 0.375112\n",
      "Train Epoch: 1 [3840/4752 (80%)]\tLoss: 0.306475\n",
      "Train Epoch: 1 [4480/4752 (93%)]\tLoss: 0.306763\n",
      "\n",
      "Validation set: Average loss: 0.0109, Accuracy: 1063/1584 (67%)\n",
      "\n",
      "Train Epoch: 2 [0/4752 (0%)]\tLoss: 0.368487\n",
      "Train Epoch: 2 [640/4752 (13%)]\tLoss: 0.302377\n",
      "Train Epoch: 2 [1280/4752 (27%)]\tLoss: 0.326948\n",
      "Train Epoch: 2 [1920/4752 (40%)]\tLoss: 0.363053\n",
      "Train Epoch: 2 [2560/4752 (53%)]\tLoss: 0.329449\n",
      "Train Epoch: 2 [3200/4752 (67%)]\tLoss: 0.325415\n",
      "Train Epoch: 2 [3840/4752 (80%)]\tLoss: 0.329252\n",
      "Train Epoch: 2 [4480/4752 (93%)]\tLoss: 0.288112\n",
      "\n",
      "Validation set: Average loss: 0.0108, Accuracy: 1175/1584 (74%)\n",
      "\n",
      "Train Epoch: 3 [0/4752 (0%)]\tLoss: 0.331590\n",
      "Train Epoch: 3 [640/4752 (13%)]\tLoss: 0.296491\n",
      "Train Epoch: 3 [1280/4752 (27%)]\tLoss: 0.303138\n",
      "Train Epoch: 3 [1920/4752 (40%)]\tLoss: 0.339679\n",
      "Train Epoch: 3 [2560/4752 (53%)]\tLoss: 0.321385\n",
      "Train Epoch: 3 [3200/4752 (67%)]\tLoss: 0.310273\n",
      "Train Epoch: 3 [3840/4752 (80%)]\tLoss: 0.331640\n",
      "Train Epoch: 3 [4480/4752 (93%)]\tLoss: 0.328422\n",
      "\n",
      "Validation set: Average loss: 0.0106, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 4 [0/4752 (0%)]\tLoss: 0.318573\n",
      "Train Epoch: 4 [640/4752 (13%)]\tLoss: 0.284151\n",
      "Train Epoch: 4 [1280/4752 (27%)]\tLoss: 0.279448\n",
      "Train Epoch: 4 [1920/4752 (40%)]\tLoss: 0.315435\n",
      "Train Epoch: 4 [2560/4752 (53%)]\tLoss: 0.297518\n",
      "Train Epoch: 4 [3200/4752 (67%)]\tLoss: 0.298475\n",
      "Train Epoch: 4 [3840/4752 (80%)]\tLoss: 0.302717\n",
      "Train Epoch: 4 [4480/4752 (93%)]\tLoss: 0.293078\n",
      "\n",
      "Validation set: Average loss: 0.0104, Accuracy: 1182/1584 (75%)\n",
      "\n",
      "Train Epoch: 5 [0/4752 (0%)]\tLoss: 0.250840\n",
      "Train Epoch: 5 [640/4752 (13%)]\tLoss: 0.274562\n"
     ]
    }
   ],
   "source": [
    "# Parameters and Hyperparameters\n",
    "n_trials=5\n",
    "num_classes = 2\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space\n",
    "    # vocabulary_size = trial.suggest_categorical('vocabulary_size', [5000, 10000, 20000, 40000])\n",
    "    vocabulary_size = 100000\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    embedding_dim = trial.suggest_categorical('embedding_dim', [64, 128, 256])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [32, 64, 128])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    step_size = trial.suggest_int('step_size', 1, 100)\n",
    "    gamma = trial.suggest_float('gamma', 0.1, 1.0, log=True)\n",
    "    sequence_length = trial.suggest_categorical('sequence_length', [50, 100, 200, 400])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Model setup with trial suggestions\n",
    "    model = PolicyNetwork(vocabulary_size, embedding_dim, hidden_dim, num_classes, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 5  # Reduced for faster optimization cycles\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        val_loss, val_accuracy = validate(model, device, validation_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Set custom attributes for the trial\n",
    "    trial.set_user_attr(\"val_loss\", val_loss)\n",
    "    trial.set_user_attr(\"val_accuracy\", val_accuracy)\n",
    "    \n",
    "    # print(f\"Returning from validate: val_loss={val_loss}, val_accuracy={val_accuracy}\")\n",
    "    # return val_loss\n",
    "\n",
    "    # Objective: maximize validation accuracy by minimizing its negative value\n",
    "    return -val_accuracy  # Return the negative accuracy\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials)  # Number of trials can be adjusted\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "# Retrieve the validation loss and accuracy from the best trial\n",
    "best_val_loss = trial.user_attrs[\"val_loss\"]\n",
    "best_val_accuracy = trial.user_attrs[\"val_accuracy\"]\n",
    "\n",
    "print(f'Best Validation Loss: {best_val_loss}')\n",
    "print(f'Best Validation Accuracy: {best_val_accuracy}')\n",
    "print('Best Trial Parameters:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
